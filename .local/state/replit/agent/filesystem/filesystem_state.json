{"file_contents":{"app.py":{"content":"import streamlit as st\nimport os\nfrom database_postgres import init_database\nfrom dashboard import render_dashboard\nfrom auth import require_authentication, render_user_info, check_authentication\n\ndef main():\n    # Set page config first (must be first Streamlit command)\n    st.set_page_config(\n        page_title=\"InnoVantage Resume Evaluation System\",\n        page_icon=\"üéØ\",\n        layout=\"wide\",\n        initial_sidebar_state=\"expanded\",\n        menu_items={\n            'Get Help': 'mailto:support@innomatics.in',\n            'Report a bug': 'mailto:tech-support@innomatics.in',\n            'About': 'InnoVantage - AI-Powered Resume Evaluation System v2.0'\n        }\n    )\n    \n    # Note: Database initialization moved after page selection to allow Student Portal access\n    \n    # Modern header with gradient-style design\n    st.markdown(\"\"\"<div style='text-align: center; padding: 1rem; background: linear-gradient(90deg, #1f77b4, #ff7f0e); border-radius: 10px; margin-bottom: 2rem;'>\n        <h1 style='color: white; margin: 0; font-size: 2.5rem;'>üéØ InnoVantage</h1>\n        <h3 style='color: white; margin: 0; font-weight: 300;'>AI-Powered Resume Evaluation System</h3>\n        <p style='color: white; margin: 0; opacity: 0.9;'>Innomatics Research Labs ‚Ä¢ Advanced Talent Analytics Platform</p>\n    </div>\"\"\", unsafe_allow_html=True)\n    \n    # Modern sidebar navigation with icons\n    st.sidebar.markdown(\"\"\"<div style='text-align: center; padding: 1rem; background: #f0f2f6; border-radius: 10px; margin-bottom: 1rem;'>\n        <h2 style='margin: 0; color: #1f77b4;'>üß≠ Navigation</h2>\n    </div>\"\"\", unsafe_allow_html=True)\n    \n    page_options = {\n        \"üë®‚Äçüéì Student Portal\": \"Student Portal\",\n        \"üìä Dashboard\": \"Dashboard\", \n        \"üìù Job Descriptions\": \"Upload Job Description\",\n        \"üìÑ Resume Upload\": \"Upload Resume\",\n        \"üöÄ Batch Processing\": \"Batch Evaluation\",\n        \"üìà Analytics\": \"Analytics\"\n    }\n    \n    selected_page = st.sidebar.selectbox(\n        \"Choose your destination:\",\n        list(page_options.keys()),\n        format_func=lambda x: x\n    )\n    page = page_options[selected_page]\n    \n    # Check if student portal is selected (no authentication required)\n    if page == \"Student Portal\":\n        st.sidebar.markdown(\"\"\"<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1rem; border-radius: 10px; text-align: center; color: white; margin-bottom: 1rem;'>\n            <h4 style='margin: 0; color: white;'>üë®‚Äçüéì Student Portal</h4>\n            <p style='margin: 0.5rem 0 0 0; opacity: 0.9;'>No login required</p>\n        </div>\"\"\", unsafe_allow_html=True)\n        \n        # Check for OpenAI API key warning for students\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            st.markdown(\"\"\"<div style='background: #e1f5fe; border-left: 4px solid #01579b; padding: 1rem; border-radius: 5px; margin: 1rem 0;'>\n                <p style='margin: 0; color: #01579b;'>‚ÑπÔ∏è <strong>Notice:</strong> Some AI features may be limited. Contact your placement coordinator for assistance.</p>\n            </div>\"\"\", unsafe_allow_html=True)\n        \n        # Render student portal\n        render_dashboard(page)\n        return\n    \n    # Initialize database for authenticated pages only\n    if 'initialized' not in st.session_state:\n        try:\n            if init_database():\n                st.session_state.initialized = True\n            else:\n                st.error(\"‚ùå Database initialization failed. Please check your database configuration.\")\n                st.info(\"üí° **Tip**: Ensure PostgreSQL is running or check your environment variables.\")\n                st.stop()\n        except Exception as e:\n            st.error(f\"‚ùå Database connection error: {str(e)}\")\n            st.info(\"üí° **Tip**: Check your DATABASE_URL and ensure the database service is running.\")\n            st.stop()\n    \n    # For all other pages, require authentication\n    if not require_authentication():\n        st.markdown(\"\"\"<div style='background: #fff3e0; border-left: 4px solid #ef6c00; padding: 1.5rem; border-radius: 10px; text-align: center; margin: 2rem 0;'>\n            <h3 style='margin: 0; color: #ef6c00;'>üîí Staff Access Required</h3>\n            <p style='margin: 0.5rem 0 0 0; color: #bf360c;'>Please login to access placement team features</p>\n        </div>\"\"\", unsafe_allow_html=True)\n        return\n    \n    # Check for OpenAI API key for staff\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        st.warning(\"‚ö†Ô∏è OpenAI API Key not configured. AI features will use rule-based fallbacks.\")\n    \n    # Render user info in sidebar for authenticated users\n    render_user_info()\n    \n    # Render selected page\n    render_dashboard(page)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":4853},"dashboard.py":{"content":"import streamlit as st\nimport pandas as pd\nimport json\nimport time\nfrom datetime import datetime\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom text_extractor import process_uploaded_file, extract_text_from_pdf, extract_text_from_docx\nfrom nlp_processor import parse_job_description\nfrom scoring_engine import ResumeScorer\nfrom database_postgres import (\n    save_job_description, save_resume, save_evaluation, \n    get_job_descriptions, get_resumes, get_evaluations, \n    get_job_by_id, get_resume_by_id, get_evaluation_stats\n)\n\ndef render_dashboard(page):\n    \"\"\"Render the selected dashboard page\"\"\"\n    \n    if page == \"Dashboard\":\n        render_main_dashboard()\n    elif page == \"Upload Job Description\":\n        render_job_upload_page()\n    elif page == \"Upload Resume\":\n        render_resume_upload_page()\n    elif page == \"Batch Evaluation\":\n        render_batch_evaluation_page()\n        st.markdown(\"---\")\n        from batch_processor import render_enhanced_batch_processing\n        render_enhanced_batch_processing()\n    elif page == \"Analytics\":\n        render_analytics_page()\n        st.markdown(\"---\")\n        render_advanced_analytics()\n    elif page == \"Student Portal\":\n        render_student_portal()\n\ndef render_main_dashboard():\n    \"\"\"Render main dashboard with overview and recent evaluations\"\"\"\n    # Modern dashboard header\n    st.markdown(\"\"\"<div style='background: linear-gradient(90deg, #667eea, #764ba2); padding: 1.5rem; border-radius: 15px; margin-bottom: 2rem; text-align: center;'>\n        <h1 style='color: white; margin: 0; font-size: 2rem;'>üìä Executive Dashboard</h1>\n        <p style='color: white; margin: 0.5rem 0 0 0; opacity: 0.9;'>Real-time insights and analytics</p>\n    </div>\"\"\", unsafe_allow_html=True)\n    \n    # Get statistics\n    stats = get_evaluation_stats()\n    \n    # Display key metrics\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"Total Jobs\", stats['total_jobs'])\n    \n    with col2:\n        st.metric(\"Total Resumes\", stats['total_resumes'])\n    \n    with col3:\n        st.metric(\"Total Evaluations\", stats['total_evaluations'])\n    \n    with col4:\n        st.metric(\"Avg Score\", f\"{stats['average_score']:.1f}\")\n    \n    # Verdict distribution chart\n    if stats['verdict_distribution']:\n        st.subheader(\"Verdict Distribution\")\n        verdict_df = pd.DataFrame(list(stats['verdict_distribution'].items()), \n                                columns=['Verdict', 'Count'])\n        \n        fig = px.pie(verdict_df, values='Count', names='Verdict', \n                    color_discrete_map={'High': '#00ff00', 'Medium': '#ffff00', 'Low': '#ff0000'})\n        st.plotly_chart(fig, use_container_width=True)\n    \n    # Recent evaluations\n    st.subheader(\"Recent Evaluations\")\n    recent_evaluations = get_evaluations()[:10]  # Get last 10 evaluations\n    \n    if recent_evaluations:\n        eval_data = []\n        for eval_row in recent_evaluations:\n            # PostgreSQL query returns: e.*, j.title as job_title, j.company, r.filename, r.candidate_name, r.candidate_email\n            # So the columns are: evaluation columns (0-10) + job_title (11) + company (12) + filename (13) + candidate_name (14) + candidate_email (15)\n            eval_data.append({\n                'Job Title': eval_row[11] if len(eval_row) > 11 else 'Unknown',  # job_title\n                'Company': eval_row[12] if len(eval_row) > 12 else 'Unknown',    # company\n                'Candidate': eval_row[14] if len(eval_row) > 14 else 'Unknown',  # candidate_name\n                'Score': eval_row[3],       # relevance_score\n                'Verdict': eval_row[6],     # verdict\n                'Date': eval_row[10]        # evaluated_at\n            })\n        \n        df = pd.DataFrame(eval_data)\n        st.dataframe(df, use_container_width=True)\n    else:\n        st.info(\"No evaluations found. Start by uploading job descriptions and resumes!\")\n\ndef render_job_upload_page():\n    \"\"\"Render job description upload page with PDF/DOCX import support\"\"\"\n    st.header(\"üìù Upload Job Description\")\n    \n    # File upload section\n    uploaded_file = st.file_uploader(\n        \"Upload Job Description File\",\n        type=['pdf', 'docx', 'doc'],\n        help=\"Supported formats: PDF, DOCX, DOC\"\n    )\n    \n    # Initialize variables\n    extracted_text = \"\"\n    prefilled_title = \"\"\n    prefilled_company = \"\"\n    prefilled_location = \"\"\n    \n    # Process uploaded file if present\n    if uploaded_file:\n        with st.spinner(\"Extracting text from document...\"):\n            file_bytes = uploaded_file.read()\n            filename = uploaded_file.name\n            file_extension = filename.lower().split('.')[-1]\n            \n            # Extract text based on file type\n            if file_extension == 'pdf':\n                from text_extractor import extract_text_from_pdf\n                extracted_text = extract_text_from_pdf(file_bytes)\n            elif file_extension in ['docx', 'doc']:\n                from text_extractor import extract_text_from_docx\n                extracted_text = extract_text_from_docx(file_bytes)\n            \n            if not extracted_text:\n                st.error(\"Could not extract text from the file. Please check the file format and try again.\")\n                return\n            \n            # Smart prefilling using regex patterns\n            import re\n            text_lines = extracted_text.split('\\n')\n            \n            # Extract Job Title - look for first non-empty line ‚â§ 100 chars\n            for line in text_lines[:5]:\n                line = line.strip()\n                if line and len(line) <= 100 and not any(keyword in line.lower() for keyword in ['company', 'location', 'description', 'requirements']):\n                    # Remove common prefixes\n                    line = re.sub(r'^(job\\s*title\\s*[:-]\\s*|position\\s*[:-]\\s*|role\\s*[:-]\\s*)', '', line, flags=re.IGNORECASE).strip()\n                    if line:\n                        prefilled_title = line\n                        break\n            \n            # Extract Company - look for patterns\n            company_pattern = r'(?:company|organization|employer|firm)\\s*[:-]\\s*(.+?)(?:\\n|$)'\n            company_match = re.search(company_pattern, extracted_text, re.IGNORECASE | re.MULTILINE)\n            if company_match:\n                prefilled_company = company_match.group(1).strip()[:50]  # Limit length\n            \n            # Extract Location - look for patterns\n            location_pattern = r'(?:location|place|office|city)\\s*[:-]\\s*(.+?)(?:\\n|$)'\n            location_match = re.search(location_pattern, extracted_text, re.IGNORECASE | re.MULTILINE)\n            if location_match:\n                prefilled_location = location_match.group(1).strip()[:50]  # Limit length\n            \n            st.success(f\"‚úÖ Text extracted successfully! ({len(extracted_text)} characters)\")\n    \n    # Form with extracted/prefilled data\n    with st.form(\"job_upload_form\"):\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            job_title = st.text_input(\"Job Title*\", value=prefilled_title, placeholder=\"e.g., Senior Data Scientist\")\n            company = st.text_input(\"Company\", value=prefilled_company, placeholder=\"e.g., Tech Corp\")\n        \n        with col2:\n            location = st.text_input(\"Location\", value=prefilled_location, placeholder=\"e.g., Hyderabad, India\")\n        \n        # Show extracted text in editable area\n        job_description = st.text_area(\n            \"Job Description*\", \n            value=extracted_text,\n            height=300, \n            placeholder=\"Upload a PDF/DOCX file above or paste the complete job description here...\"\n        )\n        \n        submitted = st.form_submit_button(\"Parse and Save Job Description\")\n        \n        if submitted:\n            if not job_title or not job_description:\n                st.error(\"Please fill in all required fields (marked with *). Make sure to upload a file or enter job description text.\")\n            else:\n                with st.spinner(\"Parsing job description...\"):\n                    # Parse job description using NLP\n                    parsed_requirements = parse_job_description(job_description)\n                    \n                    # Save to database\n                    job_id = save_job_description(\n                        job_title, company, location, job_description,\n                        parsed_requirements.get('required_skills', []),\n                        parsed_requirements.get('preferred_skills', []),\n                        {\n                            'experience_required': parsed_requirements.get('experience_required', 0),\n                            'education_required': parsed_requirements.get('education_required', 'unknown'),\n                            'key_responsibilities': parsed_requirements.get('key_responsibilities', [])\n                        }\n                    )\n                \n                st.success(f\"‚úÖ Job description saved successfully! (ID: {job_id})\")\n                \n                # Display parsed information\n                st.subheader(\"Parsed Job Requirements\")\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    st.write(\"**Required Skills:**\")\n                    for skill in parsed_requirements.get('required_skills', []):\n                        st.write(f\"‚Ä¢ {skill}\")\n                \n                with col2:\n                    st.write(\"**Preferred Skills:**\")\n                    for skill in parsed_requirements.get('preferred_skills', []):\n                        st.write(f\"‚Ä¢ {skill}\")\n                \n                st.write(f\"**Experience Required:** {parsed_requirements.get('experience_required', 0)} years\")\n                st.write(f\"**Education Required:** {parsed_requirements.get('education_required', 'Not specified')}\")\n\ndef render_resume_upload_page():\n    \"\"\"Render resume upload page\"\"\"\n    st.header(\"üìÑ Upload Resume\")\n    \n    # Job selection\n    jobs = get_job_descriptions()\n    if not jobs:\n        st.warning(\"Please upload at least one job description first!\")\n        return\n    \n    job_options = {f\"{job[1]} - {job[2] or 'Company Not Specified'}\": job[0] for job in jobs}\n    selected_job = st.selectbox(\"Select Job Position\", options=list(job_options.keys()))\n    \n    if selected_job:\n        job_id = job_options[selected_job]\n        \n        # File upload\n        uploaded_file = st.file_uploader(\n            \"Upload Resume\", \n            type=['pdf', 'docx', 'doc'],\n            help=\"Supported formats: PDF, DOCX, DOC\"\n        )\n        \n        if uploaded_file:\n            with st.spinner(\"Processing resume...\"):\n                # Extract text and information from resume\n                resume_data = process_uploaded_file(uploaded_file)\n                \n                if resume_data:\n                    # Display extracted information\n                    st.subheader(\"Extracted Information\")\n                    \n                    col1, col2 = st.columns(2)\n                    with col1:\n                        st.write(\"**Contact Information:**\")\n                        st.write(f\"Name: {resume_data['contact_info'].get('name', 'Not found')}\")\n                        st.write(f\"Email: {resume_data['contact_info'].get('email', 'Not found')}\")\n                        st.write(f\"Phone: {resume_data['contact_info'].get('phone', 'Not found')}\")\n                    \n                    with col2:\n                        st.write(\"**File Information:**\")\n                        st.write(f\"Filename: {resume_data['filename']}\")\n                        st.write(f\"Text Length: {len(resume_data['extracted_text'])} characters\")\n                    \n                    # Show extracted text preview\n                    with st.expander(\"Preview Extracted Text\"):\n                        st.text_area(\"Extracted Text\", resume_data['extracted_text'][:1000] + \"...\" if len(resume_data['extracted_text']) > 1000 else resume_data['extracted_text'], height=200)\n                    \n                    # Save and evaluate buttons\n                    col1, col2 = st.columns(2)\n                    \n                    with col1:\n                        if st.button(\"Save Resume\", type=\"primary\"):\n                            # Save resume to database\n                            resume_id = save_resume(\n                                resume_data['filename'],\n                                resume_data['contact_info'].get('name', ''),\n                                resume_data['contact_info'].get('email', ''),\n                                resume_data['extracted_text'],\n                                resume_data['sections'].get('skills', ''),\n                                resume_data['sections'].get('experience', ''),\n                                resume_data['sections'].get('education', '')\n                            )\n                            st.success(f\"‚úÖ Resume saved successfully! (ID: {resume_id})\")\n                    \n                    with col2:\n                        if st.button(\"Evaluate Resume\", type=\"secondary\"):\n                            # Get job details\n                            job_details = get_job_by_id(job_id)\n                            if job_details:\n                                with st.spinner(\"Evaluating resume...\"):\n                                    # Prepare job requirements - handle both string and already parsed data\n                                    required_skills = job_details[5] if job_details[5] else []\n                                    if isinstance(required_skills, str):\n                                        required_skills = json.loads(required_skills)\n                                    \n                                    preferred_skills = job_details[6] if job_details[6] else []\n                                    if isinstance(preferred_skills, str):\n                                        preferred_skills = json.loads(preferred_skills)\n                                    \n                                    qualifications = job_details[7] if job_details[7] else {}\n                                    if isinstance(qualifications, str):\n                                        qualifications = json.loads(qualifications)\n                                    \n                                    job_requirements = {\n                                        'required_skills': required_skills,\n                                        'preferred_skills': preferred_skills,\n                                        'experience_required': qualifications.get('experience_required', 0) if isinstance(qualifications, dict) else 0,\n                                        'education_required': qualifications.get('education_required', 'unknown') if isinstance(qualifications, dict) else 'unknown'\n                                    }\n                                    \n                                    # Initialize scorer and evaluate\n                                    scorer = ResumeScorer()\n                                    evaluation_result = scorer.evaluate_resume(\n                                        resume_data, job_details[3], job_requirements\n                                    )\n                                    \n                                    # Display evaluation results\n                                    display_evaluation_results(evaluation_result)\n\ndef render_batch_evaluation_page():\n    \"\"\"Render batch evaluation page\"\"\"\n    st.header(\"üîÑ Batch Evaluation\")\n    \n    # Job selection\n    jobs = get_job_descriptions()\n    if not jobs:\n        st.warning(\"Please upload at least one job description first!\")\n        return\n    \n    job_options = {f\"{job[1]} - {job[2] or 'Company Not Specified'}\": job[0] for job in jobs}\n    selected_job = st.selectbox(\"Select Job Position for Batch Evaluation\", options=list(job_options.keys()))\n    \n    if selected_job:\n        job_id = job_options[selected_job]\n        \n        # Get all resumes\n        resumes = get_resumes()\n        if not resumes:\n            st.warning(\"No resumes found. Please upload some resumes first!\")\n            return\n        \n        st.write(f\"Found {len(resumes)} resumes to evaluate\")\n        \n        if st.button(\"Start Batch Evaluation\", type=\"primary\"):\n            # Get job details\n            job_details = get_job_by_id(job_id)\n            if job_details:\n                job_requirements = {\n                    'required_skills': json.loads(job_details[5]) if job_details[5] else [],\n                    'preferred_skills': json.loads(job_details[6]) if job_details[6] else [],\n                    'experience_required': json.loads(job_details[7]).get('experience_required', 0) if job_details[7] else 0,\n                    'education_required': json.loads(job_details[7]).get('education_required', 'unknown') if job_details[7] else 'unknown'\n                }\n                \n                # Initialize scorer\n                scorer = ResumeScorer()\n                \n                # Progress bar\n                progress_bar = st.progress(0)\n                status_text = st.empty()\n                \n                evaluated_count = 0\n                total_resumes = len(resumes)\n                \n                for i, resume in enumerate(resumes):\n                    status_text.text(f\"Evaluating resume {i+1}/{total_resumes}: {resume[1]}\")\n                    \n                    # Prepare resume data\n                    resume_data = {\n                        'filename': resume[1],\n                        'extracted_text': resume[3],\n                        'contact_info': {\n                            'name': resume[2],\n                            'email': resume[3]\n                        },\n                        'sections': {\n                            'skills': json.loads(resume[4]) if resume[4] else '',\n                            'experience': json.loads(resume[5]) if resume[5] else '',\n                            'education': json.loads(resume[6]) if resume[6] else ''\n                        }\n                    }\n                    \n                    # Evaluate resume\n                    evaluation_result = scorer.evaluate_resume(\n                        resume_data, job_details[3], job_requirements\n                    )\n                    \n                    # Save evaluation\n                    save_evaluation(\n                        job_id, resume[0],\n                        evaluation_result['relevance_score'],\n                        evaluation_result['hard_match_score'],\n                        evaluation_result['semantic_match_score'],\n                        evaluation_result['verdict'],\n                        evaluation_result['missing_skills'],\n                        evaluation_result['improvement_suggestions'],\n                        evaluation_result['evaluation_details']\n                    )\n                    \n                    evaluated_count += 1\n                    progress_bar.progress(evaluated_count / total_resumes)\n                \n                status_text.text(\"‚úÖ Batch evaluation completed!\")\n                st.success(f\"Successfully evaluated {evaluated_count} resumes!\")\n    \n    # Display existing evaluations\n    st.subheader(\"Evaluation Results\")\n    \n    # Filters\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        job_filter = st.selectbox(\"Filter by Job\", [\"All\"] + list(job_options.keys()))\n    \n    with col2:\n        min_score = st.slider(\"Minimum Score\", 0, 100, 0)\n    \n    with col3:\n        verdict_filter = st.selectbox(\"Filter by Verdict\", [\"All\", \"High\", \"Medium\", \"Low\"])\n    \n    # Get filtered evaluations\n    job_id_filter = job_options.get(job_filter) if job_filter != \"All\" else None\n    verdict_filter = verdict_filter if verdict_filter != \"All\" else None\n    \n    evaluations = get_evaluations(job_id_filter, min_score, verdict_filter)\n    \n    if evaluations:\n        # Create DataFrame for display\n        eval_data = []\n        for eval_row in evaluations:\n            eval_data.append({\n                'ID': eval_row[0],\n                'Job Title': eval_row[11] if len(eval_row) > 11 else 'Unknown',\n                'Company': eval_row[12] if len(eval_row) > 12 else 'Unknown',\n                'Candidate': eval_row[14] if len(eval_row) > 14 else 'Unknown',\n                'Email': eval_row[15] if len(eval_row) > 15 else 'Not provided',\n                'Score': eval_row[3],\n                'Verdict': eval_row[6],\n                'Date': eval_row[10]\n            })\n        \n        df = pd.DataFrame(eval_data)\n        \n        # Display results\n        st.dataframe(df, use_container_width=True)\n        \n        # Export functionality\n        if st.button(\"Export Results to CSV\"):\n            csv = df.to_csv(index=False)\n            st.download_button(\n                label=\"Download CSV\",\n                data=csv,\n                file_name=f\"evaluations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n                mime=\"text/csv\"\n            )\n    else:\n        st.info(\"No evaluations found with the selected filters.\")\n\ndef render_analytics_page():\n    \"\"\"Render analytics and insights page\"\"\"\n    st.header(\"üìà Analytics & Insights\")\n    \n    # Get all evaluations\n    evaluations = get_evaluations()\n    \n    if not evaluations:\n        st.info(\"No evaluation data available yet. Complete some evaluations to see analytics.\")\n        return\n    \n    # Create DataFrame\n    eval_data = []\n    for eval_row in evaluations:\n        eval_data.append({\n            'job_title': eval_row[14],\n            'company': eval_row[15],\n            'candidate_name': eval_row[17],\n            'relevance_score': eval_row[3],\n            'hard_match_score': eval_row[4],\n            'semantic_match_score': eval_row[5],\n            'verdict': eval_row[6],\n            'evaluated_at': eval_row[10]\n        })\n    \n    df = pd.DataFrame(eval_data)\n    \n    # Score distribution\n    st.subheader(\"Score Distribution\")\n    fig_hist = px.histogram(df, x='relevance_score', nbins=20, title=\"Distribution of Relevance Scores\")\n    st.plotly_chart(fig_hist, use_container_width=True)\n    \n    # Score comparison by job\n    st.subheader(\"Average Scores by Job Position\")\n    job_scores = df.groupby('job_title').agg({\n        'relevance_score': 'mean',\n        'hard_match_score': 'mean',\n        'semantic_match_score': 'mean'\n    }).round(2).reset_index()\n    \n    fig_bar = px.bar(job_scores, x='job_title', y='relevance_score', \n                     title=\"Average Relevance Score by Job Position\")\n    fig_bar.update_xaxes(tickangle=45)\n    st.plotly_chart(fig_bar, use_container_width=True)\n    \n    # Verdict distribution over time\n    st.subheader(\"Verdict Trends Over Time\")\n    df['date'] = pd.to_datetime(df['evaluated_at']).dt.date\n    verdict_trends = df.groupby(['date', 'verdict']).size().reset_index(name='count')\n    \n    fig_line = px.line(verdict_trends, x='date', y='count', color='verdict',\n                       title=\"Verdict Distribution Over Time\")\n    st.plotly_chart(fig_line, use_container_width=True)\n    \n    # Summary statistics\n    st.subheader(\"Summary Statistics\")\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.metric(\"Average Relevance Score\", f\"{df['relevance_score'].mean():.1f}\")\n        st.metric(\"Highest Score\", f\"{df['relevance_score'].max():.1f}\")\n        st.metric(\"Total Candidates Evaluated\", len(df))\n    \n    with col2:\n        st.metric(\"Average Hard Match Score\", f\"{df['hard_match_score'].mean():.1f}\")\n        st.metric(\"Average Semantic Score\", f\"{df['semantic_match_score'].mean():.1f}\")\n        high_potential = len(df[df['verdict'] == 'High'])\n        st.metric(\"High Potential Candidates\", high_potential)\n\ndef display_evaluation_results(evaluation_result):\n    \"\"\"Display evaluation results in a formatted way\"\"\"\n    st.subheader(\"üéØ Evaluation Results\")\n    \n    # Main scores\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"Relevance Score\", f\"{evaluation_result['relevance_score']}/100\")\n    \n    with col2:\n        st.metric(\"Hard Match Score\", f\"{evaluation_result['hard_match_score']:.1f}/100\")\n    \n    with col3:\n        st.metric(\"Semantic Score\", f\"{evaluation_result['semantic_match_score']:.1f}/100\")\n    \n    with col4:\n        verdict_color = {\"High\": \"üü¢\", \"Medium\": \"üü°\", \"Low\": \"üî¥\"}\n        st.metric(\"Verdict\", f\"{verdict_color.get(evaluation_result['verdict'], '‚ö™')} {evaluation_result['verdict']}\")\n    \n    # Score breakdown chart\n    scores_data = {\n        'Score Type': ['Hard Match', 'Semantic Match'],\n        'Score': [evaluation_result['hard_match_score'], evaluation_result['semantic_match_score']]\n    }\n    fig = px.bar(scores_data, x='Score Type', y='Score', title=\"Score Breakdown\")\n    fig.update_layout(yaxis=dict(range=[0, 100]))\n    st.plotly_chart(fig, use_container_width=True)\n    \n    # Missing skills\n    if evaluation_result['missing_skills']:\n        st.subheader(\"‚ùå Missing Skills\")\n        missing_skills_text = \", \".join(evaluation_result['missing_skills'])\n        st.error(f\"Missing: {missing_skills_text}\")\n    \n    # Improvement suggestions\n    if evaluation_result['improvement_suggestions']:\n        st.subheader(\"üí° Improvement Suggestions\")\n        for i, suggestion in enumerate(evaluation_result['improvement_suggestions'], 1):\n            st.write(f\"{i}. {suggestion}\")\n    \n    # Detailed analysis\n    with st.expander(\"Detailed Analysis\"):\n        st.json(evaluation_result['evaluation_details'])\n\ndef render_student_portal():\n    \"\"\"Render student portal for viewing evaluation feedback\"\"\"\n    st.header(\"üéì Student Portal\")\n    st.markdown(\"View your resume evaluation feedback and improvement suggestions\")\n    \n    # Student email input for finding their evaluations\n    st.subheader(\"Find Your Evaluation Results\")\n    student_email = st.text_input(\"Enter your email address:\", placeholder=\"your.email@example.com\")\n    \n    if student_email:\n        # Get evaluations for this student\n        all_evaluations = get_evaluations()\n        student_evaluations = []\n        \n        for eval_row in all_evaluations:\n            # Check if email matches (index 15 for candidate_email)\n            if len(eval_row) > 15 and eval_row[15] and eval_row[15].lower() == student_email.lower():\n                student_evaluations.append(eval_row)\n        \n        if student_evaluations:\n            st.success(f\"Found {len(student_evaluations)} evaluation(s) for {student_email}\")\n            \n            for i, eval_row in enumerate(student_evaluations, 1):\n                with st.expander(f\"Evaluation #{i} - {eval_row[11] if len(eval_row) > 11 else 'Unknown Job'} at {eval_row[12] if len(eval_row) > 12 else 'Unknown Company'}\"):\n                    \n                    # Basic evaluation info\n                    col1, col2, col3 = st.columns(3)\n                    \n                    with col1:\n                        score = eval_row[3]\n                        verdict = eval_row[6]\n                        verdict_color = {\"High\": \"üü¢\", \"Medium\": \"üü°\", \"Low\": \"üî¥\"}.get(verdict, \"‚ö™\")\n                        st.metric(\"Overall Score\", f\"{score}/100\")\n                        st.write(f\"**Verdict:** {verdict_color} {verdict}\")\n                    \n                    with col2:\n                        st.metric(\"Technical Match\", f\"{eval_row[4]:.1f}/100\")\n                        st.metric(\"Semantic Match\", f\"{eval_row[5]:.1f}/100\")\n                    \n                    with col3:\n                        st.write(f\"**Job:** {eval_row[11] if len(eval_row) > 11 else 'Unknown'}\")\n                        st.write(f\"**Company:** {eval_row[12] if len(eval_row) > 12 else 'Unknown'}\")\n                        st.write(f\"**Evaluated:** {eval_row[10]}\")\n                    \n                    # Missing skills\n                    if eval_row[7]:  # missing_skills\n                        try:\n                            missing_skills = json.loads(eval_row[7])\n                            if missing_skills:\n                                st.subheader(\"üìã Skills to Develop\")\n                                skills_text = \", \".join(missing_skills)\n                                st.error(f\"Consider learning: {skills_text}\")\n                        except:\n                            pass\n                    \n                    # Improvement suggestions\n                    if eval_row[8]:  # improvement_suggestions\n                        try:\n                            suggestions = json.loads(eval_row[8])\n                            if suggestions:\n                                st.subheader(\"üí° Improvement Suggestions\")\n                                for j, suggestion in enumerate(suggestions, 1):\n                                    st.write(f\"{j}. {suggestion}\")\n                        except:\n                            pass\n                    \n                    # Detailed analysis\n                    if eval_row[9]:  # evaluation_details\n                        with st.expander(\"üìä Detailed Analysis\"):\n                            try:\n                                details = json.loads(eval_row[9])\n                                \n                                # Skills analysis\n                                if 'hard_match_details' in details and 'skills' in details['hard_match_details']:\n                                    skills_info = details['hard_match_details']['skills']\n                                    st.write(\"**Skills Match Analysis:**\")\n                                    st.write(f\"‚Ä¢ Required skills matched: {skills_info.get('matched_required', 0)}/{skills_info.get('total_required', 0)}\")\n                                    st.write(f\"‚Ä¢ Preferred skills matched: {skills_info.get('matched_preferred', 0)}/{skills_info.get('total_preferred', 0)}\")\n                                \n                                # Experience analysis\n                                if 'hard_match_details' in details and 'experience' in details['hard_match_details']:\n                                    exp_info = details['hard_match_details']['experience']\n                                    st.write(\"**Experience Analysis:**\")\n                                    st.write(f\"‚Ä¢ Your experience: {exp_info.get('resume_years', 0)} years\")\n                                    st.write(f\"‚Ä¢ Required experience: {exp_info.get('required_years', 0)} years\")\n                                \n                                # Education analysis\n                                if 'hard_match_details' in details and 'education' in details['hard_match_details']:\n                                    edu_info = details['hard_match_details']['education']\n                                    st.write(\"**Education Analysis:**\")\n                                    st.write(f\"‚Ä¢ Your education: {edu_info.get('resume_level', 'Unknown').title()}\")\n                                    st.write(f\"‚Ä¢ Required education: {edu_info.get('required_level', 'Unknown').title()}\")\n                                \n                            except:\n                                st.write(\"Detailed analysis data not available\")\n        else:\n            st.info(\"No evaluations found for this email address. Make sure you've submitted your resume for evaluation.\")\n    \n    # General improvement tips\n    st.markdown(\"---\")\n    st.subheader(\"üíº General Career Tips\")\n    \n    tips_col1, tips_col2 = st.columns(2)\n    \n    with tips_col1:\n        st.markdown(\"\"\"\n        **Technical Skills:**\n        - Keep your technical skills updated with industry trends\n        - Add specific project examples that demonstrate your skills\n        - Include metrics and achievements in your experience\n        - Consider getting relevant certifications\n        \"\"\")\n    \n    with tips_col2:\n        st.markdown(\"\"\"\n        **Resume Tips:**\n        - Use clear, professional formatting\n        - Quantify your achievements with numbers\n        - Tailor your resume for each job application\n        - Keep it concise but comprehensive\n        \"\"\")\n    \n    # Contact information\n    st.markdown(\"---\")\n    st.info(\"üí¨ **Need help?** Contact your placement coordinator for personalized guidance on improving your profile.\")\n\ndef render_advanced_analytics():\n    \"\"\"Render advanced analytics and insights\"\"\"\n    st.header(\"üìà Advanced Analytics & Insights\")\n    st.markdown(\"Deep insights and reporting for placement team decision making\")\n    \n    # Get all data\n    evaluations = get_evaluations()\n    jobs = get_job_descriptions()\n    resumes = get_resumes()\n    \n    if not evaluations:\n        st.info(\"No evaluation data available yet. Complete some evaluations to see advanced analytics.\")\n        return\n    \n    # Create comprehensive DataFrame\n    eval_data = []\n    for eval_row in evaluations:\n        eval_data.append({\n            'evaluation_id': eval_row[0],\n            'job_id': eval_row[1],\n            'resume_id': eval_row[2],\n            'relevance_score': eval_row[3],\n            'hard_match_score': eval_row[4],\n            'semantic_match_score': eval_row[5],\n            'verdict': eval_row[6],\n            'job_title': eval_row[11] if len(eval_row) > 11 else 'Unknown',\n            'company': eval_row[12] if len(eval_row) > 12 else 'Unknown',\n            'candidate_name': eval_row[14] if len(eval_row) > 14 else 'Unknown',\n            'candidate_email': eval_row[15] if len(eval_row) > 15 else 'Unknown',\n            'evaluated_at': eval_row[10]\n        })\n    \n    df = pd.DataFrame(eval_data)\n    \n    # Advanced metrics dashboard\n    st.subheader(\"üéØ Key Performance Indicators\")\n    \n    col1, col2, col3, col4, col5 = st.columns(5)\n    \n    with col1:\n        avg_score = df['relevance_score'].mean()\n        st.metric(\"Average Score\", f\"{avg_score:.1f}\", f\"{avg_score-70:.1f}\" if avg_score > 70 else f\"+{70-avg_score:.1f}\")\n    \n    with col2:\n        high_quality = len(df[df['relevance_score'] >= 80])\n        st.metric(\"High Quality Candidates\", high_quality, f\"{(high_quality/len(df)*100):.1f}%\")\n    \n    with col3:\n        avg_hard_match = df['hard_match_score'].mean()\n        st.metric(\"Avg Technical Match\", f\"{avg_hard_match:.1f}\")\n    \n    with col4:\n        avg_semantic = df['semantic_match_score'].mean()\n        st.metric(\"Avg Semantic Match\", f\"{avg_semantic:.1f}\")\n    \n    with col5:\n        total_evaluations = len(df)\n        st.metric(\"Total Evaluations\", total_evaluations)\n    \n    # Performance by location analysis\n    st.subheader(\"üåç Performance by Location\")\n    if 'location' in df.columns:\n        location_stats = df.groupby('location').agg({\n            'relevance_score': ['mean', 'count'],\n            'verdict': lambda x: (x == 'High').sum()\n        }).round(2)\n        \n        location_stats.columns = ['Avg Score', 'Total Evaluations', 'High Potential']\n        st.dataframe(location_stats, use_container_width=True)\n    \n    # Advanced score distribution analysis\n    st.subheader(\"üìä Score Distribution Analysis\")\n    \n    # Score distribution by verdict\n    fig_box = px.box(df, x='verdict', y='relevance_score', \n                     title=\"Score Distribution by Verdict\",\n                     color='verdict',\n                     color_discrete_map={'High': '#28a745', 'Medium': '#ffc107', 'Low': '#dc3545'})\n    st.plotly_chart(fig_box, use_container_width=True)\n    \n    # Time series analysis\n    st.subheader(\"üìÖ Evaluation Trends Over Time\")\n    df['date'] = pd.to_datetime(df['evaluated_at']).dt.date\n    \n    # Daily evaluation counts and average scores\n    daily_stats = df.groupby('date').agg({\n        'relevance_score': ['mean', 'count'],\n        'verdict': lambda x: (x == 'High').sum()\n    }).reset_index()\n    \n    daily_stats.columns = ['date', 'avg_score', 'total_evals', 'high_potential']\n    \n    fig_time = px.line(daily_stats, x='date', y='avg_score', \n                       title=\"Average Scores Over Time\",\n                       markers=True)\n    fig_time.add_scatter(x=daily_stats['date'], y=daily_stats['total_evals'], \n                        mode='lines+markers', name='Daily Evaluations', yaxis='y2')\n    \n    fig_time.update_layout(yaxis2=dict(title=\"Number of Evaluations\", overlaying='y', side='right'))\n    st.plotly_chart(fig_time, use_container_width=True)\n    \n    # Job performance comparison\n    st.subheader(\"üíº Job Position Analysis\")\n    \n    job_performance = df.groupby('job_title').agg({\n        'relevance_score': ['mean', 'std', 'count'],\n        'hard_match_score': 'mean',\n        'semantic_match_score': 'mean',\n        'verdict': lambda x: (x == 'High').sum()\n    }).round(2)\n    \n    job_performance.columns = ['Avg Score', 'Score StdDev', 'Total Candidates', 'Avg Hard Match', 'Avg Semantic', 'High Potential']\n    job_performance = job_performance.sort_values(by='Avg Score', ascending=False)\n    \n    st.dataframe(job_performance, use_container_width=True)\n    \n    # Candidate quality heatmap\n    st.subheader(\"üî• Quality Heatmap\")\n    \n    # Create score bins\n    df['score_bin'] = pd.cut(df['relevance_score'], bins=[0, 50, 70, 85, 100], \n                            labels=['Poor (0-50)', 'Fair (50-70)', 'Good (70-85)', 'Excellent (85-100)'])\n    \n    heatmap_data = df.groupby(['job_title', 'score_bin']).size().unstack(fill_value=0)\n    \n    fig_heatmap = px.imshow(heatmap_data.values, \n                           x=heatmap_data.columns, \n                           y=heatmap_data.index,\n                           title=\"Candidate Quality Distribution by Job Position\",\n                           aspect=\"auto\")\n    st.plotly_chart(fig_heatmap, use_container_width=True)\n    \n    # Skills gap analysis\n    st.subheader(\"üéØ Skills Gap Analysis\")\n    \n    try:\n        # Analyze missing skills from evaluations\n        missing_skills_data = []\n        for eval_row in evaluations:\n            if eval_row[7]:  # missing_skills column\n                try:\n                    missing_skills = json.loads(eval_row[7])\n                    job_title = eval_row[11] if len(eval_row) > 11 else 'Unknown'\n                    for skill in missing_skills:\n                        missing_skills_data.append({\n                            'job_title': job_title,\n                            'skill': skill,\n                            'evaluation_id': eval_row[0]\n                        })\n                except:\n                    pass\n        \n        if missing_skills_data:\n            skills_df = pd.DataFrame(missing_skills_data)\n            \n            # Most commonly missing skills\n            top_missing = skills_df['skill'].value_counts().head(10)\n            \n            fig_skills = px.bar(x=top_missing.index, y=top_missing.values,\n                               title=\"Top 10 Most Commonly Missing Skills\",\n                               labels={'x': 'Skills', 'y': 'Frequency'})\n            st.plotly_chart(fig_skills, use_container_width=True)\n            \n            # Skills gap by job position\n            skills_by_job = skills_df.groupby(['job_title', 'skill']).size().reset_index(name='count')\n            skills_pivot = skills_by_job.pivot(index='job_title', columns='skill', values='count').fillna(0)\n            \n            st.subheader(\"Missing Skills by Job Position\")\n            st.dataframe(skills_pivot, use_container_width=True)\n    \n    except Exception as e:\n        st.info(\"Skills gap analysis not available - no sufficient data\")\n    \n    # Recommendations engine\n    st.subheader(\"ü§ñ AI Recommendations\")\n    \n    recommendations = []\n    \n    # Score-based recommendations\n    if avg_score < 65:\n        recommendations.append(\"üìâ **Low Average Score Alert**: Consider revising job requirements or improving candidate screening process.\")\n    \n    if df['hard_match_score'].mean() < df['semantic_match_score'].mean() - 10:\n        recommendations.append(\"üîß **Skills Mismatch**: Candidates have good overall profiles but lack specific technical skills. Consider skills training programs.\")\n    \n    if len(df[df['verdict'] == 'High']) / len(df) < 0.2:\n        recommendations.append(\"üéØ **Low High-Potential Rate**: Only {:.1%} of candidates are high-potential. Review sourcing strategies.\".format(len(df[df['verdict'] == 'High']) / len(df)))\n    \n    # Time-based recommendations\n    if len(daily_stats) > 7:\n        recent_avg = daily_stats.tail(3)['avg_score'].mean()\n        older_avg = daily_stats.iloc[:-3]['avg_score'].mean()\n        \n        if recent_avg < older_avg - 5:\n            recommendations.append(\"üìâ **Declining Quality Trend**: Recent candidates show lower scores. Review current sourcing channels.\")\n        elif recent_avg > older_avg + 5:\n            recommendations.append(\"üìà **Improving Quality Trend**: Recent candidates show higher scores. Current sourcing strategy is working well.\")\n    \n    if recommendations:\n        for rec in recommendations:\n            st.info(rec)\n    else:\n        st.success(\"‚úÖ **Performance is Good**: No major issues detected in current evaluation patterns.\")\n    \n    # Export comprehensive report\n    st.subheader(\"üìÑ Export Reports\")\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        if st.button(\"üìä Export Analytics Report\"):\n            report_data = {\n                'summary_stats': {\n                    'total_evaluations': len(df),\n                    'average_score': avg_score,\n                    'high_potential_rate': len(df[df['verdict'] == 'High']) / len(df),\n                    'avg_hard_match': avg_hard_match,\n                    'avg_semantic_match': avg_semantic\n                },\n                'job_performance': job_performance.to_dict(),\n                'daily_trends': daily_stats.to_dict('records'),\n                'recommendations': recommendations\n            }\n            \n            report_json = json.dumps(report_data, indent=2, default=str)\n            st.download_button(\n                label=\"üì• Download Analytics Report (JSON)\",\n                data=report_json,\n                file_name=f\"analytics_report_{time.strftime('%Y%m%d_%H%M%S')}.json\",\n                mime=\"application/json\"\n            )\n    \n    with col2:\n        if st.button(\"üìã Export Evaluation Data\"):\n            csv_data = df.to_csv(index=False)\n            st.download_button(\n                label=\"üì• Download Raw Data (CSV)\",\n                data=csv_data,\n                file_name=f\"evaluation_data_{time.strftime('%Y%m%d_%H%M%S')}.csv\",\n                mime=\"text/csv\"\n            )\n    \n    with col3:\n        if st.button(\"üéØ Export High Potential List\"):\n            high_potential = df[df['verdict'] == 'High'][['candidate_name', 'candidate_email', 'job_title', 'company', 'relevance_score']].sort_values(by='relevance_score', ascending=False)\n            high_potential_csv = high_potential.to_csv(index=False)\n            st.download_button(\n                label=\"üì• Download High Potential (CSV)\",\n                data=high_potential_csv,\n                file_name=f\"high_potential_candidates_{time.strftime('%Y%m%d_%H%M%S')}.csv\",\n                mime=\"text/csv\"\n            )\n","size_bytes":43363},"database.py":{"content":"import sqlite3\nimport json\nfrom datetime import datetime\nimport streamlit as st\n\nDATABASE_PATH = \"resume_evaluation.db\"\n\ndef get_connection():\n    \"\"\"Get database connection\"\"\"\n    return sqlite3.connect(DATABASE_PATH)\n\ndef init_database():\n    \"\"\"Initialize database with required tables\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    # Job descriptions table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS job_descriptions (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            title TEXT NOT NULL,\n            company TEXT,\n            location TEXT,\n            description TEXT NOT NULL,\n            required_skills TEXT,\n            preferred_skills TEXT,\n            qualifications TEXT,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    ''')\n    \n    # Resumes table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS resumes (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            filename TEXT NOT NULL,\n            candidate_name TEXT,\n            candidate_email TEXT,\n            extracted_text TEXT NOT NULL,\n            skills TEXT,\n            experience TEXT,\n            education TEXT,\n            uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    ''')\n    \n    # Evaluations table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS evaluations (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            job_id INTEGER NOT NULL,\n            resume_id INTEGER NOT NULL,\n            relevance_score REAL NOT NULL,\n            hard_match_score REAL NOT NULL,\n            semantic_match_score REAL NOT NULL,\n            verdict TEXT NOT NULL,\n            missing_skills TEXT,\n            improvement_suggestions TEXT,\n            evaluation_details TEXT,\n            evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (job_id) REFERENCES job_descriptions (id),\n            FOREIGN KEY (resume_id) REFERENCES resumes (id)\n        )\n    ''')\n    \n    conn.commit()\n    conn.close()\n\ndef save_job_description(title, company, location, description, required_skills, preferred_skills, qualifications):\n    \"\"\"Save job description to database\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('''\n        INSERT INTO job_descriptions (title, company, location, description, required_skills, preferred_skills, qualifications)\n        VALUES (?, ?, ?, ?, ?, ?, ?)\n    ''', (title, company, location, description, json.dumps(required_skills), json.dumps(preferred_skills), json.dumps(qualifications)))\n    \n    job_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    return job_id\n\ndef save_resume(filename, candidate_name, candidate_email, extracted_text, skills, experience, education):\n    \"\"\"Save resume to database\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('''\n        INSERT INTO resumes (filename, candidate_name, candidate_email, extracted_text, skills, experience, education)\n        VALUES (?, ?, ?, ?, ?, ?, ?)\n    ''', (filename, candidate_name, candidate_email, extracted_text, json.dumps(skills), json.dumps(experience), json.dumps(education)))\n    \n    resume_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    return resume_id\n\ndef save_evaluation(job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details):\n    \"\"\"Save evaluation results to database\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('''\n        INSERT INTO evaluations (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n    ''', (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, json.dumps(missing_skills), json.dumps(improvement_suggestions), json.dumps(evaluation_details)))\n    \n    evaluation_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    return evaluation_id\n\ndef get_job_descriptions():\n    \"\"\"Get all job descriptions\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM job_descriptions ORDER BY created_at DESC')\n    jobs = cursor.fetchall()\n    conn.close()\n    \n    return jobs\n\ndef get_resumes():\n    \"\"\"Get all resumes\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM resumes ORDER BY uploaded_at DESC')\n    resumes = cursor.fetchall()\n    conn.close()\n    \n    return resumes\n\ndef get_evaluations(job_id=None, min_score=None, verdict=None):\n    \"\"\"Get evaluations with optional filters\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    query = '''\n        SELECT e.*, j.title as job_title, j.company, r.filename, r.candidate_name, r.candidate_email\n        FROM evaluations e\n        JOIN job_descriptions j ON e.job_id = j.id\n        JOIN resumes r ON e.resume_id = r.id\n        WHERE 1=1\n    '''\n    params = []\n    \n    if job_id:\n        query += ' AND e.job_id = ?'\n        params.append(job_id)\n    \n    if min_score:\n        query += ' AND e.relevance_score >= ?'\n        params.append(min_score)\n    \n    if verdict:\n        query += ' AND e.verdict = ?'\n        params.append(verdict)\n    \n    query += ' ORDER BY e.relevance_score DESC, e.evaluated_at DESC'\n    \n    cursor.execute(query, params)\n    evaluations = cursor.fetchall()\n    conn.close()\n    \n    return evaluations\n\ndef get_job_by_id(job_id):\n    \"\"\"Get job description by ID\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM job_descriptions WHERE id = ?', (job_id,))\n    job = cursor.fetchone()\n    conn.close()\n    \n    return job\n\ndef get_resume_by_id(resume_id):\n    \"\"\"Get resume by ID\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM resumes WHERE id = ?', (resume_id,))\n    resume = cursor.fetchone()\n    conn.close()\n    \n    return resume\n\ndef get_evaluation_stats():\n    \"\"\"Get evaluation statistics\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    # Total counts\n    cursor.execute('SELECT COUNT(*) FROM job_descriptions')\n    total_jobs = cursor.fetchone()[0]\n    \n    cursor.execute('SELECT COUNT(*) FROM resumes')\n    total_resumes = cursor.fetchone()[0]\n    \n    cursor.execute('SELECT COUNT(*) FROM evaluations')\n    total_evaluations = cursor.fetchone()[0]\n    \n    # Verdict distribution\n    cursor.execute('SELECT verdict, COUNT(*) FROM evaluations GROUP BY verdict')\n    verdict_dist = cursor.fetchall()\n    \n    # Average scores\n    cursor.execute('SELECT AVG(relevance_score) FROM evaluations')\n    avg_score = cursor.fetchone()[0] or 0\n    \n    conn.close()\n    \n    return {\n        'total_jobs': total_jobs,\n        'total_resumes': total_resumes,\n        'total_evaluations': total_evaluations,\n        'verdict_distribution': dict(verdict_dist),\n        'average_score': avg_score\n    }\n","size_bytes":7039},"database_postgres.py":{"content":"import psycopg2\nimport psycopg2.extras\nimport json\nimport os\nfrom datetime import datetime\nimport streamlit as st\n\n# Get PostgreSQL connection details from environment variables\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nPGHOST = os.getenv(\"PGHOST\")\nPGPORT = os.getenv(\"PGPORT\")\nPGUSER = os.getenv(\"PGUSER\")\nPGPASSWORD = os.getenv(\"PGPASSWORD\")\nPGDATABASE = os.getenv(\"PGDATABASE\")\n\ndef get_connection():\n    \"\"\"Get PostgreSQL database connection\"\"\"\n    try:\n        if DATABASE_URL:\n            return psycopg2.connect(DATABASE_URL)\n        else:\n            return psycopg2.connect(\n                host=PGHOST,\n                port=PGPORT,\n                user=PGUSER,\n                password=PGPASSWORD,\n                database=PGDATABASE\n            )\n    except Exception as e:\n        # Log the detailed error server-side but show generic message to users\n        print(f\"Database connection error: {e}\")\n        return None\n\ndef init_database():\n    \"\"\"Initialize PostgreSQL database with required tables\"\"\"\n    conn = get_connection()\n    if not conn:\n        return False\n        \n    cursor = conn.cursor()\n    \n    try:\n        # Job descriptions table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS job_descriptions (\n                id SERIAL PRIMARY KEY,\n                title TEXT NOT NULL,\n                company TEXT,\n                location TEXT,\n                description TEXT NOT NULL,\n                required_skills JSONB,\n                preferred_skills JSONB,\n                qualifications JSONB,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Resumes table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS resumes (\n                id SERIAL PRIMARY KEY,\n                filename TEXT NOT NULL,\n                candidate_name TEXT,\n                candidate_email TEXT,\n                extracted_text TEXT NOT NULL,\n                skills JSONB,\n                experience TEXT,\n                education TEXT,\n                uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Evaluations table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS evaluations (\n                id SERIAL PRIMARY KEY,\n                job_id INTEGER NOT NULL,\n                resume_id INTEGER NOT NULL,\n                relevance_score REAL NOT NULL,\n                hard_match_score REAL NOT NULL,\n                semantic_match_score REAL NOT NULL,\n                verdict TEXT NOT NULL,\n                missing_skills JSONB,\n                improvement_suggestions JSONB,\n                evaluation_details JSONB,\n                evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (job_id) REFERENCES job_descriptions (id),\n                FOREIGN KEY (resume_id) REFERENCES resumes (id)\n            )\n        ''')\n        \n        # Users table for authentication\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS users (\n                id SERIAL PRIMARY KEY,\n                username TEXT UNIQUE NOT NULL,\n                email TEXT UNIQUE NOT NULL,\n                password_hash TEXT NOT NULL,\n                location TEXT,\n                role TEXT DEFAULT 'placement_team',\n                is_active BOOLEAN DEFAULT TRUE,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # User sessions table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS user_sessions (\n                id SERIAL PRIMARY KEY,\n                user_id INTEGER NOT NULL,\n                session_token TEXT UNIQUE NOT NULL,\n                expires_at TIMESTAMP NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (user_id) REFERENCES users (id)\n            )\n        ''')\n        \n        conn.commit()\n        return True\n        \n    except Exception as e:\n        # Log the detailed error server-side but show generic message to users\n        print(f\"Database initialization error: {e}\")\n        conn.rollback()\n        return False\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_job_description(title, company, location, description, required_skills, preferred_skills, qualifications):\n    \"\"\"Save job description to PostgreSQL database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO job_descriptions (title, company, location, description, required_skills, preferred_skills, qualifications)\n            VALUES (%s, %s, %s, %s, %s, %s, %s) RETURNING id\n        ''', (title, company, location, description, json.dumps(required_skills), json.dumps(preferred_skills), json.dumps(qualifications)))\n        \n        job_id = cursor.fetchone()[0]\n        conn.commit()\n        return job_id\n        \n    except Exception as e:\n        print(f\"Error saving job description: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_resume(filename, candidate_name, candidate_email, extracted_text, skills, experience, education):\n    \"\"\"Save resume to PostgreSQL database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO resumes (filename, candidate_name, candidate_email, extracted_text, skills, experience, education)\n            VALUES (%s, %s, %s, %s, %s, %s, %s) RETURNING id\n        ''', (filename, candidate_name, candidate_email, extracted_text, json.dumps(skills), json.dumps(experience), json.dumps(education)))\n        \n        resume_id = cursor.fetchone()[0]\n        conn.commit()\n        return resume_id\n        \n    except Exception as e:\n        print(f\"Error saving resume: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_evaluation(job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details):\n    \"\"\"Save evaluation results to PostgreSQL database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO evaluations (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details)\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id\n        ''', (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, json.dumps(missing_skills), json.dumps(improvement_suggestions), json.dumps(evaluation_details)))\n        \n        evaluation_id = cursor.fetchone()[0]\n        conn.commit()\n        return evaluation_id\n        \n    except Exception as e:\n        print(f\"Error saving evaluation: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_job_descriptions():\n    \"\"\"Get all job descriptions\"\"\"\n    conn = get_connection()\n    if not conn:\n        return []\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM job_descriptions ORDER BY created_at DESC')\n        jobs = cursor.fetchall()\n        return jobs\n        \n    except Exception as e:\n        print(f\"Error fetching job descriptions: {e}\")\n        st.error(\"Unable to load job descriptions. Please refresh the page.\")\n        return []\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_resumes():\n    \"\"\"Get all resumes\"\"\"\n    conn = get_connection()\n    if not conn:\n        return []\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM resumes ORDER BY uploaded_at DESC')\n        resumes = cursor.fetchall()\n        return resumes\n        \n    except Exception as e:\n        print(f\"Error fetching resumes: {e}\")\n        st.error(\"Unable to load resumes. Please refresh the page.\")\n        return []\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_evaluations(job_id=None, min_score=None, verdict=None):\n    \"\"\"Get evaluations with optional filters\"\"\"\n    conn = get_connection()\n    if not conn:\n        return []\n        \n    cursor = conn.cursor()\n    \n    try:\n        query = '''\n            SELECT e.*, j.title as job_title, j.company, r.filename, r.candidate_name, r.candidate_email\n            FROM evaluations e\n            JOIN job_descriptions j ON e.job_id = j.id\n            JOIN resumes r ON e.resume_id = r.id\n            WHERE 1=1\n        '''\n        params = []\n        \n        if job_id:\n            query += ' AND e.job_id = %s'\n            params.append(job_id)\n        \n        if min_score:\n            query += ' AND e.relevance_score >= %s'\n            params.append(min_score)\n        \n        if verdict:\n            query += ' AND e.verdict = %s'\n            params.append(verdict)\n        \n        query += ' ORDER BY e.relevance_score DESC, e.evaluated_at DESC'\n        \n        cursor.execute(query, params)\n        evaluations = cursor.fetchall()\n        return evaluations\n        \n    except Exception as e:\n        print(f\"Error fetching evaluations: {e}\")\n        st.error(\"Unable to load evaluations. Please refresh the page.\")\n        return []\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_job_by_id(job_id):\n    \"\"\"Get job description by ID\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM job_descriptions WHERE id = %s', (job_id,))\n        job = cursor.fetchone()\n        return job\n        \n    except Exception as e:\n        print(f\"Error fetching job: {e}\")\n        st.error(\"Unable to load job information. Please try again.\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_resume_by_id(resume_id):\n    \"\"\"Get resume by ID\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM resumes WHERE id = %s', (resume_id,))\n        resume = cursor.fetchone()\n        return resume\n        \n    except Exception as e:\n        print(f\"Error fetching resume: {e}\")\n        st.error(\"Unable to load resume information. Please try again.\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_evaluation_stats():\n    \"\"\"Get evaluation statistics\"\"\"\n    conn = get_connection()\n    if not conn:\n        return {'total_jobs': 0, 'total_resumes': 0, 'total_evaluations': 0, 'verdict_distribution': {}, 'average_score': 0}\n        \n    cursor = conn.cursor()\n    \n    try:\n        # Total counts\n        cursor.execute('SELECT COUNT(*) FROM job_descriptions')\n        total_jobs = cursor.fetchone()[0]\n        \n        cursor.execute('SELECT COUNT(*) FROM resumes')\n        total_resumes = cursor.fetchone()[0]\n        \n        cursor.execute('SELECT COUNT(*) FROM evaluations')\n        total_evaluations = cursor.fetchone()[0]\n        \n        # Verdict distribution\n        cursor.execute('SELECT verdict, COUNT(*) FROM evaluations GROUP BY verdict')\n        verdict_dist = cursor.fetchall()\n        \n        # Average scores\n        cursor.execute('SELECT AVG(relevance_score) FROM evaluations')\n        avg_score_result = cursor.fetchone()[0]\n        avg_score = float(avg_score_result) if avg_score_result else 0\n        \n        return {\n            'total_jobs': total_jobs,\n            'total_resumes': total_resumes,\n            'total_evaluations': total_evaluations,\n            'verdict_distribution': dict(verdict_dist),\n            'average_score': avg_score\n        }\n        \n    except Exception as e:\n        print(f\"Error fetching evaluation stats: {e}\")\n        st.error(\"Unable to load statistics. Please refresh the page.\")\n        return {'total_jobs': 0, 'total_resumes': 0, 'total_evaluations': 0, 'verdict_distribution': {}, 'average_score': 0}\n    finally:\n        cursor.close()\n        conn.close()\n\n# User authentication functions\ndef save_user(username, email, password_hash, location, role='placement_team'):\n    \"\"\"Save user to database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO users (username, email, password_hash, location, role)\n            VALUES (%s, %s, %s, %s, %s) RETURNING id\n        ''', (username, email, password_hash, location, role))\n        \n        user_id = cursor.fetchone()[0]\n        conn.commit()\n        return user_id\n        \n    except Exception as e:\n        print(f\"Error saving user: {e}\")\n        st.error(\"Registration failed. Please try again.\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_user_by_username(username):\n    \"\"\"Get user by username\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM users WHERE username = %s AND is_active = TRUE', (username,))\n        user = cursor.fetchone()\n        return user\n        \n    except Exception as e:\n        print(f\"Error fetching user: {e}\")\n        st.error(\"Login failed. Please check your credentials.\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_session(user_id, session_token, expires_at):\n    \"\"\"Save user session\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO user_sessions (user_id, session_token, expires_at)\n            VALUES (%s, %s, %s) RETURNING id\n        ''', (user_id, session_token, expires_at))\n        \n        session_id = cursor.fetchone()[0]\n        conn.commit()\n        return session_id\n        \n    except Exception as e:\n        print(f\"Error saving session: {e}\")\n        st.error(\"Session creation failed. Please try logging in again.\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_session(session_token):\n    \"\"\"Get session by token\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            SELECT s.*, u.username, u.email, u.location, u.role \n            FROM user_sessions s\n            JOIN users u ON s.user_id = u.id\n            WHERE s.session_token = %s AND s.expires_at > CURRENT_TIMESTAMP AND u.is_active = TRUE\n        ''', (session_token,))\n        session = cursor.fetchone()\n        return session\n        \n    except Exception as e:\n        print(f\"Error fetching session: {e}\")\n        st.error(\"Session verification failed. Please log in again.\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()","size_bytes":15113},"nlp_processor.py":{"content":"import spacy\nimport re\nimport json\nimport os\nfrom collections import Counter\nfrom openai import OpenAI\n\n# Initialize spaCy model\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept OSError:\n    import subprocess\n    import sys\n    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n    nlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize OpenAI client\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n\n# Common skills database\nTECHNICAL_SKILLS = [\n    'python', 'java', 'javascript', 'c++', 'c#', 'php', 'ruby', 'go', 'rust', 'kotlin',\n    'swift', 'typescript', 'scala', 'r', 'matlab', 'sql', 'nosql', 'mongodb', 'postgresql',\n    'mysql', 'oracle', 'redis', 'elasticsearch', 'docker', 'kubernetes', 'aws', 'azure',\n    'gcp', 'terraform', 'jenkins', 'git', 'github', 'gitlab', 'jira', 'confluence',\n    'react', 'angular', 'vue', 'node.js', 'express', 'django', 'flask', 'spring',\n    'laravel', 'rails', 'asp.net', 'tensorflow', 'pytorch', 'scikit-learn', 'pandas',\n    'numpy', 'matplotlib', 'seaborn', 'tableau', 'power bi', 'excel', 'spark', 'hadoop',\n    'kafka', 'rabbitmq', 'microservices', 'restful', 'graphql', 'soap', 'api', 'agile',\n    'scrum', 'devops', 'ci/cd', 'machine learning', 'deep learning', 'ai', 'nlp',\n    'computer vision', 'data science', 'data analysis', 'statistics', 'blockchain',\n    'cybersecurity', 'linux', 'windows', 'macos', 'bash', 'powershell', 'html', 'css',\n    'bootstrap', 'sass', 'less', 'webpack', 'npm', 'yarn', 'junit', 'selenium', 'pytest'\n]\n\ndef extract_skills_from_text(text):\n    \"\"\"Extract technical skills from text\"\"\"\n    text_lower = text.lower()\n    found_skills = []\n    \n    # Direct skill matching\n    for skill in TECHNICAL_SKILLS:\n        if skill.lower() in text_lower:\n            found_skills.append(skill)\n    \n    # Entity recognition for additional skills\n    doc = nlp(text)\n    for ent in doc.ents:\n        if ent.label_ in ['ORG', 'PRODUCT'] and len(ent.text) > 2:\n            potential_skill = ent.text.lower().strip()\n            if potential_skill not in [s.lower() for s in found_skills]:\n                found_skills.append(ent.text.strip())\n    \n    return list(set(found_skills))\n\ndef extract_experience_years(text):\n    \"\"\"Extract years of experience from text\"\"\"\n    experience_patterns = [\n        r'(\\d+)\\+?\\s*years?\\s+(?:of\\s+)?experience',\n        r'(\\d+)\\+?\\s*yrs?\\s+(?:of\\s+)?experience',\n        r'experience\\s*:\\s*(\\d+)\\+?\\s*years?',\n        r'(\\d+)\\+?\\s*years?\\s+in',\n        r'over\\s+(\\d+)\\s+years?',\n        r'more\\s+than\\s+(\\d+)\\s+years?'\n    ]\n    \n    text_lower = text.lower()\n    years = []\n    \n    for pattern in experience_patterns:\n        matches = re.findall(pattern, text_lower)\n        years.extend([int(match) for match in matches])\n    \n    return max(years) if years else 0\n\ndef extract_education_level(text):\n    \"\"\"Extract education level from text\"\"\"\n    education_levels = {\n        'phd': ['ph.d', 'phd', 'doctorate', 'doctoral'],\n        'masters': ['master', 'msc', 'ma', 'mba', 'ms', 'm.sc', 'm.a', 'm.s'],\n        'bachelors': ['bachelor', 'bsc', 'ba', 'be', 'btech', 'b.sc', 'b.a', 'b.e', 'b.tech'],\n        'diploma': ['diploma', 'certificate'],\n        'high_school': ['high school', 'secondary', '12th', 'intermediate']\n    }\n    \n    text_lower = text.lower()\n    detected_levels = []\n    \n    for level, keywords in education_levels.items():\n        for keyword in keywords:\n            if keyword in text_lower:\n                detected_levels.append(level)\n                break\n    \n    # Return highest level found\n    level_hierarchy = ['phd', 'masters', 'bachelors', 'diploma', 'high_school']\n    for level in level_hierarchy:\n        if level in detected_levels:\n            return level\n    \n    return 'unknown'\n\ndef parse_job_description(job_text):\n    \"\"\"Parse job description to extract requirements\"\"\"\n    if not openai_client:\n        return parse_job_description_rule_based(job_text)\n    \n    try:\n        # the newest OpenAI model is \"gpt-5\" which was released August 7, 2025.\n        # do not change this unless explicitly requested by the user\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an expert HR analyst. Parse the job description and extract key information. Respond with JSON in this exact format: {'required_skills': ['skill1', 'skill2'], 'preferred_skills': ['skill1', 'skill2'], 'experience_required': number, 'education_required': 'level', 'key_responsibilities': ['resp1', 'resp2']}\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Parse this job description:\\n\\n{job_text}\"\n                }\n            ],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {}\n        return result\n    \n    except Exception as e:\n        print(f\"Error parsing job description with AI: {e}\")\n        return parse_job_description_rule_based(job_text)\n\ndef parse_job_description_rule_based(job_text):\n    \"\"\"Rule-based job description parsing as fallback\"\"\"\n    text_lower = job_text.lower()\n    \n    # Extract skills\n    skills = extract_skills_from_text(job_text)\n    \n    # Separate required vs preferred\n    required_skills = []\n    preferred_skills = []\n    \n    # Look for required/must-have sections\n    required_indicators = ['required', 'must have', 'essential', 'mandatory']\n    preferred_indicators = ['preferred', 'nice to have', 'plus', 'bonus', 'desirable']\n    \n    lines = job_text.split('\\n')\n    current_section = 'unknown'\n    \n    for line in lines:\n        line_lower = line.lower()\n        if any(indicator in line_lower for indicator in required_indicators):\n            current_section = 'required'\n        elif any(indicator in line_lower for indicator in preferred_indicators):\n            current_section = 'preferred'\n        \n        # Extract skills from current line\n        line_skills = [skill for skill in skills if skill.lower() in line_lower]\n        \n        if current_section == 'required':\n            required_skills.extend(line_skills)\n        elif current_section == 'preferred':\n            preferred_skills.extend(line_skills)\n    \n    # If no clear separation, assume all skills are required\n    if not required_skills and not preferred_skills:\n        required_skills = skills[:len(skills)//2] if len(skills) > 5 else skills\n        preferred_skills = skills[len(skills)//2:] if len(skills) > 5 else []\n    \n    return {\n        'required_skills': list(set(required_skills)),\n        'preferred_skills': list(set(preferred_skills)),\n        'experience_required': extract_experience_years(job_text),\n        'education_required': extract_education_level(job_text),\n        'key_responsibilities': extract_responsibilities(job_text)\n    }\n\ndef extract_responsibilities(text):\n    \"\"\"Extract key responsibilities from job description\"\"\"\n    lines = text.split('\\n')\n    responsibilities = []\n    \n    responsibility_indicators = ['responsible for', 'duties', 'responsibilities', 'role includes', 'you will']\n    \n    for i, line in enumerate(lines):\n        line_lower = line.lower().strip()\n        if any(indicator in line_lower for indicator in responsibility_indicators):\n            # Look at next few lines for bullet points\n            for j in range(i+1, min(i+10, len(lines))):\n                next_line = lines[j].strip()\n                if next_line and (next_line.startswith('-') or next_line.startswith('‚Ä¢') or next_line.startswith('*')):\n                    responsibilities.append(next_line.lstrip('-‚Ä¢* '))\n                elif not next_line:\n                    break\n    \n    return responsibilities[:5]  # Return top 5 responsibilities\n\ndef analyze_resume_semantic(resume_text, job_requirements):\n    \"\"\"Perform semantic analysis of resume against job requirements using AI\"\"\"\n    if not openai_client:\n        return {\n            'semantic_score': 50,  # Default neutral score\n            'analysis': 'AI analysis not available - API key not configured',\n            'strengths': [],\n            'gaps': []\n        }\n    \n    try:\n        prompt = f\"\"\"\n        Analyze this resume against the job requirements and provide a semantic match score.\n        \n        Job Requirements:\n        - Required Skills: {job_requirements.get('required_skills', [])}\n        - Preferred Skills: {job_requirements.get('preferred_skills', [])}\n        - Experience Required: {job_requirements.get('experience_required', 0)} years\n        - Education Required: {job_requirements.get('education_required', 'unknown')}\n        \n        Resume Text:\n        {resume_text[:2000]}\n        \n        Provide a JSON response with:\n        {{\n            \"semantic_score\": number (0-100),\n            \"analysis\": \"detailed analysis text\",\n            \"strengths\": [\"strength1\", \"strength2\"],\n            \"gaps\": [\"gap1\", \"gap2\"]\n        }}\n        \"\"\"\n        \n        # the newest OpenAI model is \"gpt-5\" which was released August 7, 2025.\n        # do not change this unless explicitly requested by the user\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an expert resume analyzer. Provide detailed semantic analysis comparing resumes to job requirements.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {}\n        return result\n    \n    except Exception as e:\n        print(f\"Error in semantic analysis: {e}\")\n        return {\n            'semantic_score': 50,\n            'analysis': f'Error in AI analysis: {str(e)}',\n            'strengths': [],\n            'gaps': []\n        }\n\ndef generate_improvement_suggestions(resume_analysis, job_requirements, missing_skills):\n    \"\"\"Generate personalized improvement suggestions for candidates\"\"\"\n    if not openai_client:\n        return [\n            \"Consider acquiring the missing technical skills identified in the analysis\",\n            \"Highlight relevant project experience more prominently\",\n            \"Add specific metrics and achievements to demonstrate impact\"\n        ]\n    \n    try:\n        prompt = f\"\"\"\n        Based on this resume analysis and job requirements, provide 3-5 specific, actionable improvement suggestions for the candidate.\n        \n        Job Requirements:\n        {job_requirements}\n        \n        Resume Analysis:\n        {resume_analysis}\n        \n        Missing Skills:\n        {missing_skills}\n        \n        Provide suggestions as a JSON array of strings:\n        {{\"suggestions\": [\"suggestion1\", \"suggestion2\", \"suggestion3\"]}}\n        \"\"\"\n        \n        # the newest OpenAI model is \"gpt-5\" which was released August 7, 2025.\n        # do not change this unless explicitly requested by the user\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a career counselor providing specific, actionable advice to job candidates.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {}\n        return result.get('suggestions', [])\n    \n    except Exception as e:\n        print(f\"Error generating suggestions: {e}\")\n        return [\n            \"Consider acquiring the missing technical skills\",\n            \"Add more specific project details and achievements\",\n            \"Improve resume format and structure for better readability\"\n        ]\n","size_bytes":12460},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"bcrypt>=4.3.0\",\n    \"docx>=0.2.4\",\n    \"fuzzywuzzy>=0.18.0\",\n    \"google-genai>=1.38.0\",\n    \"numpy>=2.3.3\",\n    \"openai>=1.108.1\",\n    \"pandas>=2.3.2\",\n    \"plotly>=6.3.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"pymupdf>=1.26.4\",\n    \"python-docx>=1.2.0\",\n    \"python-levenshtein>=0.27.1\",\n    \"scikit-learn>=1.7.2\",\n    \"spacy>=3.8.7\",\n    \"streamlit>=1.49.1\",\n]\n","size_bytes":509},"scoring_engine.py":{"content":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom nlp_processor import extract_skills_from_text, extract_experience_years, extract_education_level, analyze_resume_semantic, generate_improvement_suggestions\nfrom fuzzywuzzy import fuzz\nimport json\n\nclass ResumeScorer:\n    def __init__(self):\n        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n        \n    def calculate_hard_match_score(self, resume_data, job_requirements):\n        \"\"\"Calculate hard match score based on exact keyword and skill matching\"\"\"\n        scores = {}\n        \n        # Skills matching\n        resume_skills = extract_skills_from_text(resume_data['extracted_text'])\n        required_skills = job_requirements.get('required_skills', [])\n        preferred_skills = job_requirements.get('preferred_skills', [])\n        \n        # Exact skill matches\n        exact_required_matches = len(set([s.lower() for s in resume_skills]) & set([s.lower() for s in required_skills]))\n        exact_preferred_matches = len(set([s.lower() for s in resume_skills]) & set([s.lower() for s in preferred_skills]))\n        \n        # Fuzzy skill matches (for handling variations)\n        fuzzy_required_matches = 0\n        fuzzy_preferred_matches = 0\n        \n        for resume_skill in resume_skills:\n            for required_skill in required_skills:\n                if fuzz.ratio(resume_skill.lower(), required_skill.lower()) > 80:\n                    fuzzy_required_matches += 1\n                    break\n                    \n        for resume_skill in resume_skills:\n            for preferred_skill in preferred_skills:\n                if fuzz.ratio(resume_skill.lower(), preferred_skill.lower()) > 80:\n                    fuzzy_preferred_matches += 1\n                    break\n        \n        # Calculate skill scores\n        total_required_skills = len(required_skills) if required_skills else 1\n        total_preferred_skills = len(preferred_skills) if preferred_skills else 1\n        \n        required_skill_score = min(100, ((exact_required_matches + fuzzy_required_matches) / total_required_skills) * 100)\n        preferred_skill_score = min(100, ((exact_preferred_matches + fuzzy_preferred_matches) / total_preferred_skills) * 100)\n        \n        scores['skills'] = {\n            'required_score': required_skill_score,\n            'preferred_score': preferred_skill_score,\n            'matched_required': exact_required_matches + fuzzy_required_matches,\n            'matched_preferred': exact_preferred_matches + fuzzy_preferred_matches,\n            'total_required': total_required_skills,\n            'total_preferred': total_preferred_skills\n        }\n        \n        # Experience matching\n        resume_experience = extract_experience_years(resume_data['extracted_text'])\n        required_experience = job_requirements.get('experience_required', 0)\n        \n        if required_experience == 0:\n            experience_score = 100\n        else:\n            experience_ratio = min(1.0, resume_experience / required_experience)\n            experience_score = experience_ratio * 100\n        \n        scores['experience'] = {\n            'score': experience_score,\n            'resume_years': resume_experience,\n            'required_years': required_experience\n        }\n        \n        # Education matching\n        resume_education = extract_education_level(resume_data['extracted_text'])\n        required_education = job_requirements.get('education_required', 'unknown')\n        \n        education_hierarchy = {\n            'high_school': 1,\n            'diploma': 2,\n            'bachelors': 3,\n            'masters': 4,\n            'phd': 5,\n            'unknown': 0\n        }\n        \n        resume_edu_level = education_hierarchy.get(resume_education, 0)\n        required_edu_level = education_hierarchy.get(required_education, 0)\n        \n        if required_edu_level == 0:\n            education_score = 100\n        elif resume_edu_level >= required_edu_level:\n            education_score = 100\n        else:\n            education_score = max(0, (resume_edu_level / required_edu_level) * 100)\n        \n        scores['education'] = {\n            'score': education_score,\n            'resume_level': resume_education,\n            'required_level': required_education\n        }\n        \n        # Calculate weighted hard match score\n        weights = {\n            'required_skills': 0.5,\n            'preferred_skills': 0.2,\n            'experience': 0.2,\n            'education': 0.1\n        }\n        \n        total_hard_score = (\n            required_skill_score * weights['required_skills'] +\n            preferred_skill_score * weights['preferred_skills'] +\n            experience_score * weights['experience'] +\n            education_score * weights['education']\n        )\n        \n        return total_hard_score, scores\n    \n    def calculate_semantic_match_score(self, resume_data, job_description_text):\n        \"\"\"Calculate semantic match score using TF-IDF and cosine similarity\"\"\"\n        try:\n            # Prepare texts\n            resume_text = resume_data['extracted_text']\n            \n            # Create TF-IDF vectors\n            documents = [resume_text, job_description_text]\n            tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)\n            \n            # Calculate cosine similarity\n            similarity_matrix = cosine_similarity(tfidf_matrix)\n            semantic_score = similarity_matrix[0][1] * 100\n            \n            return semantic_score\n            \n        except Exception as e:\n            print(f\"Error in semantic scoring: {e}\")\n            return 50  # Default neutral score\n    \n    def identify_missing_skills(self, resume_data, job_requirements):\n        \"\"\"Identify missing skills and qualifications\"\"\"\n        resume_skills = [s.lower() for s in extract_skills_from_text(resume_data['extracted_text'])]\n        required_skills = [s.lower() for s in job_requirements.get('required_skills', [])]\n        preferred_skills = [s.lower() for s in job_requirements.get('preferred_skills', [])]\n        \n        missing_required = []\n        missing_preferred = []\n        \n        # Check for missing required skills\n        for skill in required_skills:\n            if not any(fuzz.ratio(skill, resume_skill) > 80 for resume_skill in resume_skills):\n                missing_required.append(skill)\n        \n        # Check for missing preferred skills\n        for skill in preferred_skills:\n            if not any(fuzz.ratio(skill, resume_skill) > 80 for resume_skill in resume_skills):\n                missing_preferred.append(skill)\n        \n        # Check experience gap\n        resume_experience = extract_experience_years(resume_data['extracted_text'])\n        required_experience = job_requirements.get('experience_required', 0)\n        experience_gap = max(0, required_experience - resume_experience)\n        \n        # Check education gap\n        resume_education = extract_education_level(resume_data['extracted_text'])\n        required_education = job_requirements.get('education_required', 'unknown')\n        \n        education_hierarchy = {\n            'high_school': 1, 'diploma': 2, 'bachelors': 3, 'masters': 4, 'phd': 5, 'unknown': 0\n        }\n        \n        resume_edu_level = education_hierarchy.get(resume_education, 0)\n        required_edu_level = education_hierarchy.get(required_education, 0)\n        education_gap = required_edu_level > resume_edu_level\n        \n        return {\n            'missing_required_skills': missing_required,\n            'missing_preferred_skills': missing_preferred,\n            'experience_gap_years': experience_gap,\n            'education_gap': education_gap,\n            'current_education': resume_education,\n            'required_education': required_education\n        }\n    \n    def determine_verdict(self, relevance_score):\n        \"\"\"Determine fit verdict based on relevance score\"\"\"\n        if relevance_score >= 75:\n            return \"High\"\n        elif relevance_score >= 50:\n            return \"Medium\"\n        else:\n            return \"Low\"\n    \n    def evaluate_resume(self, resume_data, job_description_text, job_requirements):\n        \"\"\"Complete resume evaluation pipeline\"\"\"\n        try:\n            # Calculate hard match score\n            hard_match_score, hard_match_details = self.calculate_hard_match_score(resume_data, job_requirements)\n            \n            # Calculate semantic match score using TF-IDF\n            tfidf_semantic_score = self.calculate_semantic_match_score(resume_data, job_description_text)\n            \n            # Get AI-powered semantic analysis\n            ai_semantic_analysis = analyze_resume_semantic(resume_data['extracted_text'], job_requirements)\n            ai_semantic_score = ai_semantic_analysis.get('semantic_score', 50)\n            \n            # Combine semantic scores (weighted average)\n            combined_semantic_score = (tfidf_semantic_score * 0.4) + (ai_semantic_score * 0.6)\n            \n            # Calculate final relevance score\n            weights = {\n                'hard_match': 0.6,\n                'semantic_match': 0.4\n            }\n            \n            relevance_score = (\n                hard_match_score * weights['hard_match'] +\n                combined_semantic_score * weights['semantic_match']\n            )\n            \n            # Identify missing skills and gaps\n            missing_elements = self.identify_missing_skills(resume_data, job_requirements)\n            \n            # Determine verdict\n            verdict = self.determine_verdict(relevance_score)\n            \n            # Generate improvement suggestions\n            improvement_suggestions = generate_improvement_suggestions(\n                ai_semantic_analysis, job_requirements, missing_elements\n            )\n            \n            # Compile evaluation details\n            evaluation_details = {\n                'hard_match_details': hard_match_details,\n                'tfidf_semantic_score': tfidf_semantic_score,\n                'ai_semantic_analysis': ai_semantic_analysis,\n                'combined_semantic_score': combined_semantic_score,\n                'scoring_weights': weights,\n                'missing_elements': missing_elements\n            }\n            \n            return {\n                'relevance_score': round(relevance_score, 2),\n                'hard_match_score': round(hard_match_score, 2),\n                'semantic_match_score': round(combined_semantic_score, 2),\n                'verdict': verdict,\n                'missing_skills': missing_elements['missing_required_skills'] + missing_elements['missing_preferred_skills'],\n                'improvement_suggestions': improvement_suggestions,\n                'evaluation_details': evaluation_details\n            }\n            \n        except Exception as e:\n            print(f\"Error in resume evaluation: {e}\")\n            return {\n                'relevance_score': 0,\n                'hard_match_score': 0,\n                'semantic_match_score': 0,\n                'verdict': 'Low',\n                'missing_skills': [],\n                'improvement_suggestions': ['Error occurred during evaluation'],\n                'evaluation_details': {'error': str(e)}\n            }\n","size_bytes":11375},"text_extractor.py":{"content":"try:\n    import pymupdf as fitz  # PyMuPDF\nexcept ImportError:\n    import fitz  # fallback\nimport docx\nimport re\nimport streamlit as st\nfrom io import BytesIO\n\ndef extract_text_from_pdf(file_bytes):\n    \"\"\"Extract text from PDF file\"\"\"\n    try:\n        # Open PDF from bytes\n        pdf_document = fitz.open(stream=file_bytes, filetype=\"pdf\")\n        text = \"\"\n        \n        for page_num in range(pdf_document.page_count):\n            page = pdf_document.load_page(page_num)\n            # Use get_text() method for text extraction\n            page_text = page.get_text() if hasattr(page, 'get_text') else page.get_text('text')\n            text += page_text\n        \n        pdf_document.close()\n        return clean_text(text)\n    \n    except Exception as e:\n        print(f\"Error extracting text from PDF: {str(e)}\")\n        st.error(\"Failed to process PDF file. Please try with a different file.\")\n        return \"\"\n\ndef extract_text_from_docx(file_bytes):\n    \"\"\"Extract text from DOCX file\"\"\"\n    try:\n        # Open DOCX from bytes\n        doc = docx.Document(BytesIO(file_bytes))\n        text = \"\"\n        \n        for paragraph in doc.paragraphs:\n            text += paragraph.text + \"\\n\"\n        \n        return clean_text(text)\n    \n    except Exception as e:\n        print(f\"Error extracting text from DOCX: {str(e)}\")\n        st.error(\"Failed to process DOCX file. Please try with a different file.\")\n        return \"\"\n\ndef clean_text(text):\n    \"\"\"Clean and normalize extracted text\"\"\"\n    if not text:\n        return \"\"\n    \n    # Remove extra whitespace and normalize\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    \n    # Remove common PDF artifacts\n    text = re.sub(r'[^\\w\\s\\-.,;:()\\[\\]@+/&%]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text\n\ndef extract_contact_info(text):\n    \"\"\"Extract contact information from text\"\"\"\n    contact_info = {}\n    \n    # Email extraction\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    emails = re.findall(email_pattern, text)\n    contact_info['email'] = emails[0] if emails else \"\"\n    \n    # Phone extraction\n    phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n    phones = re.findall(phone_pattern, text)\n    contact_info['phone'] = phones[0] if phones else \"\"\n    \n    # Name extraction (basic - first few words before common resume sections)\n    lines = text.split('\\n')\n    name = \"\"\n    for line in lines[:5]:  # Check first 5 lines\n        line = line.strip()\n        if line and len(line.split()) <= 4 and not any(keyword in line.lower() for keyword in ['resume', 'cv', 'curriculum', 'vitae', 'email', 'phone', '@']):\n            if not re.search(r'\\d', line):  # No numbers in name\n                name = line\n                break\n    \n    contact_info['name'] = name\n    \n    return contact_info\n\ndef extract_sections(text):\n    \"\"\"Extract different sections from resume text\"\"\"\n    sections = {\n        'experience': '',\n        'education': '',\n        'skills': '',\n        'projects': '',\n        'certifications': ''\n    }\n    \n    # Common section headers\n    section_patterns = {\n        'experience': r'(experience|work\\s+experience|professional\\s+experience|employment|career)',\n        'education': r'(education|academic|qualification|degree)',\n        'skills': r'(skills|technical\\s+skills|competencies|expertise)',\n        'projects': r'(projects|personal\\s+projects|portfolio)',\n        'certifications': r'(certifications?|certificates?|credentials)'\n    }\n    \n    text_lower = text.lower()\n    \n    for section_name, pattern in section_patterns.items():\n        # Find section start\n        matches = list(re.finditer(pattern, text_lower))\n        if matches:\n            start_pos = matches[0].start()\n            \n            # Find next section or end of text\n            end_pos = len(text)\n            for other_pattern in section_patterns.values():\n                if other_pattern != pattern:\n                    other_matches = list(re.finditer(other_pattern, text_lower[start_pos + 50:]))\n                    if other_matches:\n                        potential_end = start_pos + 50 + other_matches[0].start()\n                        if potential_end < end_pos:\n                            end_pos = potential_end\n            \n            sections[section_name] = text[start_pos:end_pos].strip()\n    \n    return sections\n\ndef process_uploaded_file(uploaded_file):\n    \"\"\"Process uploaded resume file and extract all information\"\"\"\n    if uploaded_file is None:\n        return None\n    \n    file_bytes = uploaded_file.read()\n    filename = uploaded_file.name\n    file_extension = filename.lower().split('.')[-1]\n    \n    # Extract text based on file type\n    if file_extension == 'pdf':\n        extracted_text = extract_text_from_pdf(file_bytes)\n    elif file_extension in ['docx', 'doc']:\n        extracted_text = extract_text_from_docx(file_bytes)\n    else:\n        st.error(\"Unsupported file format. Please upload PDF or DOCX files.\")\n        return None\n    \n    if not extracted_text:\n        st.error(\"Could not extract text from the file. Please check the file format.\")\n        return None\n    \n    # Extract contact information\n    contact_info = extract_contact_info(extracted_text)\n    \n    # Extract sections\n    sections = extract_sections(extracted_text)\n    \n    return {\n        'filename': filename,\n        'extracted_text': extracted_text,\n        'contact_info': contact_info,\n        'sections': sections\n    }\n","size_bytes":5520},"utils.py":{"content":"import re\nimport string\nimport unicodedata\nfrom datetime import datetime\nimport streamlit as st\n\ndef clean_filename(filename):\n    \"\"\"Clean filename for safe storage\"\"\"\n    # Remove extension for processing\n    name, ext = filename.rsplit('.', 1) if '.' in filename else (filename, '')\n    \n    # Remove special characters and normalize\n    name = re.sub(r'[^\\w\\s-]', '', name.strip())\n    name = re.sub(r'[-\\s]+', '_', name)\n    \n    # Add timestamp to make unique\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    return f\"{name}_{timestamp}.{ext}\" if ext else f\"{name}_{timestamp}\"\n\ndef normalize_text(text):\n    \"\"\"Normalize text for better processing\"\"\"\n    if not text:\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove special characters but keep important punctuation\n    text = re.sub(r'[^\\w\\s.,;:()\\-]', ' ', text)\n    \n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndef extract_keywords(text, min_length=3):\n    \"\"\"Extract keywords from text\"\"\"\n    if not text:\n        return []\n    \n    # Normalize text\n    text = normalize_text(text)\n    \n    # Split into words\n    words = text.split()\n    \n    # Filter keywords\n    keywords = []\n    for word in words:\n        if (len(word) >= min_length and \n            word not in string.punctuation and\n            not word.isdigit()):\n            keywords.append(word)\n    \n    return list(set(keywords))\n\ndef calculate_text_similarity(text1, text2):\n    \"\"\"Calculate simple text similarity using Jaccard similarity\"\"\"\n    if not text1 or not text2:\n        return 0.0\n    \n    # Get keywords from both texts\n    keywords1 = set(extract_keywords(text1))\n    keywords2 = set(extract_keywords(text2))\n    \n    if not keywords1 or not keywords2:\n        return 0.0\n    \n    # Calculate Jaccard similarity\n    intersection = len(keywords1.intersection(keywords2))\n    union = len(keywords1.union(keywords2))\n    \n    return intersection / union if union > 0 else 0.0\n\ndef format_date(date_string):\n    \"\"\"Format date string for display\"\"\"\n    try:\n        if isinstance(date_string, str):\n            dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n        else:\n            dt = date_string\n        return dt.strftime('%Y-%m-%d %H:%M')\n    except:\n        return str(date_string)\n\ndef validate_email(email):\n    \"\"\"Validate email format\"\"\"\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\ndef sanitize_input(text, max_length=None):\n    \"\"\"Sanitize user input\"\"\"\n    if not text:\n        return \"\"\n    \n    # Remove potentially harmful characters\n    text = re.sub(r'[<>\"\\']', '', text)\n    \n    # Limit length if specified\n    if max_length and len(text) > max_length:\n        text = text[:max_length]\n    \n    return text.strip()\n\ndef create_download_link(data, filename, mime_type=\"text/plain\"):\n    \"\"\"Create a download link for data\"\"\"\n    return st.download_button(\n        label=f\"Download {filename}\",\n        data=data,\n        file_name=filename,\n        mime=mime_type\n    )\n\ndef format_score(score, precision=1):\n    \"\"\"Format score for display\"\"\"\n    try:\n        return f\"{float(score):.{precision}f}\"\n    except:\n        return \"N/A\"\n\ndef get_verdict_color(verdict):\n    \"\"\"Get color for verdict display\"\"\"\n    colors = {\n        'High': '#28a745',    # Green\n        'Medium': '#ffc107',  # Yellow\n        'Low': '#dc3545',     # Red\n    }\n    return colors.get(verdict, '#6c757d')  # Default gray\n\ndef truncate_text(text, max_length=100):\n    \"\"\"Truncate text with ellipsis\"\"\"\n    if not text:\n        return \"\"\n    \n    if len(text) <= max_length:\n        return text\n    \n    return text[:max_length-3] + \"...\"\n\ndef parse_json_safely(json_string):\n    \"\"\"Safely parse JSON string\"\"\"\n    try:\n        import json\n        return json.loads(json_string) if json_string else {}\n    except:\n        return {}\n\ndef format_list_for_display(items, separator=\", \"):\n    \"\"\"Format list for display\"\"\"\n    if not items:\n        return \"None\"\n    \n    if isinstance(items, str):\n        items = parse_json_safely(items)\n    \n    if isinstance(items, list):\n        return separator.join(str(item) for item in items)\n    \n    return str(items)\n\ndef highlight_keywords(text, keywords, highlight_color=\"#ffff00\"):\n    \"\"\"Highlight keywords in text for display\"\"\"\n    if not text or not keywords:\n        return text\n    \n    highlighted_text = text\n    for keyword in keywords:\n        pattern = re.compile(re.escape(keyword), re.IGNORECASE)\n        highlighted_text = pattern.sub(\n            f'<mark style=\"background-color: {highlight_color};\">{keyword}</mark>',\n            highlighted_text\n        )\n    \n    return highlighted_text\n\ndef calculate_completion_percentage(completed_items, total_items):\n    \"\"\"Calculate completion percentage\"\"\"\n    if total_items == 0:\n        return 0\n    return (completed_items / total_items) * 100\n\ndef get_file_size_mb(file_bytes):\n    \"\"\"Get file size in MB\"\"\"\n    return len(file_bytes) / (1024 * 1024)\n\ndef validate_file_size(file_bytes, max_size_mb=10):\n    \"\"\"Validate file size\"\"\"\n    size_mb = get_file_size_mb(file_bytes)\n    return size_mb <= max_size_mb, size_mb\n\ndef create_progress_indicator(current, total, label=\"Progress\"):\n    \"\"\"Create a progress indicator\"\"\"\n    if total == 0:\n        return st.write(f\"{label}: No items to process\")\n    \n    percentage = (current / total) * 100\n    return st.progress(percentage / 100)\n","size_bytes":5596},"auth.py":{"content":"import streamlit as st\nimport bcrypt\nimport secrets\nfrom datetime import datetime, timedelta\nfrom database_postgres import save_user, get_user_by_username, save_session, get_session\n\ndef hash_password(password):\n    \"\"\"Hash password using bcrypt with salt\"\"\"\n    salt = bcrypt.gensalt()\n    return bcrypt.hashpw(password.encode('utf-8'), salt).decode('utf-8')\n\ndef verify_password(password, password_hash):\n    \"\"\"Verify password against bcrypt hash\"\"\"\n    return bcrypt.checkpw(password.encode('utf-8'), password_hash.encode('utf-8'))\n\ndef generate_session_token():\n    \"\"\"Generate secure session token\"\"\"\n    return secrets.token_urlsafe(32)\n\ndef login_user(username, password):\n    \"\"\"Authenticate user and create session\"\"\"\n    user = get_user_by_username(username)\n    if user and verify_password(password, user[3]):  # user[3] is password_hash\n        # Create session\n        session_token = generate_session_token()\n        expires_at = datetime.now() + timedelta(hours=24)\n        \n        session_id = save_session(user[0], session_token, expires_at)\n        if session_id:\n            st.session_state.authenticated = True\n            st.session_state.user_id = user[0]\n            st.session_state.username = user[1]\n            st.session_state.email = user[2]\n            st.session_state.location = user[4]\n            st.session_state.role = user[5]\n            st.session_state.session_token = session_token\n            return True\n    return False\n\ndef register_user(username, email, password, location, role='placement_team'):\n    \"\"\"Register new user\"\"\"\n    # Check if user already exists\n    existing_user = get_user_by_username(username)\n    if existing_user:\n        return False, \"Username already exists\"\n    \n    # Hash password\n    password_hash = hash_password(password)\n    \n    # Save user\n    user_id = save_user(username, email, password_hash, location, role)\n    if user_id:\n        return True, \"User registered successfully\"\n    else:\n        return False, \"Registration failed\"\n\ndef logout_user():\n    \"\"\"Logout user and clear session\"\"\"\n    if 'authenticated' in st.session_state:\n        del st.session_state.authenticated\n    if 'user_id' in st.session_state:\n        del st.session_state.user_id\n    if 'username' in st.session_state:\n        del st.session_state.username\n    if 'email' in st.session_state:\n        del st.session_state.email\n    if 'location' in st.session_state:\n        del st.session_state.location\n    if 'role' in st.session_state:\n        del st.session_state.role\n    if 'session_token' in st.session_state:\n        del st.session_state.session_token\n\ndef check_authentication():\n    \"\"\"Check if user is authenticated\"\"\"\n    if 'authenticated' not in st.session_state:\n        return False\n    \n    if 'session_token' not in st.session_state:\n        return False\n    \n    # Verify session is still valid\n    session = get_session(st.session_state.session_token)\n    if not session:\n        logout_user()\n        return False\n    \n    return True\n\ndef require_authentication():\n    \"\"\"Decorator to require authentication for pages\"\"\"\n    if not check_authentication():\n        render_login_page()\n        return False\n    return True\n\ndef render_login_page():\n    \"\"\"Render login/registration page\"\"\"\n    # Modern login header\n    st.markdown(\"\"\"<div style='text-align: center; padding: 2rem; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; margin-bottom: 2rem;'>\n        <h1 style='color: white; margin: 0; font-size: 2.5rem;'>üîê InnoVantage Portal</h1>\n        <h3 style='color: white; margin: 0; font-weight: 300; opacity: 0.9;'>Placement Team Access</h3>\n        <p style='color: white; margin: 0; opacity: 0.8;'>Innomatics Research Labs ‚Ä¢ Secure Authentication</p>\n    </div>\"\"\", unsafe_allow_html=True)\n    \n    tab1, tab2 = st.tabs([\"Login\", \"Register\"])\n    \n    with tab1:\n        st.markdown(\"\"\"<div style='background: #f8f9fa; padding: 1.5rem; border-radius: 10px; margin-bottom: 1rem;'>\n            <h3 style='margin: 0; color: #495057; text-align: center;'>üõãÔ∏è Login to Your Account</h3>\n        </div>\"\"\", unsafe_allow_html=True)\n        with st.form(\"login_form\"):\n            username = st.text_input(\"Username\")\n            password = st.text_input(\"Password\", type=\"password\")\n            login_button = st.form_submit_button(\"Login\")\n            \n            if login_button:\n                if username and password:\n                    if login_user(username, password):\n                        st.success(\"Login successful!\")\n                        st.rerun()\n                    else:\n                        st.error(\"Invalid username or password\")\n                else:\n                    st.error(\"Please enter both username and password\")\n    \n    with tab2:\n        st.markdown(\"\"\"<div style='background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin-bottom: 1rem;'>\n            <h3 style='margin: 0; color: #2d5a2d; text-align: center;'>‚ú® Register New Account</h3>\n        </div>\"\"\", unsafe_allow_html=True)\n        with st.form(\"register_form\"):\n            new_username = st.text_input(\"Username\", key=\"reg_username\")\n            new_email = st.text_input(\"Email\", key=\"reg_email\")\n            new_password = st.text_input(\"Password\", type=\"password\", key=\"reg_password\")\n            confirm_password = st.text_input(\"Confirm Password\", type=\"password\", key=\"reg_confirm\")\n            location = st.selectbox(\"Location\", \n                                  [\"Hyderabad\", \"Bangalore\", \"Pune\", \"Delhi NCR\", \"Other\"],\n                                  key=\"reg_location\")\n            role = st.selectbox(\"Role\", \n                              [\"placement_team\", \"admin\", \"mentor\"],\n                              key=\"reg_role\")\n            \n            register_button = st.form_submit_button(\"Register\")\n            \n            if register_button:\n                if not all([new_username, new_email, new_password, confirm_password, location]):\n                    st.error(\"Please fill in all fields\")\n                elif new_password != confirm_password:\n                    st.error(\"Passwords do not match\")\n                elif len(new_password) < 6:\n                    st.error(\"Password must be at least 6 characters long\")\n                else:\n                    success, message = register_user(new_username, new_email, new_password, location, role)\n                    if success:\n                        st.success(message)\n                        st.info(\"You can now login with your new account\")\n                    else:\n                        st.error(message)\n\ndef render_user_info():\n    \"\"\"Render user information in sidebar\"\"\"\n    if check_authentication():\n        st.sidebar.markdown(\"---\")\n        st.sidebar.subheader(\"üë§ User Info\")\n        st.sidebar.write(f\"**Username:** {st.session_state.username}\")\n        st.sidebar.write(f\"**Location:** {st.session_state.location}\")\n        st.sidebar.write(f\"**Role:** {st.session_state.role.title()}\")\n        \n        if st.sidebar.button(\"Logout\"):\n            logout_user()\n            st.rerun()\n\ndef get_user_location():\n    \"\"\"Get current user's location\"\"\"\n    if check_authentication():\n        return st.session_state.location\n    return None\n\ndef get_user_role():\n    \"\"\"Get current user's role\"\"\"\n    if check_authentication():\n        return st.session_state.role\n    return None\n\ndef is_admin():\n    \"\"\"Check if current user is admin\"\"\"\n    return get_user_role() == 'admin'\n\ndef can_access_analytics():\n    \"\"\"Check if user can access analytics\"\"\"\n    role = get_user_role()\n    return role in ['admin', 'placement_team']","size_bytes":7662},"batch_processor.py":{"content":"import streamlit as st\nimport pandas as pd\nimport json\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import List, Dict, Any\nfrom text_extractor import process_uploaded_file\nfrom nlp_processor import parse_job_description\nfrom scoring_engine import ResumeScorer\nfrom database_postgres import save_resume, save_evaluation, get_job_by_id\n\nclass BatchProcessor:\n    def __init__(self, max_workers=4):\n        \"\"\"Initialize batch processor with configurable concurrency\"\"\"\n        self.max_workers = max_workers\n        self.scorer = ResumeScorer()\n    \n    def process_resume_batch(self, uploaded_files, job_id, progress_callback=None):\n        \"\"\"Process multiple resume files in batch with concurrent processing\"\"\"\n        total_files = len(uploaded_files)\n        results = []\n        \n        # Get job details once\n        job_details = get_job_by_id(job_id)\n        if not job_details:\n            return {\"error\": \"Job not found\"}\n        \n        job_requirements = {\n            'required_skills': json.loads(job_details[5]) if job_details[5] else [],\n            'preferred_skills': json.loads(job_details[6]) if job_details[6] else [],\n            'experience_required': json.loads(job_details[7]).get('experience_required', 0) if job_details[7] else 0,\n            'education_required': json.loads(job_details[7]).get('education_required', 'unknown') if job_details[7] else 'unknown'\n        }\n        \n        # Process resumes concurrently\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all tasks\n            future_to_file = {}\n            for i, uploaded_file in enumerate(uploaded_files):\n                future = executor.submit(\n                    self._process_single_resume, \n                    uploaded_file, job_id, job_details, job_requirements\n                )\n                future_to_file[future] = (i, uploaded_file.name)\n            \n            # Collect results as they complete\n            completed = 0\n            for future in as_completed(future_to_file):\n                file_index, filename = future_to_file[future]\n                \n                try:\n                    result = future.result()\n                    results.append({\n                        'index': file_index,\n                        'filename': filename,\n                        'status': 'success',\n                        'result': result\n                    })\n                except Exception as e:\n                    results.append({\n                        'index': file_index,\n                        'filename': filename,\n                        'status': 'error',\n                        'error': str(e)\n                    })\n                \n                completed += 1\n                if progress_callback:\n                    progress_callback(completed, total_files, filename)\n        \n        # Sort results by original index\n        results.sort(key=lambda x: x['index'])\n        \n        return {\n            'total_processed': total_files,\n            'successful': len([r for r in results if r['status'] == 'success']),\n            'failed': len([r for r in results if r['status'] == 'error']),\n            'results': results\n        }\n    \n    def _process_single_resume(self, uploaded_file, job_id, job_details, job_requirements):\n        \"\"\"Process a single resume file\"\"\"\n        # Reset file pointer\n        uploaded_file.seek(0)\n        \n        # Extract text and information from resume\n        resume_data = process_uploaded_file(uploaded_file)\n        if not resume_data:\n            raise Exception(\"Failed to extract text from resume\")\n        \n        # Save resume to database\n        resume_id = save_resume(\n            resume_data['filename'],\n            resume_data['contact_info'].get('name', ''),\n            resume_data['contact_info'].get('email', ''),\n            resume_data['extracted_text'],\n            resume_data['sections'].get('skills', ''),\n            resume_data['sections'].get('experience', ''),\n            resume_data['sections'].get('education', '')\n        )\n        \n        if not resume_id:\n            raise Exception(\"Failed to save resume to database\")\n        \n        # Evaluate resume\n        evaluation_result = self.scorer.evaluate_resume(\n            resume_data, job_details[3], job_requirements\n        )\n        \n        # Save evaluation\n        evaluation_id = save_evaluation(\n            job_id, resume_id,\n            evaluation_result['relevance_score'],\n            evaluation_result['hard_match_score'],\n            evaluation_result['semantic_match_score'],\n            evaluation_result['verdict'],\n            evaluation_result['missing_skills'],\n            evaluation_result['improvement_suggestions'],\n            evaluation_result['evaluation_details']\n        )\n        \n        if not evaluation_id:\n            raise Exception(\"Failed to save evaluation to database\")\n        \n        return {\n            'resume_id': resume_id,\n            'evaluation_id': evaluation_id,\n            'score': evaluation_result['relevance_score'],\n            'verdict': evaluation_result['verdict'],\n            'candidate_name': resume_data['contact_info'].get('name', 'Unknown'),\n            'candidate_email': resume_data['contact_info'].get('email', '')\n        }\n    \n    def generate_batch_report(self, batch_results, job_title):\n        \"\"\"Generate a comprehensive batch processing report\"\"\"\n        if 'results' not in batch_results:\n            return None\n        \n        successful_results = [r for r in batch_results['results'] if r['status'] == 'success']\n        \n        if not successful_results:\n            return None\n        \n        # Create DataFrame for analysis\n        data = []\n        for result in successful_results:\n            if 'result' in result:\n                data.append({\n                    'Filename': result['filename'],\n                    'Candidate': result['result'].get('candidate_name', 'Unknown'),\n                    'Email': result['result'].get('candidate_email', ''),\n                    'Score': result['result'].get('score', 0),\n                    'Verdict': result['result'].get('verdict', 'Unknown')\n                })\n        \n        df = pd.DataFrame(data)\n        \n        # Generate summary statistics\n        summary = {\n            'total_processed': batch_results['total_processed'],\n            'successful': batch_results['successful'],\n            'failed': batch_results['failed'],\n            'average_score': df['Score'].mean() if not df.empty else 0,\n            'high_potential': len(df[df['Verdict'] == 'High']) if not df.empty else 0,\n            'medium_potential': len(df[df['Verdict'] == 'Medium']) if not df.empty else 0,\n            'low_potential': len(df[df['Verdict'] == 'Low']) if not df.empty else 0,\n            'top_candidates': df.nlargest(5, 'Score').to_dict('records') if not df.empty else []\n        }\n        \n        return {\n            'summary': summary,\n            'dataframe': df,\n            'job_title': job_title\n        }\n\ndef render_enhanced_batch_processing():\n    \"\"\"Render enhanced batch processing interface\"\"\"\n    st.header(\"üöÄ Enhanced Batch Processing\")\n    st.markdown(\"Process thousands of resumes efficiently with parallel processing\")\n    \n    # Job selection\n    from database_postgres import get_job_descriptions\n    jobs = get_job_descriptions()\n    if not jobs:\n        st.warning(\"Please upload at least one job description first!\")\n        return\n    \n    job_options = {f\"{job[1]} - {job[2] or 'Company Not Specified'}\": job[0] for job in jobs}\n    selected_job = st.selectbox(\"Select Job Position for Batch Evaluation\", options=list(job_options.keys()))\n    \n    if not selected_job:\n        return\n    \n    job_id = job_options[selected_job]\n    job_title = selected_job.split(' - ')[0]\n    \n    # Batch processing configuration\n    st.subheader(\"‚öôÔ∏è Processing Configuration\")\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        max_workers = st.slider(\"Concurrent Workers\", min_value=1, max_value=8, value=4, \n                               help=\"Higher values process faster but use more resources\")\n    \n    with col2:\n        chunk_size = st.slider(\"Batch Chunk Size\", min_value=10, max_value=100, value=25,\n                              help=\"Number of resumes to process in each batch\")\n    \n    # File upload\n    st.subheader(\"üìÑ Upload Resume Files\")\n    uploaded_files = st.file_uploader(\n        \"Upload Multiple Resume Files\", \n        type=['pdf', 'docx', 'doc'],\n        accept_multiple_files=True,\n        help=\"Upload multiple PDF or DOCX files for batch processing\"\n    )\n    \n    if uploaded_files:\n        st.success(f\"üìÅ {len(uploaded_files)} files selected for processing\")\n        \n        # File validation\n        valid_files = []\n        invalid_files = []\n        \n        for file in uploaded_files:\n            if file.size > 10 * 1024 * 1024:  # 10MB limit\n                invalid_files.append(f\"{file.name} (too large: {file.size/1024/1024:.1f}MB)\")\n            else:\n                valid_files.append(file)\n        \n        if invalid_files:\n            st.error(f\"‚ùå Invalid files: {', '.join(invalid_files)}\")\n        \n        if valid_files:\n            st.info(f\"‚úÖ {len(valid_files)} valid files ready for processing\")\n            \n            # Processing options\n            st.subheader(\"üéØ Processing Options\")\n            \n            col1, col2 = st.columns(2)\n            with col1:\n                save_intermediate = st.checkbox(\"Save intermediate results\", value=True,\n                                              help=\"Save progress even if processing is interrupted\")\n            \n            with col2:\n                generate_report = st.checkbox(\"Generate detailed report\", value=True,\n                                            help=\"Create comprehensive analysis report after processing\")\n            \n            # Start processing button\n            if st.button(\"üöÄ Start Batch Processing\", type=\"primary\"):\n                processor = BatchProcessor(max_workers=max_workers)\n                \n                # Initialize progress tracking\n                progress_bar = st.progress(0)\n                status_text = st.empty()\n                results_container = st.empty()\n                \n                start_time = time.time()\n                \n                def update_progress(completed, total, current_file):\n                    progress = completed / total\n                    progress_bar.progress(progress)\n                    elapsed = time.time() - start_time\n                    remaining = (elapsed / completed * (total - completed)) if completed > 0 else 0\n                    \n                    status_text.text(\n                        f\"Processing {completed}/{total} files | \"\n                        f\"Current: {current_file} | \"\n                        f\"Elapsed: {elapsed:.1f}s | \"\n                        f\"ETA: {remaining:.1f}s\"\n                    )\n                \n                try:\n                    # Process files in chunks if there are many\n                    if len(valid_files) > chunk_size:\n                        st.info(f\"Processing {len(valid_files)} files in chunks of {chunk_size}\")\n                        \n                        all_results = []\n                        for i in range(0, len(valid_files), chunk_size):\n                            chunk = valid_files[i:i + chunk_size]\n                            st.write(f\"Processing chunk {i//chunk_size + 1}/{(len(valid_files)-1)//chunk_size + 1}\")\n                            \n                            chunk_results = processor.process_resume_batch(\n                                chunk, job_id, update_progress\n                            )\n                            all_results.extend(chunk_results.get('results', []))\n                        \n                        # Combine chunk results\n                        batch_results = {\n                            'total_processed': len(valid_files),\n                            'successful': len([r for r in all_results if r['status'] == 'success']),\n                            'failed': len([r for r in all_results if r['status'] == 'error']),\n                            'results': all_results\n                        }\n                    else:\n                        # Process all files at once\n                        batch_results = processor.process_resume_batch(\n                            valid_files, job_id, update_progress\n                        )\n                    \n                    # Processing completed\n                    total_time = time.time() - start_time\n                    status_text.text(f\"‚úÖ Processing completed in {total_time:.1f} seconds\")\n                    \n                    # Display results\n                    st.success(f\"üéâ Batch processing completed!\")\n                    \n                    col1, col2, col3 = st.columns(3)\n                    with col1:\n                        st.metric(\"Total Processed\", batch_results['total_processed'])\n                    with col2:\n                        st.metric(\"Successful\", batch_results['successful'])\n                    with col3:\n                        st.metric(\"Failed\", batch_results['failed'])\n                    \n                    # Generate and display report\n                    if generate_report and int(batch_results.get('successful', 0)) > 0:\n                        st.subheader(\"üìä Batch Processing Report\")\n                        \n                        report = processor.generate_batch_report(batch_results, job_title)\n                        if report:\n                            # Summary metrics\n                            summary = report['summary']\n                            \n                            col1, col2, col3, col4 = st.columns(4)\n                            with col1:\n                                st.metric(\"Average Score\", f\"{summary['average_score']:.1f}\")\n                            with col2:\n                                st.metric(\"High Potential\", summary['high_potential'])\n                            with col3:\n                                st.metric(\"Medium Potential\", summary['medium_potential'])\n                            with col4:\n                                st.metric(\"Low Potential\", summary['low_potential'])\n                            \n                            # Top candidates\n                            if summary['top_candidates']:\n                                st.subheader(\"üèÜ Top Candidates\")\n                                top_df = pd.DataFrame(summary['top_candidates'])\n                                st.dataframe(top_df, use_container_width=True)\n                            \n                            # Full results\n                            st.subheader(\"üìã All Results\")\n                            st.dataframe(report['dataframe'], use_container_width=True)\n                            \n                            # Download report\n                            csv = report['dataframe'].to_csv(index=False)\n                            st.download_button(\n                                label=\"üì• Download Results CSV\",\n                                data=csv,\n                                file_name=f\"batch_evaluation_{job_title}_{time.strftime('%Y%m%d_%H%M%S')}.csv\",\n                                mime=\"text/csv\"\n                            )\n                    \n                    # Error details\n                    failed_results = [r for r in batch_results.get('results', []) if isinstance(r, dict) and r.get('status') == 'error']\n                    if failed_results:\n                        with st.expander(f\"‚ùå Failed Files ({len(failed_results)})\"):\n                            for failed in failed_results:\n                                st.error(f\"**{failed.get('filename', 'Unknown')}**: {failed.get('error', 'Unknown error')}\")\n                \n                except Exception as e:\n                    st.error(f\"‚ùå Batch processing failed: {str(e)}\")\n                    status_text.text(\"Processing failed\")\n    \n    # Usage tips\n    st.markdown(\"---\")\n    st.subheader(\"üí° Performance Tips\")\n    \n    tips_col1, tips_col2 = st.columns(2)\n    \n    with tips_col1:\n        st.markdown(\"\"\"\n        **Optimization:**\n        - Use 4-6 concurrent workers for best performance\n        - Process files in chunks of 25-50 for large batches\n        - Ensure stable internet connection for AI features\n        - Use smaller file sizes when possible\n        \"\"\")\n    \n    with tips_col2:\n        st.markdown(\"\"\"\n        **Best Practices:**\n        - Process during off-peak hours for large batches\n        - Validate file formats before uploading\n        - Keep backup of original files\n        - Monitor system resources during processing\n        \"\"\")","size_bytes":16954},"replit.md":{"content":"# Overview\n\nThe AI-Powered Resume Evaluation System is an automated solution designed for Innomatics Research Labs to streamline their resume screening process across multiple locations (Hyderabad, Bangalore, Pune, and Delhi NCR). The system evaluates resumes against job descriptions using a hybrid approach that combines rule-based keyword matching with AI-powered semantic analysis to generate relevance scores (0-100) and provide actionable feedback to both placement teams and students.\n\nThe application serves two main user groups: placement team members who require authentication to access advanced features, and students who can use a simplified portal without login requirements. The system processes PDF and DOCX resume files, extracts and analyzes text content, and provides comprehensive evaluation reports with missing skills identification and improvement suggestions.\n\n# User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n# System Architecture\n\n## Frontend Architecture\n- **Framework**: Streamlit web application with responsive design\n- **Page Structure**: Multi-page navigation with sidebar menu supporting Student Portal, Dashboard, Job Description Upload, Resume Upload, Batch Evaluation, and Analytics\n- **Authentication Flow**: Role-based access with unauthenticated student portal and authenticated staff portal\n- **State Management**: Streamlit session state for user authentication and application data persistence\n\n## Backend Architecture\n- **Application Layer**: Python-based modular architecture with specialized processors\n- **Text Processing Pipeline**: \n  - Text extraction from PDF (PyMuPDF) and DOCX (python-docx) files\n  - NLP processing using spaCy for entity recognition and skill extraction\n  - Semantic analysis integration with OpenAI GPT models\n- **Scoring Engine**: Hybrid evaluation system combining hard keyword matching with soft semantic similarity using TF-IDF vectorization and cosine similarity\n- **Batch Processing**: Concurrent resume processing using ThreadPoolExecutor for scalable bulk evaluations\n\n## Data Storage Solutions\n- **Primary Database**: PostgreSQL with connection pooling and environment-variable configuration\n- **Schema Design**: \n  - User management with bcrypt password hashing and session tokens\n  - Job descriptions with structured skill requirements and qualifications\n  - Resume storage with extracted text and metadata\n  - Evaluation results with detailed scoring breakdowns\n- **Fallback Support**: SQLite database implementation for local development\n\n## Authentication and Authorization\n- **Password Security**: bcrypt hashing with salt for secure password storage\n- **Session Management**: Token-based sessions with configurable expiration (24-hour default)\n- **Role-based Access**: Placement team authentication required for administrative features\n- **Student Access**: Open access portal for resume submission and basic evaluation features\n\n## Processing Architecture\n- **Skill Extraction**: Comprehensive technical skills database with fuzzy matching capabilities\n- **Experience Analysis**: Pattern recognition for extracting years of experience and education levels\n- **Scoring Algorithm**: Weighted combination of required skills (70%), preferred skills (20%), and experience matching (10%)\n- **Analytics Engine**: Statistical analysis and visualization using Plotly for evaluation trends and insights\n\n# External Dependencies\n\n## AI and Machine Learning Services\n- **OpenAI API**: GPT model integration for semantic resume analysis and improvement suggestions\n- **spaCy**: Natural language processing library for entity recognition and text analysis\n- **scikit-learn**: Machine learning utilities for TF-IDF vectorization and similarity calculations\n- **FuzzyWuzzy**: Fuzzy string matching for flexible skill and keyword matching\n\n## Database and Storage\n- **PostgreSQL**: Primary production database with full ACID compliance\n- **psycopg2**: PostgreSQL adapter for Python database connectivity\n- **SQLite**: Alternative database option for development and testing environments\n\n## Document Processing\n- **PyMuPDF (fitz)**: PDF text extraction and processing capabilities\n- **python-docx**: Microsoft Word document text extraction\n- **Streamlit**: Web application framework for user interface and file upload handling\n\n## Data Analysis and Visualization\n- **pandas**: Data manipulation and analysis for evaluation results\n- **numpy**: Numerical computing support for scoring algorithms\n- **Plotly**: Interactive data visualization for analytics dashboard\n- **matplotlib/seaborn**: Statistical plotting and chart generation\n\n## Security and Utilities\n- **bcrypt**: Password hashing and verification for user authentication\n- **secrets**: Cryptographically secure token generation for sessions\n- **concurrent.futures**: Thread pool execution for batch processing optimization\n\n## Environment Configuration\n- **Environment Variables**: Database connection strings, API keys, and configuration parameters\n- **Streamlit Secrets**: Secure credential management for deployed applications","size_bytes":5090},"LOCAL_SETUP_GUIDE.md":{"content":"# InnoVantage - Local Development Setup Guide\n\n## Quick Start Guide for Running the Project Locally\n\nThis guide will help you set up and run the InnoVantage Resume Evaluation System on your local machine.\n\n---\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Installation Methods](#installation-methods)\n3. [Environment Setup](#environment-setup)\n4. [Database Configuration](#database-configuration)\n5. [Running the Application](#running-the-application)\n6. [Troubleshooting](#troubleshooting)\n7. [Development Workflow](#development-workflow)\n\n---\n\n## Prerequisites\n\n### System Requirements\n- **Operating System**: Windows 10+, macOS 10.14+, or Linux (Ubuntu 18.04+)\n- **Memory**: Minimum 4GB RAM (8GB recommended)\n- **Storage**: At least 2GB free space\n- **Network**: Internet connection for AI features and package installation\n\n### Required Software\n- **Python 3.11+**: Download from [python.org](https://python.org/downloads/)\n- **Git**: Download from [git-scm.com](https://git-scm.com/downloads)\n- **PostgreSQL** (Optional): Download from [postgresql.org](https://postgresql.org/download/)\n\n---\n\n## Installation Methods\n\n### Method 1: Using UV (Recommended)\n\nUV is a fast Python package manager that's already configured for this project.\n\n#### Step 1: Install UV\n```bash\n# On macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Alternative: Using pip\npip install uv\n```\n\n#### Step 2: Clone and Setup\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd innomatics-resume-evaluator\n\n# Install dependencies using UV\nuv sync\n\n# Download spaCy language model\nuv run python -m spacy download en_core_web_sm\n```\n\n### Method 2: Using Traditional pip + venv\n\n#### Step 1: Create Virtual Environment\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd innomatics-resume-evaluator\n\n# Create virtual environment\npython -m venv .venv\n\n# Activate virtual environment\n# On Windows:\n.venv\\Scripts\\activate\n# On macOS/Linux:\nsource .venv/bin/activate\n```\n\n#### Step 2: Install Dependencies\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Alternative: Install from pyproject.toml\npip install -e .\n\n# Download spaCy language model\npython -m spacy download en_core_web_sm\n```\n\n---\n\n## Environment Setup\n\n### Step 1: Create Environment File\nCreate a `.env` file in the project root:\n\n```bash\n# Copy the example environment file\ncp .env.example .env\n```\n\n### Step 2: Configure Environment Variables\nEdit the `.env` file with your settings:\n\n```bash\n# Database Configuration (Optional - defaults to SQLite)\nDATABASE_URL=postgresql://username:password@localhost:5432/resume_evaluation\n\n# OpenAI API Configuration (Optional - enables AI features)\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Application Settings\nFLASK_ENV=development\nDEBUG=True\n\n# Streamlit Configuration\nSTREAMLIT_SERVER_PORT=5000\nSTREAMLIT_SERVER_ADDRESS=0.0.0.0\n```\n\n### Step 3: API Key Setup (Optional)\n\n#### OpenAI API Key\n1. Visit [OpenAI API](https://platform.openai.com/api-keys)\n2. Create an account or sign in\n3. Generate a new API key\n4. Add it to your `.env` file\n\n**Note**: The application works without OpenAI API key but with limited AI features.\n\n---\n\n## Database Configuration\n\n### Option 1: SQLite (Default - No Setup Required)\nThe application automatically creates a SQLite database file. No additional setup needed.\n\n```bash\n# SQLite database will be created automatically as:\n# resume_evaluation.db\n```\n\n### Option 2: PostgreSQL (Recommended for Production)\n\n#### Install PostgreSQL\n```bash\n# On macOS using Homebrew\nbrew install postgresql\nbrew services start postgresql\n\n# On Ubuntu/Debian\nsudo apt update\nsudo apt install postgresql postgresql-contrib\nsudo systemctl start postgresql\n\n# On Windows\n# Download and install from postgresql.org\n```\n\n#### Create Database\n```bash\n# Access PostgreSQL\nsudo -u postgres psql\n\n# Create database and user\nCREATE DATABASE resume_evaluation;\nCREATE USER resume_user WITH PASSWORD 'your_password';\nGRANT ALL PRIVILEGES ON DATABASE resume_evaluation TO resume_user;\n\\q\n```\n\n#### Update Environment\n```bash\n# Add to .env file\nDATABASE_URL=postgresql://resume_user:your_password@localhost:5432/resume_evaluation\n```\n\n---\n\n## Running the Application\n\n### Method 1: Using UV (Recommended)\n```bash\n# Navigate to project directory\ncd innomatics-resume-evaluator\n\n# Run the application\nuv run streamlit run app.py\n\n# Alternative: Run with specific configuration\nuv run streamlit run app.py --server.port 5000 --server.address 0.0.0.0\n```\n\n### Method 2: Using Traditional Python\n```bash\n# Activate virtual environment (if using venv)\nsource .venv/bin/activate  # On macOS/Linux\n# or\n.venv\\Scripts\\activate     # On Windows\n\n# Run the application\nstreamlit run app.py\n\n# Alternative: Run with specific configuration\nstreamlit run app.py --server.port 5000 --server.address 0.0.0.0\n```\n\n### Access the Application\nOnce running, the application will be available at:\n- **Local**: http://localhost:5000\n- **Network**: http://0.0.0.0:5000\n\n---\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n#### 1. Port Already in Use\n```bash\n# Find process using port 5000\nlsof -i :5000  # On macOS/Linux\nnetstat -ano | findstr :5000  # On Windows\n\n# Kill the process or use different port\nstreamlit run app.py --server.port 8501\n```\n\n#### 2. Package Installation Errors\n```bash\n# Clear pip cache\npip cache purge\n\n# Upgrade pip\npip install --upgrade pip\n\n# Install packages one by one\npip install streamlit pandas numpy\n```\n\n#### 3. Database Connection Issues\n```bash\n# Check PostgreSQL service\nsudo systemctl status postgresql  # On Linux\nbrew services list | grep postgresql  # On macOS\n\n# Test database connection\npsql postgresql://username:password@localhost:5432/database_name\n```\n\n#### 4. spaCy Model Download Issues\n```bash\n# Manual download\npython -m spacy download en_core_web_sm\n\n# Alternative download method\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0.tar.gz\n```\n\n#### 5. Memory Issues\n```bash\n# Increase virtual memory (if needed)\n# On Linux: Add swap space\nsudo fallocate -l 2G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n```\n\n### Performance Optimization\n\n#### 1. Python Optimization\n```bash\n# Use Python 3.11+ for better performance\npython --version\n\n# Enable bytecode optimization\nexport PYTHONOPTIMIZE=2\n```\n\n#### 2. Memory Management\n```bash\n# Monitor memory usage\npip install memory-profiler\npython -m memory_profiler app.py\n```\n\n#### 3. Database Optimization\n```bash\n# For PostgreSQL: Tune configuration\n# Edit postgresql.conf:\nshared_buffers = 256MB\nwork_mem = 4MB\nmaintenance_work_mem = 64MB\n```\n\n---\n\n## Development Workflow\n\n### Project Structure\n```\ninnomatics-resume-evaluator/\n‚îú‚îÄ‚îÄ app.py                 # Main application entry point\n‚îú‚îÄ‚îÄ dashboard.py           # UI components and pages\n‚îú‚îÄ‚îÄ auth.py               # Authentication system\n‚îú‚îÄ‚îÄ database_postgres.py  # Database operations\n‚îú‚îÄ‚îÄ text_extractor.py     # File processing\n‚îú‚îÄ‚îÄ nlp_processor.py      # NLP and AI processing\n‚îú‚îÄ‚îÄ scoring_engine.py     # Evaluation algorithms\n‚îú‚îÄ‚îÄ batch_processor.py    # Batch processing system\n‚îú‚îÄ‚îÄ utils.py              # Utility functions\n‚îú‚îÄ‚îÄ pyproject.toml        # Project dependencies\n‚îú‚îÄ‚îÄ .streamlit/           # Streamlit configuration\n‚îÇ   ‚îî‚îÄ‚îÄ config.toml\n‚îú‚îÄ‚îÄ test_files/           # Sample files for testing\n‚îú‚îÄ‚îÄ attached_assets/      # Generated assets\n‚îî‚îÄ‚îÄ .env                  # Environment variables\n```\n\n### Development Commands\n```bash\n# Install development dependencies\nuv add --dev pytest black flake8 mypy\n\n# Run tests\nuv run pytest\n\n# Format code\nuv run black .\n\n# Check code quality\nuv run flake8 .\nuv run mypy .\n\n# Update dependencies\nuv lock --upgrade\n```\n\n### Testing the Application\n\n#### 1. Basic Functionality Test\n1. Access the application at http://localhost:5000\n2. Navigate to \"Student Portal\"\n3. Upload a sample resume (use files in `test_files/`)\n4. Verify evaluation results are displayed\n\n#### 2. Authentication Test\n1. Navigate to \"Dashboard\" (requires login)\n2. Register a new account\n3. Login with created credentials\n4. Verify access to authenticated features\n\n#### 3. Job Description Test\n1. Login to the system\n2. Navigate to \"Job Descriptions\"\n3. Upload a sample job description\n4. Verify parsing and storage\n\n#### 4. Batch Processing Test\n1. Navigate to \"Batch Processing\"\n2. Upload multiple resume files\n3. Monitor progress and results\n4. Download generated reports\n\n---\n\n## Production Deployment\n\n### Environment Preparation\n```bash\n# Set production environment\nexport FLASK_ENV=production\nexport DEBUG=False\n\n# Use production database\nexport DATABASE_URL=postgresql://prod_user:prod_pass@prod_host:5432/prod_db\n```\n\n### Security Considerations\n- Use strong passwords for database users\n- Enable SSL/TLS for database connections\n- Set up proper firewall rules\n- Regular security updates\n- Monitor access logs\n\n### Performance Tuning\n- Use connection pooling for database\n- Enable gzip compression\n- Set up caching layers\n- Monitor resource usage\n- Scale horizontally as needed\n\n---\n\n## Getting Help\n\n### Support Resources\n- **Documentation**: This guide and PROJECT_ARCHITECTURE.md\n- **Issues**: Check existing issues or create new ones\n- **Community**: Join development discussions\n- **Contact**: reach out to development team\n\n### Log Files and Debugging\n```bash\n# Application logs\ntail -f logs/application.log\n\n# Database logs (PostgreSQL)\ntail -f /var/log/postgresql/postgresql-*.log\n\n# Streamlit logs\nstreamlit run app.py --logger.level debug\n```\n\n---\n\n## FAQ\n\n**Q: Can I run this without an OpenAI API key?**\nA: Yes, the system will fall back to rule-based evaluation methods.\n\n**Q: What file formats are supported for resumes?**\nA: PDF and DOCX formats are fully supported.\n\n**Q: How much memory does the application use?**\nA: Typically 500MB-1GB depending on batch processing load.\n\n**Q: Can I use a different database?**\nA: The system is designed for PostgreSQL but includes SQLite fallback.\n\n**Q: Is this suitable for production use?**\nA: Yes, with proper configuration and security measures.\n\n---\n\n*For additional support or questions, please refer to the project documentation or contact the development team.*","size_bytes":10406},"PROJECT_ARCHITECTURE.md":{"content":"# InnoVantage - AI-Powered Resume Evaluation System\n\n## Architecture & Functionality Documentation\n\n### Table of Contents\n1. [System Overview](#system-overview)\n2. [Architecture Components](#architecture-components)\n3. [Data Flow & Processing Pipeline](#data-flow--processing-pipeline)\n4. [Feature Breakdown](#feature-breakdown)\n5. [Security & Authentication](#security--authentication)\n6. [AI & Machine Learning Integration](#ai--machine-learning-integration)\n7. [Database Schema](#database-schema)\n8. [API Endpoints & Functions](#api-endpoints--functions)\n\n---\n\n## System Overview\n\nInnoVantage is a comprehensive AI-powered resume evaluation system designed for Innomatics Research Labs to automate and standardize their recruitment process across multiple locations (Hyderabad, Bangalore, Pune, and Delhi NCR).\n\n### Key Capabilities\n- **Automated Resume Parsing**: Extracts text and structured data from PDF/DOCX files\n- **AI-Powered Evaluation**: Uses hybrid scoring combining rule-based matching with semantic analysis\n- **Batch Processing**: Concurrent processing of multiple resumes with real-time progress tracking\n- **Role-Based Access**: Separate portals for students and placement teams\n- **Advanced Analytics**: Comprehensive insights and performance metrics\n- **Modern UI**: Streamlit-based interface with responsive design and modern styling\n\n---\n\n## Architecture Components\n\n### Frontend Layer\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ           Streamlit Web App         ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  ‚Ä¢ Modern UI with gradient styling  ‚îÇ\n‚îÇ  ‚Ä¢ Responsive multi-page navigation ‚îÇ\n‚îÇ  ‚Ä¢ Real-time progress tracking      ‚îÇ\n‚îÇ  ‚Ä¢ Interactive charts & analytics   ‚îÇ\n‚îÇ  ‚Ä¢ File upload & batch processing   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Application Layer\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         Core Application            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  app.py          ‚îÇ Main entry point ‚îÇ\n‚îÇ  dashboard.py    ‚îÇ UI components    ‚îÇ\n‚îÇ  auth.py         ‚îÇ Authentication   ‚îÇ\n‚îÇ  batch_processor.py ‚îÇ Bulk processing‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Processing Layer\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ        Processing Engines           ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  text_extractor.py   ‚îÇ File parsing ‚îÇ\n‚îÇ  nlp_processor.py    ‚îÇ NLP analysis ‚îÇ\n‚îÇ  scoring_engine.py   ‚îÇ Evaluation   ‚îÇ\n‚îÇ  utils.py           ‚îÇ Utilities    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Data Layer\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         Database Layer              ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  database_postgres.py ‚îÇ Main DB ops ‚îÇ\n‚îÇ  database.py         ‚îÇ SQLite backup‚îÇ\n‚îÇ  PostgreSQL Database ‚îÇ Primary store‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## Data Flow & Processing Pipeline\n\n### 1. Resume Upload & Processing\n```\nUser Upload ‚Üí File Validation ‚Üí Text Extraction ‚Üí NLP Processing ‚Üí Database Storage\n     ‚Üì              ‚Üì               ‚Üì              ‚Üì               ‚Üì\n   PDF/DOCX    Size/Type Check   PyMuPDF/docx   spaCy/OpenAI   PostgreSQL\n```\n\n### 2. Job Description Processing\n```\nJob Text Input ‚Üí NLP Analysis ‚Üí Skill Extraction ‚Üí Requirements Parsing ‚Üí Database Storage\n      ‚Üì              ‚Üì             ‚Üì                 ‚Üì                ‚Üì\n   Raw Text      OpenAI/spaCy   Pattern Matching   JSON Structure   PostgreSQL\n```\n\n### 3. Evaluation Pipeline\n```\nResume + Job ‚Üí Feature Extraction ‚Üí Scoring Algorithm ‚Üí Verdict Generation ‚Üí Results Storage\n     ‚Üì              ‚Üì                    ‚Üì                  ‚Üì              ‚Üì\n   Text Data    Skills/Experience   Hybrid Scoring     High/Med/Low    Database\n```\n\n### 4. Batch Processing Flow\n```\nMultiple Files ‚Üí ThreadPoolExecutor ‚Üí Parallel Processing ‚Üí Results Aggregation ‚Üí Report Generation\n      ‚Üì                ‚Üì                     ‚Üì                    ‚Üì                ‚Üì\n   File Queue      Concurrent Tasks      Individual Results      Summary Stats     CSV/JSON Export\n```\n\n---\n\n## Feature Breakdown\n\n### 1. Student Portal (Unauthenticated)\n- **Purpose**: Allow students to evaluate their resumes without login\n- **Features**:\n  - Simple resume upload interface\n  - Basic evaluation results\n  - Improvement suggestions\n  - No data persistence (privacy-focused)\n\n### 2. Dashboard (Authenticated)\n- **Purpose**: Executive overview for placement teams\n- **Components**:\n  - Real-time metrics display\n  - Verdict distribution charts\n  - Recent evaluations table\n  - Quick navigation to other features\n\n### 3. Job Description Management\n- **Upload Interface**: Structured form for job details\n- **NLP Processing**: Automatic skill and requirement extraction\n- **Storage**: Parsed requirements saved for future evaluations\n\n### 4. Resume Upload & Evaluation\n- **File Processing**: Support for PDF and DOCX formats\n- **Text Extraction**: Advanced parsing with error handling\n- **Evaluation Engine**: Multi-factor scoring algorithm\n- **Results Display**: Comprehensive feedback and suggestions\n\n### 5. Batch Processing System\n- **Concurrent Processing**: ThreadPoolExecutor for parallel operations\n- **Progress Tracking**: Real-time status updates\n- **Error Handling**: Individual file error tracking\n- **Reporting**: Comprehensive batch analysis reports\n\n### 6. Advanced Analytics\n- **Performance Metrics**: Score distributions and trends\n- **Job Analysis**: Position-wise candidate quality\n- **Skills Gap Analysis**: Missing skills identification\n- **Recommendation Engine**: AI-powered insights\n\n---\n\n## Security & Authentication\n\n### Authentication System\n```python\nbcrypt Password Hashing ‚Üí Session Token Generation ‚Üí Database Session Storage ‚Üí Middleware Validation\n        ‚Üì                        ‚Üì                         ‚Üì                      ‚Üì\n   Secure Storage          32-byte URL-safe token      24-hour expiry        Route Protection\n```\n\n### Security Features\n- **Password Security**: bcrypt hashing with salt\n- **Session Management**: Secure token-based sessions\n- **Role-Based Access**: Different permissions for users\n- **Input Validation**: File type and size restrictions\n- **Error Handling**: Safe error messages without data exposure\n\n### User Roles\n- **Student**: Open access to evaluation features\n- **Placement Team**: Full access to management features\n- **Admin**: Complete system access\n- **Mentor**: Educational features access\n\n---\n\n## AI & Machine Learning Integration\n\n### NLP Processing Pipeline\n```\nText Input ‚Üí spaCy Processing ‚Üí Entity Recognition ‚Üí Skill Extraction ‚Üí Semantic Analysis\n    ‚Üì             ‚Üì                  ‚Üì                ‚Üì                ‚Üì\nRaw Resume    Tokenization      Named Entities    Technical Skills   OpenAI Integration\n```\n\n### Scoring Algorithm\n```python\n# Hybrid Scoring Model\nfinal_score = (\n    hard_match_score * 0.7 +      # Exact keyword matching\n    semantic_score * 0.2 +        # AI semantic similarity\n    experience_score * 0.1        # Experience matching\n)\n```\n\n### AI Integration Points\n1. **Job Description Parsing**: OpenAI GPT for requirement extraction\n2. **Semantic Matching**: Embeddings-based similarity scoring\n3. **Improvement Suggestions**: AI-generated feedback\n4. **Skills Analysis**: Pattern recognition for skill identification\n\n---\n\n## Database Schema\n\n### Core Tables\n\n#### users\n```sql\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(50) UNIQUE NOT NULL,\n    email VARCHAR(100) UNIQUE NOT NULL,\n    password_hash VARCHAR(255) NOT NULL,\n    location VARCHAR(50),\n    role VARCHAR(20) DEFAULT 'placement_team',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n#### job_descriptions\n```sql\nCREATE TABLE job_descriptions (\n    id SERIAL PRIMARY KEY,\n    title VARCHAR(200) NOT NULL,\n    company VARCHAR(100),\n    location VARCHAR(100),\n    description TEXT NOT NULL,\n    required_skills JSONB,\n    preferred_skills JSONB,\n    other_requirements JSONB,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n#### resumes\n```sql\nCREATE TABLE resumes (\n    id SERIAL PRIMARY KEY,\n    filename VARCHAR(255) NOT NULL,\n    candidate_name VARCHAR(100),\n    candidate_email VARCHAR(100),\n    extracted_text TEXT,\n    skills TEXT,\n    experience TEXT,\n    education TEXT,\n    uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n#### evaluations\n```sql\nCREATE TABLE evaluations (\n    id SERIAL PRIMARY KEY,\n    job_id INTEGER REFERENCES job_descriptions(id),\n    resume_id INTEGER REFERENCES resumes(id),\n    relevance_score DECIMAL(5,2),\n    hard_match_score DECIMAL(5,2),\n    semantic_match_score DECIMAL(5,2),\n    verdict VARCHAR(20),\n    missing_skills JSONB,\n    improvement_suggestions TEXT,\n    evaluation_details JSONB,\n    evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n---\n\n## API Endpoints & Functions\n\n### Core Functions\n\n#### Database Operations\n- `init_database()`: Initialize database connection and tables\n- `save_user()`: Create new user accounts\n- `save_job_description()`: Store job requirements\n- `save_resume()`: Store resume data\n- `save_evaluation()`: Store evaluation results\n- `get_evaluations()`: Retrieve evaluation history\n\n#### Processing Functions\n- `process_uploaded_file()`: Extract text from PDF/DOCX\n- `parse_job_description()`: NLP analysis of job requirements\n- `evaluate_resume()`: Core evaluation algorithm\n- `process_resume_batch()`: Batch processing coordinator\n\n#### Authentication Functions\n- `login_user()`: User authentication\n- `register_user()`: Account creation\n- `check_authentication()`: Session validation\n- `logout_user()`: Session termination\n\n### System Configuration\n\n#### Environment Variables\n```bash\n# Database Configuration\nDATABASE_URL=postgresql://user:pass@host:port/dbname\n\n# AI Integration\nOPENAI_API_KEY=your_openai_api_key\n\n# Application Settings\nFLASK_ENV=production\nDEBUG=False\n```\n\n#### Streamlit Configuration\n```toml\n[theme]\nprimaryColor = \"#1f77b4\"\nbackgroundColor = \"#ffffff\"\nsecondaryBackgroundColor = \"#f0f2f6\"\n\n[server]\nport = 5000\naddress = \"0.0.0.0\"\nmaxUploadSize = 50\n```\n\n---\n\n## Performance & Scalability\n\n### Optimization Features\n- **Concurrent Processing**: ThreadPoolExecutor for batch operations\n- **Database Connection Pooling**: Efficient database connections\n- **Caching**: Session state management for UI performance\n- **File Validation**: Early rejection of invalid files\n- **Error Recovery**: Graceful handling of processing failures\n\n### Scalability Considerations\n- **Horizontal Scaling**: Stateless application design\n- **Database Optimization**: Indexed queries and efficient schema\n- **Memory Management**: Streaming file processing\n- **Background Processing**: Async task handling capability\n\n---\n\n## Monitoring & Maintenance\n\n### Logging & Error Tracking\n- **Application Logs**: Structured logging with severity levels\n- **Error Boundaries**: Safe error handling with user-friendly messages\n- **Performance Metrics**: Processing time and throughput monitoring\n- **Database Monitoring**: Query performance and connection health\n\n### Backup & Recovery\n- **Database Backups**: Automated PostgreSQL backups\n- **Configuration Management**: Version-controlled settings\n- **Disaster Recovery**: Multi-environment deployment capability\n\n---\n\n## Future Enhancement Opportunities\n\n### Technical Improvements\n- **API Layer**: RESTful API for external integrations\n- **Mobile App**: React Native mobile application\n- **Advanced ML**: Custom ML models for resume scoring\n- **Real-time Features**: WebSocket-based live updates\n\n### Business Features\n- **Interview Scheduling**: Integrated calendar management\n- **Candidate Tracking**: Complete recruitment pipeline\n- **Report Templates**: Customizable report generation\n- **Multi-tenant**: Support for multiple organizations\n\n---\n\n*This documentation serves as a comprehensive guide to understanding the InnoVantage system architecture and functionality. For technical support or questions, contact the development team.*","size_bytes":13046},".streamlit/config.toml":{"content":"[theme]\nprimaryColor = \"#1f77b4\"\nbackgroundColor = \"#ffffff\"\nsecondaryBackgroundColor = \"#f0f2f6\"\ntextColor = \"#262730\"\nfont = \"sans serif\"\n\n[server]\nheadless = true\naddress = \"0.0.0.0\"\nport = 5000\nmaxUploadSize = 50\n\n[browser]\ngatherUsageStats = false\n","size_bytes":253}},"version":1}