{"file_contents":{"app.py":{"content":"import streamlit as st\nimport os\nfrom database_postgres import init_database\nfrom dashboard import render_dashboard\n\n# Initialize database on app start\nif 'initialized' not in st.session_state:\n    init_database()\n    st.session_state.initialized = True\n\ndef main():\n    st.set_page_config(\n        page_title=\"Resume Evaluation System - Innomatics Research Labs\",\n        page_icon=\"📄\",\n        layout=\"wide\",\n        initial_sidebar_state=\"expanded\"\n    )\n    \n    st.title(\"🎯 AI-Powered Resume Evaluation System\")\n    st.markdown(\"**Innomatics Research Labs** - Automated Resume Relevance Check\")\n    \n    # Check for OpenAI API key\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        st.error(\"⚠️ OpenAI API Key not found. Please set the OPENAI_API_KEY environment variable.\")\n        st.info(\"Contact your system administrator to configure the API key.\")\n        return\n    \n    # Sidebar navigation\n    st.sidebar.title(\"Navigation\")\n    page = st.sidebar.selectbox(\n        \"Select Page\",\n        [\"Dashboard\", \"Upload Job Description\", \"Upload Resume\", \"Batch Evaluation\", \"Analytics\"]\n    )\n    \n    # Render selected page\n    render_dashboard(page)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":1226},"dashboard.py":{"content":"import streamlit as st\nimport pandas as pd\nimport json\nfrom datetime import datetime\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom text_extractor import process_uploaded_file\nfrom nlp_processor import parse_job_description\nfrom scoring_engine import ResumeScorer\nfrom database_postgres import (\n    save_job_description, save_resume, save_evaluation, \n    get_job_descriptions, get_resumes, get_evaluations, \n    get_job_by_id, get_resume_by_id, get_evaluation_stats\n)\n\ndef render_dashboard(page):\n    \"\"\"Render the selected dashboard page\"\"\"\n    \n    if page == \"Dashboard\":\n        render_main_dashboard()\n    elif page == \"Upload Job Description\":\n        render_job_upload_page()\n    elif page == \"Upload Resume\":\n        render_resume_upload_page()\n    elif page == \"Batch Evaluation\":\n        render_batch_evaluation_page()\n    elif page == \"Analytics\":\n        render_analytics_page()\n\ndef render_main_dashboard():\n    \"\"\"Render main dashboard with overview and recent evaluations\"\"\"\n    st.header(\"📊 Dashboard Overview\")\n    \n    # Get statistics\n    stats = get_evaluation_stats()\n    \n    # Display key metrics\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"Total Jobs\", stats['total_jobs'])\n    \n    with col2:\n        st.metric(\"Total Resumes\", stats['total_resumes'])\n    \n    with col3:\n        st.metric(\"Total Evaluations\", stats['total_evaluations'])\n    \n    with col4:\n        st.metric(\"Avg Score\", f\"{stats['average_score']:.1f}\")\n    \n    # Verdict distribution chart\n    if stats['verdict_distribution']:\n        st.subheader(\"Verdict Distribution\")\n        verdict_df = pd.DataFrame(list(stats['verdict_distribution'].items()), \n                                columns=['Verdict', 'Count'])\n        \n        fig = px.pie(verdict_df, values='Count', names='Verdict', \n                    color_discrete_map={'High': '#00ff00', 'Medium': '#ffff00', 'Low': '#ff0000'})\n        st.plotly_chart(fig, use_container_width=True)\n    \n    # Recent evaluations\n    st.subheader(\"Recent Evaluations\")\n    recent_evaluations = get_evaluations()[:10]  # Get last 10 evaluations\n    \n    if recent_evaluations:\n        eval_data = []\n        for eval_row in recent_evaluations:\n            # PostgreSQL query returns: e.*, j.title as job_title, j.company, r.filename, r.candidate_name, r.candidate_email\n            # So the columns are: evaluation columns (0-10) + job_title (11) + company (12) + filename (13) + candidate_name (14) + candidate_email (15)\n            eval_data.append({\n                'Job Title': eval_row[11] if len(eval_row) > 11 else 'Unknown',  # job_title\n                'Company': eval_row[12] if len(eval_row) > 12 else 'Unknown',    # company\n                'Candidate': eval_row[14] if len(eval_row) > 14 else 'Unknown',  # candidate_name\n                'Score': eval_row[3],       # relevance_score\n                'Verdict': eval_row[6],     # verdict\n                'Date': eval_row[10]        # evaluated_at\n            })\n        \n        df = pd.DataFrame(eval_data)\n        st.dataframe(df, use_container_width=True)\n    else:\n        st.info(\"No evaluations found. Start by uploading job descriptions and resumes!\")\n\ndef render_job_upload_page():\n    \"\"\"Render job description upload page\"\"\"\n    st.header(\"📝 Upload Job Description\")\n    \n    with st.form(\"job_upload_form\"):\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            job_title = st.text_input(\"Job Title*\", placeholder=\"e.g., Senior Data Scientist\")\n            company = st.text_input(\"Company\", placeholder=\"e.g., Tech Corp\")\n        \n        with col2:\n            location = st.text_input(\"Location\", placeholder=\"e.g., Hyderabad, India\")\n        \n        job_description = st.text_area(\"Job Description*\", height=300, \n                                     placeholder=\"Paste the complete job description here...\")\n        \n        submitted = st.form_submit_button(\"Parse and Save Job Description\")\n        \n        if submitted:\n            if not job_title or not job_description:\n                st.error(\"Please fill in all required fields (marked with *)\")\n            else:\n                with st.spinner(\"Parsing job description...\"):\n                    # Parse job description using NLP\n                    parsed_requirements = parse_job_description(job_description)\n                    \n                    # Save to database\n                    job_id = save_job_description(\n                        job_title, company, location, job_description,\n                        parsed_requirements.get('required_skills', []),\n                        parsed_requirements.get('preferred_skills', []),\n                        {\n                            'experience_required': parsed_requirements.get('experience_required', 0),\n                            'education_required': parsed_requirements.get('education_required', 'unknown'),\n                            'key_responsibilities': parsed_requirements.get('key_responsibilities', [])\n                        }\n                    )\n                \n                st.success(f\"✅ Job description saved successfully! (ID: {job_id})\")\n                \n                # Display parsed information\n                st.subheader(\"Parsed Job Requirements\")\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    st.write(\"**Required Skills:**\")\n                    for skill in parsed_requirements.get('required_skills', []):\n                        st.write(f\"• {skill}\")\n                \n                with col2:\n                    st.write(\"**Preferred Skills:**\")\n                    for skill in parsed_requirements.get('preferred_skills', []):\n                        st.write(f\"• {skill}\")\n                \n                st.write(f\"**Experience Required:** {parsed_requirements.get('experience_required', 0)} years\")\n                st.write(f\"**Education Required:** {parsed_requirements.get('education_required', 'Not specified')}\")\n\ndef render_resume_upload_page():\n    \"\"\"Render resume upload page\"\"\"\n    st.header(\"📄 Upload Resume\")\n    \n    # Job selection\n    jobs = get_job_descriptions()\n    if not jobs:\n        st.warning(\"Please upload at least one job description first!\")\n        return\n    \n    job_options = {f\"{job[1]} - {job[2] or 'Company Not Specified'}\": job[0] for job in jobs}\n    selected_job = st.selectbox(\"Select Job Position\", options=list(job_options.keys()))\n    \n    if selected_job:\n        job_id = job_options[selected_job]\n        \n        # File upload\n        uploaded_file = st.file_uploader(\n            \"Upload Resume\", \n            type=['pdf', 'docx', 'doc'],\n            help=\"Supported formats: PDF, DOCX, DOC\"\n        )\n        \n        if uploaded_file:\n            with st.spinner(\"Processing resume...\"):\n                # Extract text and information from resume\n                resume_data = process_uploaded_file(uploaded_file)\n                \n                if resume_data:\n                    # Display extracted information\n                    st.subheader(\"Extracted Information\")\n                    \n                    col1, col2 = st.columns(2)\n                    with col1:\n                        st.write(\"**Contact Information:**\")\n                        st.write(f\"Name: {resume_data['contact_info'].get('name', 'Not found')}\")\n                        st.write(f\"Email: {resume_data['contact_info'].get('email', 'Not found')}\")\n                        st.write(f\"Phone: {resume_data['contact_info'].get('phone', 'Not found')}\")\n                    \n                    with col2:\n                        st.write(\"**File Information:**\")\n                        st.write(f\"Filename: {resume_data['filename']}\")\n                        st.write(f\"Text Length: {len(resume_data['extracted_text'])} characters\")\n                    \n                    # Show extracted text preview\n                    with st.expander(\"Preview Extracted Text\"):\n                        st.text_area(\"Extracted Text\", resume_data['extracted_text'][:1000] + \"...\" if len(resume_data['extracted_text']) > 1000 else resume_data['extracted_text'], height=200)\n                    \n                    # Save and evaluate buttons\n                    col1, col2 = st.columns(2)\n                    \n                    with col1:\n                        if st.button(\"Save Resume\", type=\"primary\"):\n                            # Save resume to database\n                            resume_id = save_resume(\n                                resume_data['filename'],\n                                resume_data['contact_info'].get('name', ''),\n                                resume_data['contact_info'].get('email', ''),\n                                resume_data['extracted_text'],\n                                resume_data['sections'].get('skills', ''),\n                                resume_data['sections'].get('experience', ''),\n                                resume_data['sections'].get('education', '')\n                            )\n                            st.success(f\"✅ Resume saved successfully! (ID: {resume_id})\")\n                    \n                    with col2:\n                        if st.button(\"Evaluate Resume\", type=\"secondary\"):\n                            # Get job details\n                            job_details = get_job_by_id(job_id)\n                            if job_details:\n                                with st.spinner(\"Evaluating resume...\"):\n                                    # Prepare job requirements\n                                    job_requirements = {\n                                        'required_skills': json.loads(job_details[5]) if job_details[5] else [],\n                                        'preferred_skills': json.loads(job_details[6]) if job_details[6] else [],\n                                        'experience_required': json.loads(job_details[7]).get('experience_required', 0) if job_details[7] else 0,\n                                        'education_required': json.loads(job_details[7]).get('education_required', 'unknown') if job_details[7] else 'unknown'\n                                    }\n                                    \n                                    # Initialize scorer and evaluate\n                                    scorer = ResumeScorer()\n                                    evaluation_result = scorer.evaluate_resume(\n                                        resume_data, job_details[3], job_requirements\n                                    )\n                                    \n                                    # Display evaluation results\n                                    display_evaluation_results(evaluation_result)\n\ndef render_batch_evaluation_page():\n    \"\"\"Render batch evaluation page\"\"\"\n    st.header(\"🔄 Batch Evaluation\")\n    \n    # Job selection\n    jobs = get_job_descriptions()\n    if not jobs:\n        st.warning(\"Please upload at least one job description first!\")\n        return\n    \n    job_options = {f\"{job[1]} - {job[2] or 'Company Not Specified'}\": job[0] for job in jobs}\n    selected_job = st.selectbox(\"Select Job Position for Batch Evaluation\", options=list(job_options.keys()))\n    \n    if selected_job:\n        job_id = job_options[selected_job]\n        \n        # Get all resumes\n        resumes = get_resumes()\n        if not resumes:\n            st.warning(\"No resumes found. Please upload some resumes first!\")\n            return\n        \n        st.write(f\"Found {len(resumes)} resumes to evaluate\")\n        \n        if st.button(\"Start Batch Evaluation\", type=\"primary\"):\n            # Get job details\n            job_details = get_job_by_id(job_id)\n            if job_details:\n                job_requirements = {\n                    'required_skills': json.loads(job_details[5]) if job_details[5] else [],\n                    'preferred_skills': json.loads(job_details[6]) if job_details[6] else [],\n                    'experience_required': json.loads(job_details[7]).get('experience_required', 0) if job_details[7] else 0,\n                    'education_required': json.loads(job_details[7]).get('education_required', 'unknown') if job_details[7] else 'unknown'\n                }\n                \n                # Initialize scorer\n                scorer = ResumeScorer()\n                \n                # Progress bar\n                progress_bar = st.progress(0)\n                status_text = st.empty()\n                \n                evaluated_count = 0\n                total_resumes = len(resumes)\n                \n                for i, resume in enumerate(resumes):\n                    status_text.text(f\"Evaluating resume {i+1}/{total_resumes}: {resume[1]}\")\n                    \n                    # Prepare resume data\n                    resume_data = {\n                        'filename': resume[1],\n                        'extracted_text': resume[3],\n                        'contact_info': {\n                            'name': resume[2],\n                            'email': resume[3]\n                        },\n                        'sections': {\n                            'skills': json.loads(resume[4]) if resume[4] else '',\n                            'experience': json.loads(resume[5]) if resume[5] else '',\n                            'education': json.loads(resume[6]) if resume[6] else ''\n                        }\n                    }\n                    \n                    # Evaluate resume\n                    evaluation_result = scorer.evaluate_resume(\n                        resume_data, job_details[3], job_requirements\n                    )\n                    \n                    # Save evaluation\n                    save_evaluation(\n                        job_id, resume[0],\n                        evaluation_result['relevance_score'],\n                        evaluation_result['hard_match_score'],\n                        evaluation_result['semantic_match_score'],\n                        evaluation_result['verdict'],\n                        evaluation_result['missing_skills'],\n                        evaluation_result['improvement_suggestions'],\n                        evaluation_result['evaluation_details']\n                    )\n                    \n                    evaluated_count += 1\n                    progress_bar.progress(evaluated_count / total_resumes)\n                \n                status_text.text(\"✅ Batch evaluation completed!\")\n                st.success(f\"Successfully evaluated {evaluated_count} resumes!\")\n    \n    # Display existing evaluations\n    st.subheader(\"Evaluation Results\")\n    \n    # Filters\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        job_filter = st.selectbox(\"Filter by Job\", [\"All\"] + list(job_options.keys()))\n    \n    with col2:\n        min_score = st.slider(\"Minimum Score\", 0, 100, 0)\n    \n    with col3:\n        verdict_filter = st.selectbox(\"Filter by Verdict\", [\"All\", \"High\", \"Medium\", \"Low\"])\n    \n    # Get filtered evaluations\n    job_id_filter = job_options.get(job_filter) if job_filter != \"All\" else None\n    verdict_filter = verdict_filter if verdict_filter != \"All\" else None\n    \n    evaluations = get_evaluations(job_id_filter, min_score, verdict_filter)\n    \n    if evaluations:\n        # Create DataFrame for display\n        eval_data = []\n        for eval_row in evaluations:\n            eval_data.append({\n                'ID': eval_row[0],\n                'Job Title': eval_row[14],\n                'Company': eval_row[15],\n                'Candidate': eval_row[17] or 'Unknown',\n                'Email': eval_row[18] or 'Not provided',\n                'Score': eval_row[3],\n                'Verdict': eval_row[6],\n                'Date': eval_row[10]\n            })\n        \n        df = pd.DataFrame(eval_data)\n        \n        # Display results\n        st.dataframe(df, use_container_width=True)\n        \n        # Export functionality\n        if st.button(\"Export Results to CSV\"):\n            csv = df.to_csv(index=False)\n            st.download_button(\n                label=\"Download CSV\",\n                data=csv,\n                file_name=f\"evaluations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n                mime=\"text/csv\"\n            )\n    else:\n        st.info(\"No evaluations found with the selected filters.\")\n\ndef render_analytics_page():\n    \"\"\"Render analytics and insights page\"\"\"\n    st.header(\"📈 Analytics & Insights\")\n    \n    # Get all evaluations\n    evaluations = get_evaluations()\n    \n    if not evaluations:\n        st.info(\"No evaluation data available yet. Complete some evaluations to see analytics.\")\n        return\n    \n    # Create DataFrame\n    eval_data = []\n    for eval_row in evaluations:\n        eval_data.append({\n            'job_title': eval_row[14],\n            'company': eval_row[15],\n            'candidate_name': eval_row[17],\n            'relevance_score': eval_row[3],\n            'hard_match_score': eval_row[4],\n            'semantic_match_score': eval_row[5],\n            'verdict': eval_row[6],\n            'evaluated_at': eval_row[10]\n        })\n    \n    df = pd.DataFrame(eval_data)\n    \n    # Score distribution\n    st.subheader(\"Score Distribution\")\n    fig_hist = px.histogram(df, x='relevance_score', nbins=20, title=\"Distribution of Relevance Scores\")\n    st.plotly_chart(fig_hist, use_container_width=True)\n    \n    # Score comparison by job\n    st.subheader(\"Average Scores by Job Position\")\n    job_scores = df.groupby('job_title').agg({\n        'relevance_score': 'mean',\n        'hard_match_score': 'mean',\n        'semantic_match_score': 'mean'\n    }).round(2).reset_index()\n    \n    fig_bar = px.bar(job_scores, x='job_title', y='relevance_score', \n                     title=\"Average Relevance Score by Job Position\")\n    fig_bar.update_xaxis(tickangle=45)\n    st.plotly_chart(fig_bar, use_container_width=True)\n    \n    # Verdict distribution over time\n    st.subheader(\"Verdict Trends Over Time\")\n    df['date'] = pd.to_datetime(df['evaluated_at']).dt.date\n    verdict_trends = df.groupby(['date', 'verdict']).size().reset_index(name='count')\n    \n    fig_line = px.line(verdict_trends, x='date', y='count', color='verdict',\n                       title=\"Verdict Distribution Over Time\")\n    st.plotly_chart(fig_line, use_container_width=True)\n    \n    # Summary statistics\n    st.subheader(\"Summary Statistics\")\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.metric(\"Average Relevance Score\", f\"{df['relevance_score'].mean():.1f}\")\n        st.metric(\"Highest Score\", f\"{df['relevance_score'].max():.1f}\")\n        st.metric(\"Total Candidates Evaluated\", len(df))\n    \n    with col2:\n        st.metric(\"Average Hard Match Score\", f\"{df['hard_match_score'].mean():.1f}\")\n        st.metric(\"Average Semantic Score\", f\"{df['semantic_match_score'].mean():.1f}\")\n        high_potential = len(df[df['verdict'] == 'High'])\n        st.metric(\"High Potential Candidates\", high_potential)\n\ndef display_evaluation_results(evaluation_result):\n    \"\"\"Display evaluation results in a formatted way\"\"\"\n    st.subheader(\"🎯 Evaluation Results\")\n    \n    # Main scores\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"Relevance Score\", f\"{evaluation_result['relevance_score']}/100\")\n    \n    with col2:\n        st.metric(\"Hard Match Score\", f\"{evaluation_result['hard_match_score']:.1f}/100\")\n    \n    with col3:\n        st.metric(\"Semantic Score\", f\"{evaluation_result['semantic_match_score']:.1f}/100\")\n    \n    with col4:\n        verdict_color = {\"High\": \"🟢\", \"Medium\": \"🟡\", \"Low\": \"🔴\"}\n        st.metric(\"Verdict\", f\"{verdict_color.get(evaluation_result['verdict'], '⚪')} {evaluation_result['verdict']}\")\n    \n    # Score breakdown chart\n    scores_data = {\n        'Score Type': ['Hard Match', 'Semantic Match'],\n        'Score': [evaluation_result['hard_match_score'], evaluation_result['semantic_match_score']]\n    }\n    fig = px.bar(scores_data, x='Score Type', y='Score', title=\"Score Breakdown\")\n    fig.update_layout(yaxis=dict(range=[0, 100]))\n    st.plotly_chart(fig, use_container_width=True)\n    \n    # Missing skills\n    if evaluation_result['missing_skills']:\n        st.subheader(\"❌ Missing Skills\")\n        missing_skills_text = \", \".join(evaluation_result['missing_skills'])\n        st.error(f\"Missing: {missing_skills_text}\")\n    \n    # Improvement suggestions\n    if evaluation_result['improvement_suggestions']:\n        st.subheader(\"💡 Improvement Suggestions\")\n        for i, suggestion in enumerate(evaluation_result['improvement_suggestions'], 1):\n            st.write(f\"{i}. {suggestion}\")\n    \n    # Detailed analysis\n    with st.expander(\"Detailed Analysis\"):\n        st.json(evaluation_result['evaluation_details'])\n","size_bytes":20848},"database.py":{"content":"import sqlite3\nimport json\nfrom datetime import datetime\nimport streamlit as st\n\nDATABASE_PATH = \"resume_evaluation.db\"\n\ndef get_connection():\n    \"\"\"Get database connection\"\"\"\n    return sqlite3.connect(DATABASE_PATH)\n\ndef init_database():\n    \"\"\"Initialize database with required tables\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    # Job descriptions table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS job_descriptions (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            title TEXT NOT NULL,\n            company TEXT,\n            location TEXT,\n            description TEXT NOT NULL,\n            required_skills TEXT,\n            preferred_skills TEXT,\n            qualifications TEXT,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    ''')\n    \n    # Resumes table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS resumes (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            filename TEXT NOT NULL,\n            candidate_name TEXT,\n            candidate_email TEXT,\n            extracted_text TEXT NOT NULL,\n            skills TEXT,\n            experience TEXT,\n            education TEXT,\n            uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    ''')\n    \n    # Evaluations table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS evaluations (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            job_id INTEGER NOT NULL,\n            resume_id INTEGER NOT NULL,\n            relevance_score REAL NOT NULL,\n            hard_match_score REAL NOT NULL,\n            semantic_match_score REAL NOT NULL,\n            verdict TEXT NOT NULL,\n            missing_skills TEXT,\n            improvement_suggestions TEXT,\n            evaluation_details TEXT,\n            evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (job_id) REFERENCES job_descriptions (id),\n            FOREIGN KEY (resume_id) REFERENCES resumes (id)\n        )\n    ''')\n    \n    conn.commit()\n    conn.close()\n\ndef save_job_description(title, company, location, description, required_skills, preferred_skills, qualifications):\n    \"\"\"Save job description to database\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('''\n        INSERT INTO job_descriptions (title, company, location, description, required_skills, preferred_skills, qualifications)\n        VALUES (?, ?, ?, ?, ?, ?, ?)\n    ''', (title, company, location, description, json.dumps(required_skills), json.dumps(preferred_skills), json.dumps(qualifications)))\n    \n    job_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    return job_id\n\ndef save_resume(filename, candidate_name, candidate_email, extracted_text, skills, experience, education):\n    \"\"\"Save resume to database\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('''\n        INSERT INTO resumes (filename, candidate_name, candidate_email, extracted_text, skills, experience, education)\n        VALUES (?, ?, ?, ?, ?, ?, ?)\n    ''', (filename, candidate_name, candidate_email, extracted_text, json.dumps(skills), json.dumps(experience), json.dumps(education)))\n    \n    resume_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    return resume_id\n\ndef save_evaluation(job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details):\n    \"\"\"Save evaluation results to database\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('''\n        INSERT INTO evaluations (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n    ''', (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, json.dumps(missing_skills), json.dumps(improvement_suggestions), json.dumps(evaluation_details)))\n    \n    evaluation_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    return evaluation_id\n\ndef get_job_descriptions():\n    \"\"\"Get all job descriptions\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM job_descriptions ORDER BY created_at DESC')\n    jobs = cursor.fetchall()\n    conn.close()\n    \n    return jobs\n\ndef get_resumes():\n    \"\"\"Get all resumes\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM resumes ORDER BY uploaded_at DESC')\n    resumes = cursor.fetchall()\n    conn.close()\n    \n    return resumes\n\ndef get_evaluations(job_id=None, min_score=None, verdict=None):\n    \"\"\"Get evaluations with optional filters\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    query = '''\n        SELECT e.*, j.title as job_title, j.company, r.filename, r.candidate_name, r.candidate_email\n        FROM evaluations e\n        JOIN job_descriptions j ON e.job_id = j.id\n        JOIN resumes r ON e.resume_id = r.id\n        WHERE 1=1\n    '''\n    params = []\n    \n    if job_id:\n        query += ' AND e.job_id = ?'\n        params.append(job_id)\n    \n    if min_score:\n        query += ' AND e.relevance_score >= ?'\n        params.append(min_score)\n    \n    if verdict:\n        query += ' AND e.verdict = ?'\n        params.append(verdict)\n    \n    query += ' ORDER BY e.relevance_score DESC, e.evaluated_at DESC'\n    \n    cursor.execute(query, params)\n    evaluations = cursor.fetchall()\n    conn.close()\n    \n    return evaluations\n\ndef get_job_by_id(job_id):\n    \"\"\"Get job description by ID\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM job_descriptions WHERE id = ?', (job_id,))\n    job = cursor.fetchone()\n    conn.close()\n    \n    return job\n\ndef get_resume_by_id(resume_id):\n    \"\"\"Get resume by ID\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute('SELECT * FROM resumes WHERE id = ?', (resume_id,))\n    resume = cursor.fetchone()\n    conn.close()\n    \n    return resume\n\ndef get_evaluation_stats():\n    \"\"\"Get evaluation statistics\"\"\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    \n    # Total counts\n    cursor.execute('SELECT COUNT(*) FROM job_descriptions')\n    total_jobs = cursor.fetchone()[0]\n    \n    cursor.execute('SELECT COUNT(*) FROM resumes')\n    total_resumes = cursor.fetchone()[0]\n    \n    cursor.execute('SELECT COUNT(*) FROM evaluations')\n    total_evaluations = cursor.fetchone()[0]\n    \n    # Verdict distribution\n    cursor.execute('SELECT verdict, COUNT(*) FROM evaluations GROUP BY verdict')\n    verdict_dist = cursor.fetchall()\n    \n    # Average scores\n    cursor.execute('SELECT AVG(relevance_score) FROM evaluations')\n    avg_score = cursor.fetchone()[0] or 0\n    \n    conn.close()\n    \n    return {\n        'total_jobs': total_jobs,\n        'total_resumes': total_resumes,\n        'total_evaluations': total_evaluations,\n        'verdict_distribution': dict(verdict_dist),\n        'average_score': avg_score\n    }\n","size_bytes":7039},"database_postgres.py":{"content":"import psycopg2\nimport psycopg2.extras\nimport json\nimport os\nfrom datetime import datetime\nimport streamlit as st\n\n# Get PostgreSQL connection details from environment variables\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nPGHOST = os.getenv(\"PGHOST\")\nPGPORT = os.getenv(\"PGPORT\")\nPGUSER = os.getenv(\"PGUSER\")\nPGPASSWORD = os.getenv(\"PGPASSWORD\")\nPGDATABASE = os.getenv(\"PGDATABASE\")\n\ndef get_connection():\n    \"\"\"Get PostgreSQL database connection\"\"\"\n    try:\n        if DATABASE_URL:\n            return psycopg2.connect(DATABASE_URL)\n        else:\n            return psycopg2.connect(\n                host=PGHOST,\n                port=PGPORT,\n                user=PGUSER,\n                password=PGPASSWORD,\n                database=PGDATABASE\n            )\n    except Exception as e:\n        st.error(f\"Database connection error: {e}\")\n        return None\n\ndef init_database():\n    \"\"\"Initialize PostgreSQL database with required tables\"\"\"\n    conn = get_connection()\n    if not conn:\n        return False\n        \n    cursor = conn.cursor()\n    \n    try:\n        # Job descriptions table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS job_descriptions (\n                id SERIAL PRIMARY KEY,\n                title TEXT NOT NULL,\n                company TEXT,\n                location TEXT,\n                description TEXT NOT NULL,\n                required_skills JSONB,\n                preferred_skills JSONB,\n                qualifications JSONB,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Resumes table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS resumes (\n                id SERIAL PRIMARY KEY,\n                filename TEXT NOT NULL,\n                candidate_name TEXT,\n                candidate_email TEXT,\n                extracted_text TEXT NOT NULL,\n                skills JSONB,\n                experience TEXT,\n                education TEXT,\n                uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Evaluations table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS evaluations (\n                id SERIAL PRIMARY KEY,\n                job_id INTEGER NOT NULL,\n                resume_id INTEGER NOT NULL,\n                relevance_score REAL NOT NULL,\n                hard_match_score REAL NOT NULL,\n                semantic_match_score REAL NOT NULL,\n                verdict TEXT NOT NULL,\n                missing_skills JSONB,\n                improvement_suggestions JSONB,\n                evaluation_details JSONB,\n                evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (job_id) REFERENCES job_descriptions (id),\n                FOREIGN KEY (resume_id) REFERENCES resumes (id)\n            )\n        ''')\n        \n        # Users table for authentication\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS users (\n                id SERIAL PRIMARY KEY,\n                username TEXT UNIQUE NOT NULL,\n                email TEXT UNIQUE NOT NULL,\n                password_hash TEXT NOT NULL,\n                location TEXT,\n                role TEXT DEFAULT 'placement_team',\n                is_active BOOLEAN DEFAULT TRUE,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # User sessions table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS user_sessions (\n                id SERIAL PRIMARY KEY,\n                user_id INTEGER NOT NULL,\n                session_token TEXT UNIQUE NOT NULL,\n                expires_at TIMESTAMP NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (user_id) REFERENCES users (id)\n            )\n        ''')\n        \n        conn.commit()\n        return True\n        \n    except Exception as e:\n        st.error(f\"Database initialization error: {e}\")\n        conn.rollback()\n        return False\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_job_description(title, company, location, description, required_skills, preferred_skills, qualifications):\n    \"\"\"Save job description to PostgreSQL database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO job_descriptions (title, company, location, description, required_skills, preferred_skills, qualifications)\n            VALUES (%s, %s, %s, %s, %s, %s, %s) RETURNING id\n        ''', (title, company, location, description, json.dumps(required_skills), json.dumps(preferred_skills), json.dumps(qualifications)))\n        \n        job_id = cursor.fetchone()[0]\n        conn.commit()\n        return job_id\n        \n    except Exception as e:\n        st.error(f\"Error saving job description: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_resume(filename, candidate_name, candidate_email, extracted_text, skills, experience, education):\n    \"\"\"Save resume to PostgreSQL database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO resumes (filename, candidate_name, candidate_email, extracted_text, skills, experience, education)\n            VALUES (%s, %s, %s, %s, %s, %s, %s) RETURNING id\n        ''', (filename, candidate_name, candidate_email, extracted_text, json.dumps(skills), json.dumps(experience), json.dumps(education)))\n        \n        resume_id = cursor.fetchone()[0]\n        conn.commit()\n        return resume_id\n        \n    except Exception as e:\n        st.error(f\"Error saving resume: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_evaluation(job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details):\n    \"\"\"Save evaluation results to PostgreSQL database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO evaluations (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, missing_skills, improvement_suggestions, evaluation_details)\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id\n        ''', (job_id, resume_id, relevance_score, hard_match_score, semantic_match_score, verdict, json.dumps(missing_skills), json.dumps(improvement_suggestions), json.dumps(evaluation_details)))\n        \n        evaluation_id = cursor.fetchone()[0]\n        conn.commit()\n        return evaluation_id\n        \n    except Exception as e:\n        st.error(f\"Error saving evaluation: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_job_descriptions():\n    \"\"\"Get all job descriptions\"\"\"\n    conn = get_connection()\n    if not conn:\n        return []\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM job_descriptions ORDER BY created_at DESC')\n        jobs = cursor.fetchall()\n        return jobs\n        \n    except Exception as e:\n        st.error(f\"Error fetching job descriptions: {e}\")\n        return []\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_resumes():\n    \"\"\"Get all resumes\"\"\"\n    conn = get_connection()\n    if not conn:\n        return []\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM resumes ORDER BY uploaded_at DESC')\n        resumes = cursor.fetchall()\n        return resumes\n        \n    except Exception as e:\n        st.error(f\"Error fetching resumes: {e}\")\n        return []\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_evaluations(job_id=None, min_score=None, verdict=None):\n    \"\"\"Get evaluations with optional filters\"\"\"\n    conn = get_connection()\n    if not conn:\n        return []\n        \n    cursor = conn.cursor()\n    \n    try:\n        query = '''\n            SELECT e.*, j.title as job_title, j.company, r.filename, r.candidate_name, r.candidate_email\n            FROM evaluations e\n            JOIN job_descriptions j ON e.job_id = j.id\n            JOIN resumes r ON e.resume_id = r.id\n            WHERE 1=1\n        '''\n        params = []\n        \n        if job_id:\n            query += ' AND e.job_id = %s'\n            params.append(job_id)\n        \n        if min_score:\n            query += ' AND e.relevance_score >= %s'\n            params.append(min_score)\n        \n        if verdict:\n            query += ' AND e.verdict = %s'\n            params.append(verdict)\n        \n        query += ' ORDER BY e.relevance_score DESC, e.evaluated_at DESC'\n        \n        cursor.execute(query, params)\n        evaluations = cursor.fetchall()\n        return evaluations\n        \n    except Exception as e:\n        st.error(f\"Error fetching evaluations: {e}\")\n        return []\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_job_by_id(job_id):\n    \"\"\"Get job description by ID\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM job_descriptions WHERE id = %s', (job_id,))\n        job = cursor.fetchone()\n        return job\n        \n    except Exception as e:\n        st.error(f\"Error fetching job: {e}\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_resume_by_id(resume_id):\n    \"\"\"Get resume by ID\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM resumes WHERE id = %s', (resume_id,))\n        resume = cursor.fetchone()\n        return resume\n        \n    except Exception as e:\n        st.error(f\"Error fetching resume: {e}\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_evaluation_stats():\n    \"\"\"Get evaluation statistics\"\"\"\n    conn = get_connection()\n    if not conn:\n        return {'total_jobs': 0, 'total_resumes': 0, 'total_evaluations': 0, 'verdict_distribution': {}, 'average_score': 0}\n        \n    cursor = conn.cursor()\n    \n    try:\n        # Total counts\n        cursor.execute('SELECT COUNT(*) FROM job_descriptions')\n        total_jobs = cursor.fetchone()[0]\n        \n        cursor.execute('SELECT COUNT(*) FROM resumes')\n        total_resumes = cursor.fetchone()[0]\n        \n        cursor.execute('SELECT COUNT(*) FROM evaluations')\n        total_evaluations = cursor.fetchone()[0]\n        \n        # Verdict distribution\n        cursor.execute('SELECT verdict, COUNT(*) FROM evaluations GROUP BY verdict')\n        verdict_dist = cursor.fetchall()\n        \n        # Average scores\n        cursor.execute('SELECT AVG(relevance_score) FROM evaluations')\n        avg_score_result = cursor.fetchone()[0]\n        avg_score = float(avg_score_result) if avg_score_result else 0\n        \n        return {\n            'total_jobs': total_jobs,\n            'total_resumes': total_resumes,\n            'total_evaluations': total_evaluations,\n            'verdict_distribution': dict(verdict_dist),\n            'average_score': avg_score\n        }\n        \n    except Exception as e:\n        st.error(f\"Error fetching evaluation stats: {e}\")\n        return {'total_jobs': 0, 'total_resumes': 0, 'total_evaluations': 0, 'verdict_distribution': {}, 'average_score': 0}\n    finally:\n        cursor.close()\n        conn.close()\n\n# User authentication functions\ndef save_user(username, email, password_hash, location, role='placement_team'):\n    \"\"\"Save user to database\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO users (username, email, password_hash, location, role)\n            VALUES (%s, %s, %s, %s, %s) RETURNING id\n        ''', (username, email, password_hash, location, role))\n        \n        user_id = cursor.fetchone()[0]\n        conn.commit()\n        return user_id\n        \n    except Exception as e:\n        st.error(f\"Error saving user: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_user_by_username(username):\n    \"\"\"Get user by username\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('SELECT * FROM users WHERE username = %s AND is_active = TRUE', (username,))\n        user = cursor.fetchone()\n        return user\n        \n    except Exception as e:\n        st.error(f\"Error fetching user: {e}\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef save_session(user_id, session_token, expires_at):\n    \"\"\"Save user session\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            INSERT INTO user_sessions (user_id, session_token, expires_at)\n            VALUES (%s, %s, %s) RETURNING id\n        ''', (user_id, session_token, expires_at))\n        \n        session_id = cursor.fetchone()[0]\n        conn.commit()\n        return session_id\n        \n    except Exception as e:\n        st.error(f\"Error saving session: {e}\")\n        conn.rollback()\n        return None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_session(session_token):\n    \"\"\"Get session by token\"\"\"\n    conn = get_connection()\n    if not conn:\n        return None\n        \n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute('''\n            SELECT s.*, u.username, u.email, u.location, u.role \n            FROM user_sessions s\n            JOIN users u ON s.user_id = u.id\n            WHERE s.session_token = %s AND s.expires_at > CURRENT_TIMESTAMP AND u.is_active = TRUE\n        ''', (session_token,))\n        session = cursor.fetchone()\n        return session\n        \n    except Exception as e:\n        st.error(f\"Error fetching session: {e}\")\n        return None\n    finally:\n        cursor.close()\n        conn.close()","size_bytes":14297},"nlp_processor.py":{"content":"import spacy\nimport re\nimport json\nimport os\nfrom collections import Counter\nfrom openai import OpenAI\n\n# Initialize spaCy model\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept OSError:\n    import subprocess\n    import sys\n    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n    nlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize OpenAI client\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n\n# Common skills database\nTECHNICAL_SKILLS = [\n    'python', 'java', 'javascript', 'c++', 'c#', 'php', 'ruby', 'go', 'rust', 'kotlin',\n    'swift', 'typescript', 'scala', 'r', 'matlab', 'sql', 'nosql', 'mongodb', 'postgresql',\n    'mysql', 'oracle', 'redis', 'elasticsearch', 'docker', 'kubernetes', 'aws', 'azure',\n    'gcp', 'terraform', 'jenkins', 'git', 'github', 'gitlab', 'jira', 'confluence',\n    'react', 'angular', 'vue', 'node.js', 'express', 'django', 'flask', 'spring',\n    'laravel', 'rails', 'asp.net', 'tensorflow', 'pytorch', 'scikit-learn', 'pandas',\n    'numpy', 'matplotlib', 'seaborn', 'tableau', 'power bi', 'excel', 'spark', 'hadoop',\n    'kafka', 'rabbitmq', 'microservices', 'restful', 'graphql', 'soap', 'api', 'agile',\n    'scrum', 'devops', 'ci/cd', 'machine learning', 'deep learning', 'ai', 'nlp',\n    'computer vision', 'data science', 'data analysis', 'statistics', 'blockchain',\n    'cybersecurity', 'linux', 'windows', 'macos', 'bash', 'powershell', 'html', 'css',\n    'bootstrap', 'sass', 'less', 'webpack', 'npm', 'yarn', 'junit', 'selenium', 'pytest'\n]\n\ndef extract_skills_from_text(text):\n    \"\"\"Extract technical skills from text\"\"\"\n    text_lower = text.lower()\n    found_skills = []\n    \n    # Direct skill matching\n    for skill in TECHNICAL_SKILLS:\n        if skill.lower() in text_lower:\n            found_skills.append(skill)\n    \n    # Entity recognition for additional skills\n    doc = nlp(text)\n    for ent in doc.ents:\n        if ent.label_ in ['ORG', 'PRODUCT'] and len(ent.text) > 2:\n            potential_skill = ent.text.lower().strip()\n            if potential_skill not in [s.lower() for s in found_skills]:\n                found_skills.append(ent.text.strip())\n    \n    return list(set(found_skills))\n\ndef extract_experience_years(text):\n    \"\"\"Extract years of experience from text\"\"\"\n    experience_patterns = [\n        r'(\\d+)\\+?\\s*years?\\s+(?:of\\s+)?experience',\n        r'(\\d+)\\+?\\s*yrs?\\s+(?:of\\s+)?experience',\n        r'experience\\s*:\\s*(\\d+)\\+?\\s*years?',\n        r'(\\d+)\\+?\\s*years?\\s+in',\n        r'over\\s+(\\d+)\\s+years?',\n        r'more\\s+than\\s+(\\d+)\\s+years?'\n    ]\n    \n    text_lower = text.lower()\n    years = []\n    \n    for pattern in experience_patterns:\n        matches = re.findall(pattern, text_lower)\n        years.extend([int(match) for match in matches])\n    \n    return max(years) if years else 0\n\ndef extract_education_level(text):\n    \"\"\"Extract education level from text\"\"\"\n    education_levels = {\n        'phd': ['ph.d', 'phd', 'doctorate', 'doctoral'],\n        'masters': ['master', 'msc', 'ma', 'mba', 'ms', 'm.sc', 'm.a', 'm.s'],\n        'bachelors': ['bachelor', 'bsc', 'ba', 'be', 'btech', 'b.sc', 'b.a', 'b.e', 'b.tech'],\n        'diploma': ['diploma', 'certificate'],\n        'high_school': ['high school', 'secondary', '12th', 'intermediate']\n    }\n    \n    text_lower = text.lower()\n    detected_levels = []\n    \n    for level, keywords in education_levels.items():\n        for keyword in keywords:\n            if keyword in text_lower:\n                detected_levels.append(level)\n                break\n    \n    # Return highest level found\n    level_hierarchy = ['phd', 'masters', 'bachelors', 'diploma', 'high_school']\n    for level in level_hierarchy:\n        if level in detected_levels:\n            return level\n    \n    return 'unknown'\n\ndef parse_job_description(job_text):\n    \"\"\"Parse job description to extract requirements\"\"\"\n    if not openai_client:\n        return parse_job_description_rule_based(job_text)\n    \n    try:\n        # the newest OpenAI model is \"gpt-5\" which was released August 7, 2025.\n        # do not change this unless explicitly requested by the user\n        response = openai_client.chat.completions.create(\n            model=\"gpt-5\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an expert HR analyst. Parse the job description and extract key information. Respond with JSON in this exact format: {'required_skills': ['skill1', 'skill2'], 'preferred_skills': ['skill1', 'skill2'], 'experience_required': number, 'education_required': 'level', 'key_responsibilities': ['resp1', 'resp2']}\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Parse this job description:\\n\\n{job_text}\"\n                }\n            ],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {}\n        return result\n    \n    except Exception as e:\n        print(f\"Error parsing job description with AI: {e}\")\n        return parse_job_description_rule_based(job_text)\n\ndef parse_job_description_rule_based(job_text):\n    \"\"\"Rule-based job description parsing as fallback\"\"\"\n    text_lower = job_text.lower()\n    \n    # Extract skills\n    skills = extract_skills_from_text(job_text)\n    \n    # Separate required vs preferred\n    required_skills = []\n    preferred_skills = []\n    \n    # Look for required/must-have sections\n    required_indicators = ['required', 'must have', 'essential', 'mandatory']\n    preferred_indicators = ['preferred', 'nice to have', 'plus', 'bonus', 'desirable']\n    \n    lines = job_text.split('\\n')\n    current_section = 'unknown'\n    \n    for line in lines:\n        line_lower = line.lower()\n        if any(indicator in line_lower for indicator in required_indicators):\n            current_section = 'required'\n        elif any(indicator in line_lower for indicator in preferred_indicators):\n            current_section = 'preferred'\n        \n        # Extract skills from current line\n        line_skills = [skill for skill in skills if skill.lower() in line_lower]\n        \n        if current_section == 'required':\n            required_skills.extend(line_skills)\n        elif current_section == 'preferred':\n            preferred_skills.extend(line_skills)\n    \n    # If no clear separation, assume all skills are required\n    if not required_skills and not preferred_skills:\n        required_skills = skills[:len(skills)//2] if len(skills) > 5 else skills\n        preferred_skills = skills[len(skills)//2:] if len(skills) > 5 else []\n    \n    return {\n        'required_skills': list(set(required_skills)),\n        'preferred_skills': list(set(preferred_skills)),\n        'experience_required': extract_experience_years(job_text),\n        'education_required': extract_education_level(job_text),\n        'key_responsibilities': extract_responsibilities(job_text)\n    }\n\ndef extract_responsibilities(text):\n    \"\"\"Extract key responsibilities from job description\"\"\"\n    lines = text.split('\\n')\n    responsibilities = []\n    \n    responsibility_indicators = ['responsible for', 'duties', 'responsibilities', 'role includes', 'you will']\n    \n    for i, line in enumerate(lines):\n        line_lower = line.lower().strip()\n        if any(indicator in line_lower for indicator in responsibility_indicators):\n            # Look at next few lines for bullet points\n            for j in range(i+1, min(i+10, len(lines))):\n                next_line = lines[j].strip()\n                if next_line and (next_line.startswith('-') or next_line.startswith('•') or next_line.startswith('*')):\n                    responsibilities.append(next_line.lstrip('-•* '))\n                elif not next_line:\n                    break\n    \n    return responsibilities[:5]  # Return top 5 responsibilities\n\ndef analyze_resume_semantic(resume_text, job_requirements):\n    \"\"\"Perform semantic analysis of resume against job requirements using AI\"\"\"\n    if not openai_client:\n        return {\n            'semantic_score': 50,  # Default neutral score\n            'analysis': 'AI analysis not available - API key not configured',\n            'strengths': [],\n            'gaps': []\n        }\n    \n    try:\n        prompt = f\"\"\"\n        Analyze this resume against the job requirements and provide a semantic match score.\n        \n        Job Requirements:\n        - Required Skills: {job_requirements.get('required_skills', [])}\n        - Preferred Skills: {job_requirements.get('preferred_skills', [])}\n        - Experience Required: {job_requirements.get('experience_required', 0)} years\n        - Education Required: {job_requirements.get('education_required', 'unknown')}\n        \n        Resume Text:\n        {resume_text[:2000]}\n        \n        Provide a JSON response with:\n        {{\n            \"semantic_score\": number (0-100),\n            \"analysis\": \"detailed analysis text\",\n            \"strengths\": [\"strength1\", \"strength2\"],\n            \"gaps\": [\"gap1\", \"gap2\"]\n        }}\n        \"\"\"\n        \n        # the newest OpenAI model is \"gpt-5\" which was released August 7, 2025.\n        # do not change this unless explicitly requested by the user\n        response = openai_client.chat.completions.create(\n            model=\"gpt-5\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an expert resume analyzer. Provide detailed semantic analysis comparing resumes to job requirements.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {}\n        return result\n    \n    except Exception as e:\n        print(f\"Error in semantic analysis: {e}\")\n        return {\n            'semantic_score': 50,\n            'analysis': f'Error in AI analysis: {str(e)}',\n            'strengths': [],\n            'gaps': []\n        }\n\ndef generate_improvement_suggestions(resume_analysis, job_requirements, missing_skills):\n    \"\"\"Generate personalized improvement suggestions for candidates\"\"\"\n    if not openai_client:\n        return [\n            \"Consider acquiring the missing technical skills identified in the analysis\",\n            \"Highlight relevant project experience more prominently\",\n            \"Add specific metrics and achievements to demonstrate impact\"\n        ]\n    \n    try:\n        prompt = f\"\"\"\n        Based on this resume analysis and job requirements, provide 3-5 specific, actionable improvement suggestions for the candidate.\n        \n        Job Requirements:\n        {job_requirements}\n        \n        Resume Analysis:\n        {resume_analysis}\n        \n        Missing Skills:\n        {missing_skills}\n        \n        Provide suggestions as a JSON array of strings:\n        {{\"suggestions\": [\"suggestion1\", \"suggestion2\", \"suggestion3\"]}}\n        \"\"\"\n        \n        # the newest OpenAI model is \"gpt-5\" which was released August 7, 2025.\n        # do not change this unless explicitly requested by the user\n        response = openai_client.chat.completions.create(\n            model=\"gpt-5\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a career counselor providing specific, actionable advice to job candidates.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {}\n        return result.get('suggestions', [])\n    \n    except Exception as e:\n        print(f\"Error generating suggestions: {e}\")\n        return [\n            \"Consider acquiring the missing technical skills\",\n            \"Add more specific project details and achievements\",\n            \"Improve resume format and structure for better readability\"\n        ]\n","size_bytes":12442},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"docx>=0.2.4\",\n    \"fitz>=0.0.1.dev2\",\n    \"fuzzywuzzy>=0.18.0\",\n    \"numpy>=2.3.3\",\n    \"openai>=1.108.1\",\n    \"pandas>=2.3.2\",\n    \"plotly>=6.3.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"pymupdf>=1.26.4\",\n    \"python-docx>=1.2.0\",\n    \"python-levenshtein>=0.27.1\",\n    \"scikit-learn>=1.7.2\",\n    \"spacy>=3.8.7\",\n    \"streamlit>=1.49.1\",\n]\n","size_bytes":484},"scoring_engine.py":{"content":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom nlp_processor import extract_skills_from_text, extract_experience_years, extract_education_level, analyze_resume_semantic, generate_improvement_suggestions\nfrom fuzzywuzzy import fuzz\nimport json\n\nclass ResumeScorer:\n    def __init__(self):\n        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n        \n    def calculate_hard_match_score(self, resume_data, job_requirements):\n        \"\"\"Calculate hard match score based on exact keyword and skill matching\"\"\"\n        scores = {}\n        \n        # Skills matching\n        resume_skills = extract_skills_from_text(resume_data['extracted_text'])\n        required_skills = job_requirements.get('required_skills', [])\n        preferred_skills = job_requirements.get('preferred_skills', [])\n        \n        # Exact skill matches\n        exact_required_matches = len(set([s.lower() for s in resume_skills]) & set([s.lower() for s in required_skills]))\n        exact_preferred_matches = len(set([s.lower() for s in resume_skills]) & set([s.lower() for s in preferred_skills]))\n        \n        # Fuzzy skill matches (for handling variations)\n        fuzzy_required_matches = 0\n        fuzzy_preferred_matches = 0\n        \n        for resume_skill in resume_skills:\n            for required_skill in required_skills:\n                if fuzz.ratio(resume_skill.lower(), required_skill.lower()) > 80:\n                    fuzzy_required_matches += 1\n                    break\n                    \n        for resume_skill in resume_skills:\n            for preferred_skill in preferred_skills:\n                if fuzz.ratio(resume_skill.lower(), preferred_skill.lower()) > 80:\n                    fuzzy_preferred_matches += 1\n                    break\n        \n        # Calculate skill scores\n        total_required_skills = len(required_skills) if required_skills else 1\n        total_preferred_skills = len(preferred_skills) if preferred_skills else 1\n        \n        required_skill_score = min(100, ((exact_required_matches + fuzzy_required_matches) / total_required_skills) * 100)\n        preferred_skill_score = min(100, ((exact_preferred_matches + fuzzy_preferred_matches) / total_preferred_skills) * 100)\n        \n        scores['skills'] = {\n            'required_score': required_skill_score,\n            'preferred_score': preferred_skill_score,\n            'matched_required': exact_required_matches + fuzzy_required_matches,\n            'matched_preferred': exact_preferred_matches + fuzzy_preferred_matches,\n            'total_required': total_required_skills,\n            'total_preferred': total_preferred_skills\n        }\n        \n        # Experience matching\n        resume_experience = extract_experience_years(resume_data['extracted_text'])\n        required_experience = job_requirements.get('experience_required', 0)\n        \n        if required_experience == 0:\n            experience_score = 100\n        else:\n            experience_ratio = min(1.0, resume_experience / required_experience)\n            experience_score = experience_ratio * 100\n        \n        scores['experience'] = {\n            'score': experience_score,\n            'resume_years': resume_experience,\n            'required_years': required_experience\n        }\n        \n        # Education matching\n        resume_education = extract_education_level(resume_data['extracted_text'])\n        required_education = job_requirements.get('education_required', 'unknown')\n        \n        education_hierarchy = {\n            'high_school': 1,\n            'diploma': 2,\n            'bachelors': 3,\n            'masters': 4,\n            'phd': 5,\n            'unknown': 0\n        }\n        \n        resume_edu_level = education_hierarchy.get(resume_education, 0)\n        required_edu_level = education_hierarchy.get(required_education, 0)\n        \n        if required_edu_level == 0:\n            education_score = 100\n        elif resume_edu_level >= required_edu_level:\n            education_score = 100\n        else:\n            education_score = max(0, (resume_edu_level / required_edu_level) * 100)\n        \n        scores['education'] = {\n            'score': education_score,\n            'resume_level': resume_education,\n            'required_level': required_education\n        }\n        \n        # Calculate weighted hard match score\n        weights = {\n            'required_skills': 0.5,\n            'preferred_skills': 0.2,\n            'experience': 0.2,\n            'education': 0.1\n        }\n        \n        total_hard_score = (\n            required_skill_score * weights['required_skills'] +\n            preferred_skill_score * weights['preferred_skills'] +\n            experience_score * weights['experience'] +\n            education_score * weights['education']\n        )\n        \n        return total_hard_score, scores\n    \n    def calculate_semantic_match_score(self, resume_data, job_description_text):\n        \"\"\"Calculate semantic match score using TF-IDF and cosine similarity\"\"\"\n        try:\n            # Prepare texts\n            resume_text = resume_data['extracted_text']\n            \n            # Create TF-IDF vectors\n            documents = [resume_text, job_description_text]\n            tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)\n            \n            # Calculate cosine similarity\n            similarity_matrix = cosine_similarity(tfidf_matrix)\n            semantic_score = similarity_matrix[0][1] * 100\n            \n            return semantic_score\n            \n        except Exception as e:\n            print(f\"Error in semantic scoring: {e}\")\n            return 50  # Default neutral score\n    \n    def identify_missing_skills(self, resume_data, job_requirements):\n        \"\"\"Identify missing skills and qualifications\"\"\"\n        resume_skills = [s.lower() for s in extract_skills_from_text(resume_data['extracted_text'])]\n        required_skills = [s.lower() for s in job_requirements.get('required_skills', [])]\n        preferred_skills = [s.lower() for s in job_requirements.get('preferred_skills', [])]\n        \n        missing_required = []\n        missing_preferred = []\n        \n        # Check for missing required skills\n        for skill in required_skills:\n            if not any(fuzz.ratio(skill, resume_skill) > 80 for resume_skill in resume_skills):\n                missing_required.append(skill)\n        \n        # Check for missing preferred skills\n        for skill in preferred_skills:\n            if not any(fuzz.ratio(skill, resume_skill) > 80 for resume_skill in resume_skills):\n                missing_preferred.append(skill)\n        \n        # Check experience gap\n        resume_experience = extract_experience_years(resume_data['extracted_text'])\n        required_experience = job_requirements.get('experience_required', 0)\n        experience_gap = max(0, required_experience - resume_experience)\n        \n        # Check education gap\n        resume_education = extract_education_level(resume_data['extracted_text'])\n        required_education = job_requirements.get('education_required', 'unknown')\n        \n        education_hierarchy = {\n            'high_school': 1, 'diploma': 2, 'bachelors': 3, 'masters': 4, 'phd': 5, 'unknown': 0\n        }\n        \n        resume_edu_level = education_hierarchy.get(resume_education, 0)\n        required_edu_level = education_hierarchy.get(required_education, 0)\n        education_gap = required_edu_level > resume_edu_level\n        \n        return {\n            'missing_required_skills': missing_required,\n            'missing_preferred_skills': missing_preferred,\n            'experience_gap_years': experience_gap,\n            'education_gap': education_gap,\n            'current_education': resume_education,\n            'required_education': required_education\n        }\n    \n    def determine_verdict(self, relevance_score):\n        \"\"\"Determine fit verdict based on relevance score\"\"\"\n        if relevance_score >= 75:\n            return \"High\"\n        elif relevance_score >= 50:\n            return \"Medium\"\n        else:\n            return \"Low\"\n    \n    def evaluate_resume(self, resume_data, job_description_text, job_requirements):\n        \"\"\"Complete resume evaluation pipeline\"\"\"\n        try:\n            # Calculate hard match score\n            hard_match_score, hard_match_details = self.calculate_hard_match_score(resume_data, job_requirements)\n            \n            # Calculate semantic match score using TF-IDF\n            tfidf_semantic_score = self.calculate_semantic_match_score(resume_data, job_description_text)\n            \n            # Get AI-powered semantic analysis\n            ai_semantic_analysis = analyze_resume_semantic(resume_data['extracted_text'], job_requirements)\n            ai_semantic_score = ai_semantic_analysis.get('semantic_score', 50)\n            \n            # Combine semantic scores (weighted average)\n            combined_semantic_score = (tfidf_semantic_score * 0.4) + (ai_semantic_score * 0.6)\n            \n            # Calculate final relevance score\n            weights = {\n                'hard_match': 0.6,\n                'semantic_match': 0.4\n            }\n            \n            relevance_score = (\n                hard_match_score * weights['hard_match'] +\n                combined_semantic_score * weights['semantic_match']\n            )\n            \n            # Identify missing skills and gaps\n            missing_elements = self.identify_missing_skills(resume_data, job_requirements)\n            \n            # Determine verdict\n            verdict = self.determine_verdict(relevance_score)\n            \n            # Generate improvement suggestions\n            improvement_suggestions = generate_improvement_suggestions(\n                ai_semantic_analysis, job_requirements, missing_elements\n            )\n            \n            # Compile evaluation details\n            evaluation_details = {\n                'hard_match_details': hard_match_details,\n                'tfidf_semantic_score': tfidf_semantic_score,\n                'ai_semantic_analysis': ai_semantic_analysis,\n                'combined_semantic_score': combined_semantic_score,\n                'scoring_weights': weights,\n                'missing_elements': missing_elements\n            }\n            \n            return {\n                'relevance_score': round(relevance_score, 2),\n                'hard_match_score': round(hard_match_score, 2),\n                'semantic_match_score': round(combined_semantic_score, 2),\n                'verdict': verdict,\n                'missing_skills': missing_elements['missing_required_skills'] + missing_elements['missing_preferred_skills'],\n                'improvement_suggestions': improvement_suggestions,\n                'evaluation_details': evaluation_details\n            }\n            \n        except Exception as e:\n            print(f\"Error in resume evaluation: {e}\")\n            return {\n                'relevance_score': 0,\n                'hard_match_score': 0,\n                'semantic_match_score': 0,\n                'verdict': 'Low',\n                'missing_skills': [],\n                'improvement_suggestions': ['Error occurred during evaluation'],\n                'evaluation_details': {'error': str(e)}\n            }\n","size_bytes":11375},"text_extractor.py":{"content":"import fitz  # PyMuPDF\nimport docx\nimport re\nimport streamlit as st\nfrom io import BytesIO\n\ndef extract_text_from_pdf(file_bytes):\n    \"\"\"Extract text from PDF file\"\"\"\n    try:\n        # Open PDF from bytes\n        pdf_document = fitz.open(stream=file_bytes, filetype=\"pdf\")\n        text = \"\"\n        \n        for page_num in range(pdf_document.page_count):\n            page = pdf_document.load_page(page_num)\n            text += page.get_text()\n        \n        pdf_document.close()\n        return clean_text(text)\n    \n    except Exception as e:\n        st.error(f\"Error extracting text from PDF: {str(e)}\")\n        return \"\"\n\ndef extract_text_from_docx(file_bytes):\n    \"\"\"Extract text from DOCX file\"\"\"\n    try:\n        # Open DOCX from bytes\n        doc = docx.Document(BytesIO(file_bytes))\n        text = \"\"\n        \n        for paragraph in doc.paragraphs:\n            text += paragraph.text + \"\\n\"\n        \n        return clean_text(text)\n    \n    except Exception as e:\n        st.error(f\"Error extracting text from DOCX: {str(e)}\")\n        return \"\"\n\ndef clean_text(text):\n    \"\"\"Clean and normalize extracted text\"\"\"\n    if not text:\n        return \"\"\n    \n    # Remove extra whitespace and normalize\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    \n    # Remove common PDF artifacts\n    text = re.sub(r'[^\\w\\s\\-.,;:()\\[\\]@+/&%]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text\n\ndef extract_contact_info(text):\n    \"\"\"Extract contact information from text\"\"\"\n    contact_info = {}\n    \n    # Email extraction\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    emails = re.findall(email_pattern, text)\n    contact_info['email'] = emails[0] if emails else \"\"\n    \n    # Phone extraction\n    phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n    phones = re.findall(phone_pattern, text)\n    contact_info['phone'] = phones[0] if phones else \"\"\n    \n    # Name extraction (basic - first few words before common resume sections)\n    lines = text.split('\\n')\n    name = \"\"\n    for line in lines[:5]:  # Check first 5 lines\n        line = line.strip()\n        if line and len(line.split()) <= 4 and not any(keyword in line.lower() for keyword in ['resume', 'cv', 'curriculum', 'vitae', 'email', 'phone', '@']):\n            if not re.search(r'\\d', line):  # No numbers in name\n                name = line\n                break\n    \n    contact_info['name'] = name\n    \n    return contact_info\n\ndef extract_sections(text):\n    \"\"\"Extract different sections from resume text\"\"\"\n    sections = {\n        'experience': '',\n        'education': '',\n        'skills': '',\n        'projects': '',\n        'certifications': ''\n    }\n    \n    # Common section headers\n    section_patterns = {\n        'experience': r'(experience|work\\s+experience|professional\\s+experience|employment|career)',\n        'education': r'(education|academic|qualification|degree)',\n        'skills': r'(skills|technical\\s+skills|competencies|expertise)',\n        'projects': r'(projects|personal\\s+projects|portfolio)',\n        'certifications': r'(certifications?|certificates?|credentials)'\n    }\n    \n    text_lower = text.lower()\n    \n    for section_name, pattern in section_patterns.items():\n        # Find section start\n        matches = list(re.finditer(pattern, text_lower))\n        if matches:\n            start_pos = matches[0].start()\n            \n            # Find next section or end of text\n            end_pos = len(text)\n            for other_pattern in section_patterns.values():\n                if other_pattern != pattern:\n                    other_matches = list(re.finditer(other_pattern, text_lower[start_pos + 50:]))\n                    if other_matches:\n                        potential_end = start_pos + 50 + other_matches[0].start()\n                        if potential_end < end_pos:\n                            end_pos = potential_end\n            \n            sections[section_name] = text[start_pos:end_pos].strip()\n    \n    return sections\n\ndef process_uploaded_file(uploaded_file):\n    \"\"\"Process uploaded resume file and extract all information\"\"\"\n    if uploaded_file is None:\n        return None\n    \n    file_bytes = uploaded_file.read()\n    filename = uploaded_file.name\n    file_extension = filename.lower().split('.')[-1]\n    \n    # Extract text based on file type\n    if file_extension == 'pdf':\n        extracted_text = extract_text_from_pdf(file_bytes)\n    elif file_extension in ['docx', 'doc']:\n        extracted_text = extract_text_from_docx(file_bytes)\n    else:\n        st.error(\"Unsupported file format. Please upload PDF or DOCX files.\")\n        return None\n    \n    if not extracted_text:\n        st.error(\"Could not extract text from the file. Please check the file format.\")\n        return None\n    \n    # Extract contact information\n    contact_info = extract_contact_info(extracted_text)\n    \n    # Extract sections\n    sections = extract_sections(extracted_text)\n    \n    return {\n        'filename': filename,\n        'extracted_text': extracted_text,\n        'contact_info': contact_info,\n        'sections': sections\n    }\n","size_bytes":5147},"utils.py":{"content":"import re\nimport string\nimport unicodedata\nfrom datetime import datetime\nimport streamlit as st\n\ndef clean_filename(filename):\n    \"\"\"Clean filename for safe storage\"\"\"\n    # Remove extension for processing\n    name, ext = filename.rsplit('.', 1) if '.' in filename else (filename, '')\n    \n    # Remove special characters and normalize\n    name = re.sub(r'[^\\w\\s-]', '', name.strip())\n    name = re.sub(r'[-\\s]+', '_', name)\n    \n    # Add timestamp to make unique\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    return f\"{name}_{timestamp}.{ext}\" if ext else f\"{name}_{timestamp}\"\n\ndef normalize_text(text):\n    \"\"\"Normalize text for better processing\"\"\"\n    if not text:\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove special characters but keep important punctuation\n    text = re.sub(r'[^\\w\\s.,;:()\\-]', ' ', text)\n    \n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndef extract_keywords(text, min_length=3):\n    \"\"\"Extract keywords from text\"\"\"\n    if not text:\n        return []\n    \n    # Normalize text\n    text = normalize_text(text)\n    \n    # Split into words\n    words = text.split()\n    \n    # Filter keywords\n    keywords = []\n    for word in words:\n        if (len(word) >= min_length and \n            word not in string.punctuation and\n            not word.isdigit()):\n            keywords.append(word)\n    \n    return list(set(keywords))\n\ndef calculate_text_similarity(text1, text2):\n    \"\"\"Calculate simple text similarity using Jaccard similarity\"\"\"\n    if not text1 or not text2:\n        return 0.0\n    \n    # Get keywords from both texts\n    keywords1 = set(extract_keywords(text1))\n    keywords2 = set(extract_keywords(text2))\n    \n    if not keywords1 or not keywords2:\n        return 0.0\n    \n    # Calculate Jaccard similarity\n    intersection = len(keywords1.intersection(keywords2))\n    union = len(keywords1.union(keywords2))\n    \n    return intersection / union if union > 0 else 0.0\n\ndef format_date(date_string):\n    \"\"\"Format date string for display\"\"\"\n    try:\n        if isinstance(date_string, str):\n            dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n        else:\n            dt = date_string\n        return dt.strftime('%Y-%m-%d %H:%M')\n    except:\n        return str(date_string)\n\ndef validate_email(email):\n    \"\"\"Validate email format\"\"\"\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\ndef sanitize_input(text, max_length=None):\n    \"\"\"Sanitize user input\"\"\"\n    if not text:\n        return \"\"\n    \n    # Remove potentially harmful characters\n    text = re.sub(r'[<>\"\\']', '', text)\n    \n    # Limit length if specified\n    if max_length and len(text) > max_length:\n        text = text[:max_length]\n    \n    return text.strip()\n\ndef create_download_link(data, filename, mime_type=\"text/plain\"):\n    \"\"\"Create a download link for data\"\"\"\n    return st.download_button(\n        label=f\"Download {filename}\",\n        data=data,\n        file_name=filename,\n        mime=mime_type\n    )\n\ndef format_score(score, precision=1):\n    \"\"\"Format score for display\"\"\"\n    try:\n        return f\"{float(score):.{precision}f}\"\n    except:\n        return \"N/A\"\n\ndef get_verdict_color(verdict):\n    \"\"\"Get color for verdict display\"\"\"\n    colors = {\n        'High': '#28a745',    # Green\n        'Medium': '#ffc107',  # Yellow\n        'Low': '#dc3545',     # Red\n    }\n    return colors.get(verdict, '#6c757d')  # Default gray\n\ndef truncate_text(text, max_length=100):\n    \"\"\"Truncate text with ellipsis\"\"\"\n    if not text:\n        return \"\"\n    \n    if len(text) <= max_length:\n        return text\n    \n    return text[:max_length-3] + \"...\"\n\ndef parse_json_safely(json_string):\n    \"\"\"Safely parse JSON string\"\"\"\n    try:\n        import json\n        return json.loads(json_string) if json_string else {}\n    except:\n        return {}\n\ndef format_list_for_display(items, separator=\", \"):\n    \"\"\"Format list for display\"\"\"\n    if not items:\n        return \"None\"\n    \n    if isinstance(items, str):\n        items = parse_json_safely(items)\n    \n    if isinstance(items, list):\n        return separator.join(str(item) for item in items)\n    \n    return str(items)\n\ndef highlight_keywords(text, keywords, highlight_color=\"#ffff00\"):\n    \"\"\"Highlight keywords in text for display\"\"\"\n    if not text or not keywords:\n        return text\n    \n    highlighted_text = text\n    for keyword in keywords:\n        pattern = re.compile(re.escape(keyword), re.IGNORECASE)\n        highlighted_text = pattern.sub(\n            f'<mark style=\"background-color: {highlight_color};\">{keyword}</mark>',\n            highlighted_text\n        )\n    \n    return highlighted_text\n\ndef calculate_completion_percentage(completed_items, total_items):\n    \"\"\"Calculate completion percentage\"\"\"\n    if total_items == 0:\n        return 0\n    return (completed_items / total_items) * 100\n\ndef get_file_size_mb(file_bytes):\n    \"\"\"Get file size in MB\"\"\"\n    return len(file_bytes) / (1024 * 1024)\n\ndef validate_file_size(file_bytes, max_size_mb=10):\n    \"\"\"Validate file size\"\"\"\n    size_mb = get_file_size_mb(file_bytes)\n    return size_mb <= max_size_mb, size_mb\n\ndef create_progress_indicator(current, total, label=\"Progress\"):\n    \"\"\"Create a progress indicator\"\"\"\n    if total == 0:\n        return st.write(f\"{label}: No items to process\")\n    \n    percentage = (current / total) * 100\n    return st.progress(percentage / 100)\n","size_bytes":5596}},"version":1}